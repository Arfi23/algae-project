{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6f0377-3454-405c-9545-60c931f4ad24",
   "metadata": {},
   "source": [
    "### Ver3 Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa43fa3c-6f3e-4cdb-ba31-8da1fa6e3c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device yang digunakan: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n",
      "Total Memory: 17.17 GB\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 1. Import Library dan Setup Environment\n",
    "# ===========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Gunakan GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device yang digunakan:\", device)\n",
    "\n",
    "# Monitor GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937b429a-e36b-4645-9292-0f5df6c04fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 2. Fungsi Bantuan Umum\n",
    "# ===========================================\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Menetapkan seed random agar hasil eksperimen bisa direplikasi\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "def visualize_tile(x_tile, y_true=None, y_pred=None, json_path=None, class_names=None, idx=0):\n",
    "    \"\"\"Menampilkan citra tile beserta mask ground-truth dan prediksi\"\"\"\n",
    "    if isinstance(x_tile, torch.Tensor):\n",
    "        x = x_tile.cpu().numpy()\n",
    "        x = np.transpose(x, (1,2,0))  # ubah dari [B,H,W] -> [H,W,B]\n",
    "    else:\n",
    "        x = x_tile\n",
    "\n",
    "    # menampilkan pseudo-RGB (karena data hyperspectral)\n",
    "    B = x.shape[2]\n",
    "    b1, b2, b3 = int(B*0.05), int(B*0.5), int(B*0.9)\n",
    "    rgb = x[..., [b1, b2, b3]]\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "\n",
    "    # Coba baca colormap dari file JSON\n",
    "    if json_path and os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            label_info = json.load(f)\n",
    "        custom_colors = [c[\"color\"][:7] for c in label_info]\n",
    "        cmap = ListedColormap(custom_colors)\n",
    "    else:\n",
    "        print(\"File json tidak terbaca, menggunakan cmap tab20\")\n",
    "        cmap = \"tab20\"  # fallback\n",
    "\n",
    "    # Visualisasi\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.imshow(rgb_norm); plt.title(\"Citra (Pseudo-RGB)\")\n",
    "    if y_true is not None:\n",
    "        plt.subplot(1,3,2); plt.imshow(y_true, cmap=cmap); plt.title(\"Ground Truth\")\n",
    "    if y_pred is not None:\n",
    "        plt.subplot(1,3,3); plt.imshow(y_pred, cmap=cmap); plt.title(\"Prediksi\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7aceb2-d31e-42b5-ad5f-9d5f1ac74534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 3. Dataset Loader\n",
    "# ===========================================\n",
    "\n",
    "def load_label_mapping(json_path):\n",
    "    \"\"\"Membaca file label_classes.json untuk mapping id ke nama kelas\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    idx_to_name = {i: item[\"name\"] for i, item in enumerate(data)}\n",
    "    return idx_to_name\n",
    "\n",
    "def normalize_reflectance(cube):\n",
    "    \"\"\"Menormalkan reflektansi 0â€“1 per tile, hemat RAM, aman untuk mmap read-only.\"\"\"\n",
    "    if not cube.flags.writeable:\n",
    "        cube = cube.astype(np.float32, copy=True)\n",
    "\n",
    "    if cube.dtype != np.float32:\n",
    "        cube = cube.astype(np.float32, copy=False)\n",
    "\n",
    "    np.nan_to_num(cube, copy=False)\n",
    "\n",
    "    min_val = np.nanmin(cube)\n",
    "    max_val = np.nanmax(cube)\n",
    "    if max_val > min_val:\n",
    "        cube -= min_val\n",
    "        cube /= (max_val - min_val + 1e-8)\n",
    "\n",
    "    return cube\n",
    "\n",
    "\n",
    "class SeaweedDataset(Dataset):\n",
    "    \"\"\"Dataset hemat memori berbasis file .npy hasil konversi.\"\"\"\n",
    "    def __init__(self, data_files, label_map, tile_size=64, normalize=True, label_remap=None):\n",
    "        self.data_files = data_files\n",
    "        self.label_map = label_map\n",
    "        self.tile_size = tile_size\n",
    "        self.normalize = normalize\n",
    "        self.label_remap = label_remap\n",
    "\n",
    "        # Daftar pasangan (file_x, file_y)\n",
    "        self.pairs = []\n",
    "        for f in data_files:\n",
    "            if f.endswith(\"_x.npy\"):\n",
    "                fy = f.replace(\"_x.npy\", \"_y.npy\")\n",
    "                if os.path.exists(fy):\n",
    "                    self.pairs.append((f, fy))\n",
    "        \n",
    "        # SOLUSI 3: Pre-filter tile kosong\n",
    "        self.index = []\n",
    "        print(\"[INFO] Pre-filtering empty tiles...\")\n",
    "        \n",
    "        empty_count = 0\n",
    "        valid_count = 0\n",
    "        \n",
    "        for file_idx, (fx, fy) in enumerate(self.pairs):\n",
    "            x = np.load(fx, mmap_mode=\"r\")\n",
    "            y = np.load(fy, mmap_mode=\"r\")\n",
    "            H, W, _ = x.shape\n",
    "            \n",
    "            for i in range(0, H - tile_size + 1, tile_size):\n",
    "                for j in range(0, W - tile_size + 1, tile_size):\n",
    "                    y_tile = y[i:i+tile_size, j:j+tile_size]\n",
    "                    \n",
    "                    if np.any(y_tile > 0):\n",
    "                        self.index.append((file_idx, i, j))\n",
    "                        valid_count += 1\n",
    "                    else:\n",
    "                        empty_count += 1\n",
    "            del x, y\n",
    "            \n",
    "        print(f\"[INFO] Total tile valid: {valid_count}\")\n",
    "        print(f\"[INFO] Total tile empty (filtered): {empty_count}\")\n",
    "        print(f\"[INFO] Ratio valid/total: {valid_count/(valid_count+empty_count)*100:.2f}%\")\n",
    "\n",
    "        print(f\"[INFO] Total tile terdaftar: {len(self.index)} dari {len(self.pairs)} file\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, i, j = self.index[idx]\n",
    "        fx, fy = self.pairs[file_idx]\n",
    "        \n",
    "        # Memuat tile menggunakan mmap (sudah melalui pre-filtered guna mendapatkan kondisi non-empty)\n",
    "        x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "        y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "        if self.normalize:\n",
    "            x = normalize_reflectance(x)\n",
    "\n",
    "        # REMAP label bila mapping diberikan\n",
    "        if self.label_remap is not None:\n",
    "            y_remap = np.zeros_like(y, dtype=np.int64)\n",
    "            for orig_label, new_idx in self.label_remap.items():\n",
    "                y_remap[y == orig_label] = new_idx\n",
    "            y = y_remap\n",
    "        else:\n",
    "            y = y.astype(np.int64)\n",
    "\n",
    "        # Konversi ke tensor\n",
    "        x_tensor = torch.tensor(x.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "\n",
    "def detect_actual_classes(pairs):\n",
    "    \"\"\"Scan semua file y.npy untuk mendeteksi kelas yang benar-benar ada\"\"\"\n",
    "    found = set()\n",
    "    for _, fy in pairs:\n",
    "        y = np.load(fy, mmap_mode=\"r\")\n",
    "        found |= set(np.unique(y))\n",
    "    found = sorted(list(found))\n",
    "    print(f\"[INFO] Kelas AKTUAL yang ditemukan di dataset: {found}\")\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56205d90-81a4-4c31-9ccc-c0db25614cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total kelas di JSON: 41\n",
      "Total pasangan file X-Y ditemukan: 18\n",
      "\n",
      "=== FINAL SPLIT PER FILE ===\n",
      "Train : 11\n",
      "Val   : 5\n",
      "Test  : 2\n",
      "[INFO] Kelas AKTUAL yang ditemukan di dataset: [np.int32(0), np.int32(8), np.int32(12), np.int32(13), np.int32(14), np.int32(18), np.int32(38)]\n",
      "[INFO] Label remap (orig -> new): {0: 0, 8: 1, 12: 2, 13: 3, 14: 4, 18: 5, 38: 6}\n",
      "[INFO] Pre-filtering empty tiles...\n",
      "[INFO] Total tile valid: 5395\n",
      "[INFO] Total tile empty (filtered): 12805\n",
      "[INFO] Ratio valid/total: 29.64%\n",
      "[INFO] Total tile terdaftar: 5395 dari 11 file\n",
      "[INFO] Pre-filtering empty tiles...\n",
      "[INFO] Total tile valid: 5698\n",
      "[INFO] Total tile empty (filtered): 2898\n",
      "[INFO] Ratio valid/total: 66.29%\n",
      "[INFO] Total tile terdaftar: 5698 dari 5 file\n",
      "[INFO] Pre-filtering empty tiles...\n",
      "[INFO] Total tile valid: 1710\n",
      "[INFO] Total tile empty (filtered): 950\n",
      "[INFO] Ratio valid/total: 64.29%\n",
      "[INFO] Total tile terdaftar: 1710 dari 2 file\n",
      "[INFO] Pixel counts per class: {0: 14185638, 1: 840140, 2: 1566138, 3: 808104, 4: 36978, 5: 139337, 6: 1297565}\n",
      "[INFO] Class weights: [0.         0.2185292  0.11722793 0.22719245 4.9649825  1.3176336\n",
      " 0.14149204]\n",
      "\n",
      "Total TILE train: 5395, val: 5698, test: 1710\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 4. Load Dataset dan Splitting\n",
    "# ===========================================\n",
    "\n",
    "data_dir = \"../data/npy_converted\"\n",
    "label_json_path = \"../data/annotation/segmentation_masks/label_classes.json\"\n",
    "\n",
    "label_map = load_label_mapping(label_json_path)\n",
    "print(f\"Jumlah total kelas di JSON: {len(label_map)}\")\n",
    "\n",
    "# Ambil semua file _x.npy\n",
    "all_x_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\"_x.npy\")])\n",
    "pairs = [(fx, fx.replace(\"_x.npy\", \"_y.npy\")) for fx in all_x_files if os.path.exists(fx.replace(\"_x.npy\", \"_y.npy\"))]\n",
    "\n",
    "print(f\"Total pasangan file X-Y ditemukan: {len(pairs)}\")\n",
    "\n",
    "# Split deterministik berbasis urutan nama (11 train, 5 val, 2 test)\n",
    "train_pairs = pairs[:11]\n",
    "val_pairs   = pairs[11:16]\n",
    "test_pairs  = pairs[16:]\n",
    "\n",
    "print(\"\\n=== FINAL SPLIT PER FILE ===\")\n",
    "print(f\"Train : {len(train_pairs)}\")\n",
    "print(f\"Val   : {len(val_pairs)}\")\n",
    "print(f\"Test  : {len(test_pairs)}\")\n",
    "\n",
    "# DETEKSI kelas aktual\n",
    "actual_classes = detect_actual_classes(train_pairs + val_pairs + test_pairs)\n",
    "orig_classes = [int(x) for x in actual_classes]\n",
    "label_remap = {orig: idx for idx, orig in enumerate(orig_classes)}\n",
    "print(f\"[INFO] Label remap (orig -> new): {label_remap}\")\n",
    "\n",
    "# PENTING: Ubah tile_size ke 64 untuk menghemat memory\n",
    "TILE_SIZE = 32  # Turun dari 64\n",
    "\n",
    "train_dataset = SeaweedDataset([p[0] for p in train_pairs], label_map, tile_size=TILE_SIZE, label_remap=label_remap)\n",
    "val_dataset   = SeaweedDataset([p[0] for p in val_pairs], label_map, tile_size=TILE_SIZE, label_remap=label_remap)\n",
    "test_dataset  = SeaweedDataset([p[0] for p in test_pairs], label_map, tile_size=TILE_SIZE, label_remap=label_remap, normalize=False)\n",
    "\n",
    "# Hitung class weights\n",
    "counter = Counter()\n",
    "for _, fy in train_pairs:\n",
    "    y = np.load(fy, mmap_mode=\"r\")\n",
    "    for orig, new in label_remap.items():\n",
    "        cnt = int((y == orig).sum())\n",
    "        counter[new] += cnt\n",
    "\n",
    "print(f\"[INFO] Pixel counts per class: {dict(counter)}\")\n",
    "\n",
    "counts = np.array([counter.get(i, 0) for i in range(len(label_remap))], dtype=np.float64)\n",
    "eps = 1e-6\n",
    "inv_freq = 1.0 / (counts + eps)\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "inv_freq[0] = 0.0  # ignore background\n",
    "\n",
    "class_weights_np = inv_freq.astype(np.float32)\n",
    "print(f\"[INFO] Class weights: {class_weights_np}\")\n",
    "\n",
    "num_classes_actual = len(label_remap)\n",
    "print(f\"\\nTotal TILE train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aece8dd7-b31b-4785-89aa-8ea6a972b127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band input aktual: 300\n",
      "Total parameters: 1,629,767\n",
      "OptimizedFCHybridSN(\n",
      "  (conv3d_1): Conv3d(1, 16, kernel_size=(7, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3d_2): Conv3d(16, 32, kernel_size=(5, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3d_3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (spectral_pool): AdaptiveAvgPool3d(output_size=(8, None, None))\n",
      "  (conv2d_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv2d_2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv2d_3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Conv2d(64, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 5. Model OPTIMIZED HybridSN\n",
    "# ===========================================\n",
    "\n",
    "class OptimizedFCHybridSN(nn.Module):\n",
    "    \"\"\"Versi optimized dari HybridSN dengan spectral pooling\"\"\"\n",
    "    def __init__(self, in_bands=300, num_classes=7):\n",
    "        super().__init__()\n",
    "        # 3D Convolution layers\n",
    "        self.conv3d_1 = nn.Conv3d(1, 16, (7,3,3), padding=(0,1,1))\n",
    "        self.bn3d_1 = nn.BatchNorm3d(16)\n",
    "        \n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, (5,3,3), padding=(0,1,1))\n",
    "        self.bn3d_2 = nn.BatchNorm3d(32)\n",
    "        \n",
    "        self.conv3d_3 = nn.Conv3d(32, 64, (3,3,3), padding=(0,1,1))\n",
    "        self.bn3d_3 = nn.BatchNorm3d(64)\n",
    "\n",
    "        # PERBAIKAN: Tambahkan spectral pooling\n",
    "        self.spectral_pool = nn.AdaptiveAvgPool3d((8, None, None))\n",
    "        \n",
    "        # 2D Convolution layers (input channel jauh lebih kecil sekarang)\n",
    "        self.conv2d_1 = nn.Conv2d(64 * 8, 256, 3, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(256)\n",
    "        self.dropout1 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(128)\n",
    "        self.dropout2 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        self.conv2d_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.bn2d_3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.classifier = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, Bands, H, W = x.shape\n",
    "        \n",
    "        # 3D CNN processing\n",
    "        x3 = x.unsqueeze(1)\n",
    "        x3 = F.relu(self.bn3d_1(self.conv3d_1(x3)))\n",
    "        x3 = F.relu(self.bn3d_2(self.conv3d_2(x3)))\n",
    "        x3 = F.relu(self.bn3d_3(self.conv3d_3(x3)))\n",
    "        \n",
    "        # PERBAIKAN: Spectral pooling\n",
    "        x3 = self.spectral_pool(x3)\n",
    "        \n",
    "        # Reshape ke 2D\n",
    "        B, C3, reduced_spec, H, W = x3.shape\n",
    "        x2 = x3.view(B, C3 * reduced_spec, H, W)\n",
    "        \n",
    "        # 2D CNN processing\n",
    "        x2 = self.dropout1(F.relu(self.bn2d_1(self.conv2d_1(x2))))\n",
    "        x2 = self.dropout2(F.relu(self.bn2d_2(self.conv2d_2(x2))))\n",
    "        x2 = F.relu(self.bn2d_3(self.conv2d_3(x2)))\n",
    "        \n",
    "        return self.classifier(x2)\n",
    "\n",
    "\n",
    "# Ambil jumlah band dari data\n",
    "sample_x = np.load(train_pairs[0][0], mmap_mode=\"r\")\n",
    "in_bands_actual = sample_x.shape[2]\n",
    "print(f\"Band input aktual: {in_bands_actual}\")\n",
    "\n",
    "model = OptimizedFCHybridSN(in_bands=in_bands_actual, num_classes=num_classes_actual).to(device)\n",
    "\n",
    "# Hitung parameter\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd193f5a-4de7-4ff4-a3bb-926610d2cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Original class weights: {class_weights_np}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b475646-f001-4075-b0cd-89e9d142e3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Original class weights: [0.         0.2185292  0.11722793 0.22719245 4.9649825  1.3176336\n",
      " 0.14149204]\n",
      "[INFO] Smoothed class weights: [0.         0.46747106 0.34238565 0.47664708 2.228224   1.1478822\n",
      " 0.37615427]\n",
      "Setup selesai!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\algae\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 6. Loss, Optimizer, dan Metrics (STABLE + SQRT SMOOTHING)\n",
    "# ===========================================\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Enable cudnn benchmark\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 5e-5 # turun dari 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 1\n",
    "ACCUMULATION_STEPS = 8  # Efektif batch size = 8\n",
    "CLIP_NORM = 0.5 # Turun dari 1.0\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# PERBAIKAN: Smoothed class weights untuk stabilitas\n",
    "print(f\"[INFO] Original class weights: {class_weights_np}\")\n",
    "\n",
    "# Smoothing varian sqrt untuk smoothing\n",
    "class_weights_smoothed = np.sqrt(class_weights_np)\n",
    "class_weights_smoothed[0] = 0.0\n",
    "\n",
    "print(f\"[INFO] Smoothed class weights: {class_weights_smoothed}\")\n",
    "\n",
    "# Versi loss function dengan class weights\n",
    "weight_tensor = torch.from_numpy(class_weights_smoothed).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor, ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "# Versi loss function criterion non-class-weight\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "# Optimizer & scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-8)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True, min_lr=1e-7)\n",
    "\n",
    "\n",
    "# PERBAIKAN: Metrik evaluasi yang benar\n",
    "class SegmentationMetrics:\n",
    "    def __init__(self, num_classes, ignore_index=0):\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total_intersection = torch.zeros(self.num_classes)\n",
    "        self.total_union = torch.zeros(self.num_classes)\n",
    "        self.total_correct = 0\n",
    "        self.total_pixels = 0\n",
    "    \n",
    "    def update(self, pred, target):\n",
    "        \"\"\"Update metrics dengan batch baru\"\"\"\n",
    "        valid = (target != self.ignore_index)\n",
    "        pred = pred[valid]\n",
    "        target = target[valid]\n",
    "        \n",
    "        # Pixel accuracy\n",
    "        self.total_correct += (pred == target).sum().item()\n",
    "        self.total_pixels += valid.sum().item()\n",
    "        \n",
    "        # IoU per class\n",
    "        for cls in range(self.num_classes):\n",
    "            pred_i = (pred == cls)\n",
    "            target_i = (target == cls)\n",
    "            intersection = (pred_i & target_i).sum().item()\n",
    "            union = (pred_i | target_i).sum().item()\n",
    "            \n",
    "            self.total_intersection[cls] += intersection\n",
    "            self.total_union[cls] += union\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Hitung metrik final\"\"\"\n",
    "        pixel_acc = self.total_correct / (self.total_pixels + 1e-9)\n",
    "        \n",
    "        iou_per_class = self.total_intersection / (self.total_union + 1e-9)\n",
    "        # Exclude background (index 0) dan kelas yang tidak muncul\n",
    "        valid_ious = []\n",
    "        for i in range(1, self.num_classes):\n",
    "            if self.total_union[i] > 0:\n",
    "                valid_ious.append(iou_per_class[i].item())\n",
    "        \n",
    "        mean_iou = np.mean(valid_ious) if valid_ious else 0.0\n",
    "        \n",
    "        return pixel_acc, mean_iou, iou_per_class.numpy()\n",
    "\n",
    "print(\"Setup selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d7fa5-ccc0-4e19-86e4-2347271b31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST MEMORY - Jalankan sebelum training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_x = torch.randn(1, in_bands_actual, TILE_SIZE, TILE_SIZE).to(device)\n",
    "    dummy_y = model(dummy_x)\n",
    "    print(f\"Test passed! Output shape: {dummy_y.shape}\")\n",
    "    print(f\"Memory used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    del dummy_x, dummy_y\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db874dfe-32b4-46d0-ae5e-d0d2310f4f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training dari awal\n",
      "\n",
      "============================================================\n",
      "Epoch 1/2\n",
      "============================================================\n",
      "GPU Memory: 0.01 GB allocated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 1.8991 | Train Acc: 0.3833\n",
      "  Val Loss  : 1.8985 | Val Acc  : 0.4634\n",
      "  Val mIoU  : 0.1077\n",
      "  Time      : 4.15 min\n",
      "  IoU per class: [0.         0.03565709 0.         0.         0.         0.50302017]\n",
      "[OK] Best model saved! (mIoU: 0.1077)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/2\n",
      "============================================================\n",
      "GPU Memory: 0.03 GB allocated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 1.6945 | Train Acc: 0.5737\n",
      "  Val Loss  : 1.8632 | Val Acc  : 0.0623\n",
      "  Val mIoU  : 0.0263\n",
      "  Time      : 3.43 min\n",
      "  IoU per class: [0.         0.03460516 0.00362831 0.         0.         0.04056756]\n",
      "\n",
      "============================================================\n",
      "Training selesai!\n",
      "Best validation mIoU: 0.1077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 7. Training Loop (OPTIMIZED)\n",
    "# ===========================================\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "START_EPOCH = 1\n",
    "NUM_EPOCHS = 2\n",
    "best_val_miou = 0.0\n",
    "\n",
    "checkpoint_path = \"hybridsn_sgmt_ver3_checkpoint.pth\"\n",
    "best_model_path = \"hybridsn_sgmt_ver3_best_model.pth\"\n",
    "\n",
    "# Load checkpoint jika ada\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    START_EPOCH = checkpoint[\"epoch\"] + 1\n",
    "    best_val_miou = checkpoint.get(\"best_val_miou\", 0.0)\n",
    "    print(f\"[INFO] Resume dari epoch {START_EPOCH}\")\n",
    "else:\n",
    "    print(\"[INFO] Training dari awal\")\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"val_miou\": []}\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, metrics, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    metrics.reset()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    nan_count = 0  # Counter NaN\n",
    "    \n",
    "    for i, (xb, yb) in enumerate(pbar):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        # SOLUSI 1: Skip jika semua background (safety net)\n",
    "        if torch.all(yb == 0):\n",
    "            continue\n",
    "        \n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb) / ACCUMULATION_STEPS\n",
    "\n",
    "        # SOLUSI 1: CHECK NaN\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            nan_count += 1\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item() * ACCUMULATION_STEPS * xb.size(0)\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        metrics.update(preds, yb)\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item()*ACCUMULATION_STEPS:.4f}\"})\n",
    "        \n",
    "    if nan_count > 0:\n",
    "        print(f\"  [INFO] Skipped {nan_count} NaN batches in training\")\n",
    "    \n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    pixel_acc, _, _ = metrics.get_metrics()\n",
    "    \n",
    "    return avg_loss, pixel_acc\n",
    "\n",
    "def validate(model, loader, criterion, metrics, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    valid_batches = 0\n",
    "    metrics.reset()\n",
    "\n",
    "    nan_count = 0  # Counter NaN\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            # SOLUSI 1: Skip jika semua background (safety net)\n",
    "            if torch.all(yb == 0):\n",
    "                continue\n",
    "            \n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            # SOLUSI 1: CHECK NaN\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                nan_count += 1\n",
    "                continue\n",
    "            \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            valid_batches += 1\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            metrics.update(preds, yb)\n",
    "\n",
    "    if nan_count > 0:\n",
    "        print(f\"  [INFO] Skipped {nan_count} NaN batches in validation\")\n",
    "    \n",
    "    # Hitung average loss\n",
    "    if valid_batches > 0:\n",
    "        avg_loss = running_loss / (valid_batches * loader.batch_size)\n",
    "    else:\n",
    "        avg_loss = float('nan')\n",
    "        print(\"  [WARNING] Semua validation batches di-skip!\")\n",
    "    \n",
    "    pixel_acc, mean_iou, iou_per_class = metrics.get_metrics()\n",
    "    \n",
    "    return avg_loss, pixel_acc, mean_iou, iou_per_class\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(START_EPOCH, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Monitor GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated\")\n",
    "    \n",
    "    # Training\n",
    "    train_metrics = SegmentationMetrics(num_classes_actual, ignore_index=0)\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, train_metrics, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = SegmentationMetrics(num_classes_actual, ignore_index=0)\n",
    "    val_loss, val_acc, val_miou, val_iou_per_class = validate(model, val_loader, criterion, val_metrics, device)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_miou)\n",
    "    \n",
    "    # Logging\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss  : {val_loss:.4f} | Val Acc  : {val_acc:.4f}\")\n",
    "    print(f\"  Val mIoU  : {val_miou:.4f}\")\n",
    "    print(f\"  Time      : {elapsed/60:.2f} min\")\n",
    "    print(f\"  IoU per class: {val_iou_per_class[1:]}\")\n",
    "    \n",
    "    # Save history\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_miou\"].append(val_miou)\n",
    "    \n",
    "    # Checkpoint\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"best_val_miou\": best_val_miou,\n",
    "        \"history\": history\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_miou > best_val_miou:\n",
    "        best_val_miou = val_miou\n",
    "        torch.save(checkpoint, best_model_path)\n",
    "        print(f\"[OK] Best model saved! (mIoU: {best_val_miou:.4f})\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training selesai!\")\n",
    "print(f\"Best validation mIoU: {best_val_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5782b-eb5e-49f2-a514-063a455f07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 8. Plot Training History\n",
    "# ===========================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\", marker='o')\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\", marker='s')\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training vs Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history[\"train_acc\"], label=\"Train Acc\", marker='o')\n",
    "axes[1].plot(history[\"val_acc\"], label=\"Val Acc\", marker='s')\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Pixel Accuracy\")\n",
    "axes[1].set_title(\"Training vs Validation Accuracy\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# mIoU\n",
    "axes[2].plot(history[\"val_miou\"], label=\"Val mIoU\", marker='d', color='green')\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Mean IoU\")\n",
    "axes[2].set_title(\"Validation mIoU\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_history_ver3.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83908b7b-5a3b-4968-975f-ce885dc11d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 9. Testing dan Visualisasi\n",
    "# ===========================================\n",
    "\n",
    "# Load best model\n",
    "best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(best_checkpoint[\"model_state\"])\n",
    "print(f\"Loaded best model from epoch {best_checkpoint['epoch']}\")\n",
    "\n",
    "# Test evaluation\n",
    "test_metrics = SegmentationMetrics(num_classes_actual, ignore_index=0)\n",
    "test_loss, test_acc, test_miou, test_iou_per_class = validate(model, test_loader, criterion, test_metrics, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss     : {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy : {test_acc:.4f}\")\n",
    "print(f\"Test mIoU     : {test_miou:.4f}\")\n",
    "print(f\"IoU per class : {test_iou_per_class[1:]}\")\n",
    "\n",
    "# Visualisasi beberapa prediksi\n",
    "model.eval()\n",
    "num_vis = 3\n",
    "vis_samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xb, yb) in enumerate(test_loader):\n",
    "        if i >= num_vis:\n",
    "            break\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        \n",
    "        vis_samples.append((xb[0], yb[0], preds[0]))\n",
    "\n",
    "# Plot visualisasi\n",
    "for i, (x, y_true, y_pred) in enumerate(vis_samples):\n",
    "    visualize_tile(x, y_true.cpu().numpy(), y_pred.cpu().numpy(), \n",
    "                   json_path=label_json_path, idx=i)\n",
    "\n",
    "print(\"\\nSelesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600d791-fe84-468d-a3d1-abb61f96642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 10. Confusion Matrix\n",
    "# ===========================================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Collect predictions untuk confusion matrix\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(test_loader, desc=\"Computing CM\"):\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        \n",
    "        # Flatten dan filter valid pixels\n",
    "        preds_flat = preds.cpu().numpy().flatten()\n",
    "        targets_flat = yb.numpy().flatten()\n",
    "        \n",
    "        valid = targets_flat != 0  # Exclude background\n",
    "        all_preds.extend(preds_flat[valid])\n",
    "        all_targets.extend(targets_flat[valid])\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds, labels=list(range(1, num_classes_actual)))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(1, num_classes_actual),\n",
    "            yticklabels=range(1, num_classes_actual))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Test Set - Excluding Background)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_ver3.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
