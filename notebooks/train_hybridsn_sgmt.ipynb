{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a95dc77-6786-4eb7-9458-702f78fa2d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device yang digunakan: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# 1. Import Library dan Setup Environment\n",
    "# ===========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Gunakan GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device yang digunakan:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c696af3-5826-4722-bdae-93483555b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 2. Fungsi Bantuan Umum\n",
    "# ===========================================\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Menetapkan seed random agar hasil eksperimen bisa direplikasi\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "def visualize_tile(x_tile, y_true=None, y_pred=None, class_names=None, idx=0):\n",
    "    \"\"\"\n",
    "    Menampilkan citra tile beserta mask ground-truth dan prediksi\n",
    "    \"\"\"\n",
    "    if isinstance(x_tile, torch.Tensor):\n",
    "        x = x_tile.cpu().numpy()\n",
    "        x = np.transpose(x, (1,2,0))  # ubah dari [B,H,W] -> [H,W,B]\n",
    "    else:\n",
    "        x = x_tile\n",
    "\n",
    "    # menampilkan pseudo-RGB (karena data hyperspectral)\n",
    "    B = x.shape[2]\n",
    "    b1, b2, b3 = int(B*0.05), int(B*0.5), int(B*0.9)\n",
    "    rgb = x[..., [b1, b2, b3]]\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.imshow(rgb_norm); plt.title(\"Citra (Pseudo-RGB)\")\n",
    "    if y_true is not None:\n",
    "        plt.subplot(1,3,2); plt.imshow(y_true, cmap='tab20'); plt.title(\"Ground Truth\")\n",
    "    if y_pred is not None:\n",
    "        plt.subplot(1,3,3); plt.imshow(y_pred, cmap='tab20'); plt.title(\"Prediksi\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0898bb81-a9a6-439b-a6f6-380a66272726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 3. Dataset Loader (SeaweedDataset) dan Label Mapping\n",
    "# ====================================================\n",
    "\n",
    "def load_label_mapping(json_path):\n",
    "    \"\"\"Membaca file label_classes.json untuk mapping id ke nama kelas\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    idx_to_name = {i: item[\"name\"] for i, item in enumerate(data)}\n",
    "    return idx_to_name\n",
    "\n",
    "def normalize_reflectance(cube):\n",
    "    \"\"\"Menormalkan nilai reflektansi ke rentang 0-1 per tile\"\"\"\n",
    "    cube = np.nan_to_num(cube).astype(np.float32)\n",
    "    min_val = np.nanmin(cube)\n",
    "    max_val = np.nanmax(cube)\n",
    "    if max_val > min_val:\n",
    "        cube = (cube - min_val) / (max_val - min_val)\n",
    "    return cube\n",
    "\n",
    "class SeaweedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset tile-based hemat memori.\n",
    "    Tile akan dibaca langsung dari file .npz hanya saat diperlukan,\n",
    "    bukan semua dimuat ke RAM sekaligus.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_files, label_map, tile_size=128, normalize=True):\n",
    "        self.data_files = data_files\n",
    "        self.label_map = label_map\n",
    "        self.tile_size = tile_size\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Hanya menyimpan indeks tile, bukan data mentah\n",
    "        self.index = []  # daftar (file_index, i, j)\n",
    "        self.file_shapes = []\n",
    "\n",
    "        for file_idx, path in enumerate(data_files):\n",
    "            with np.load(path) as data:\n",
    "                H, W, _ = data[\"x\"].shape\n",
    "            self.file_shapes.append((H, W))\n",
    "            for i in range(0, H - tile_size + 1, tile_size):\n",
    "                for j in range(0, W - tile_size + 1, tile_size):\n",
    "                    self.index.append((file_idx, i, j))\n",
    "\n",
    "        print(f\"[INFO] Total tile terdaftar: {len(self.index)} dari {len(data_files)} file\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, i, j = self.index[idx]\n",
    "        path = self.data_files[file_idx]\n",
    "        with np.load(path) as data:\n",
    "            x = data[\"x\"][i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "            y = data[\"y\"][i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "        # Abaikan tile kosong (semua 0)\n",
    "        if not np.any(y > 0):\n",
    "            # Jika tile kosong, ambil tile lain secara acak agar batch tetap penuh\n",
    "            return self.__getitem__(np.random.randint(0, len(self.index)))\n",
    "\n",
    "        if self.normalize:\n",
    "            x = normalize_reflectance(x)\n",
    "\n",
    "        x_tensor = torch.tensor(x.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        return x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a8f3df-9e8e-434a-80fa-e38b615578bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total kelas: 41\n",
      "Contoh nama kelas: ['Deep water', 'Substrate', 'Rock', 'Bedrock', 'Boulder']\n",
      "\n",
      "Total file ditemukan: 3\n",
      "massimal_smola_maholmen_202306211129-2_hsi_003_processed.npz -> Label unik: [ 0 13 14 18 38]\n",
      "massimal_smola_maholmen_202306211129-2_hsi_004_processed.npz -> Label unik: [ 0 12]\n",
      "massimal_smola_maholmen_202306211129-2_hsi_008_processed.npz -> Label unik: [ 0 12]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.01 GiB for an array with shape (540000000,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Label unik: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Buat dataset dan dataloader\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSeaweedDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Split train-val (80%-20%)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n",
      "Cell \u001b[1;32mIn[9], line 39\u001b[0m, in \u001b[0;36mSeaweedDataset.__init__\u001b[1;34m(self, data_files, label_map, tile_size, normalize)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_idx, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_files):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(path) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m---> 39\u001b[0m         H, W, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_shapes\u001b[38;5;241m.\u001b[39mappend((H, W))\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, H \u001b[38;5;241m-\u001b[39m tile_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, tile_size):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\algae\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\algae\\lib\\site-packages\\numpy\\lib\\format.py:847\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    834\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfromfile(fp, dtype\u001b[38;5;241m=\u001b[39mdtype, count\u001b[38;5;241m=\u001b[39mcount)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m--> 847\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;66;03m# If dtype.itemsize == 0 then there's nothing more to read\u001b[39;00m\n\u001b[0;32m    851\u001b[0m         max_read_count \u001b[38;5;241m=\u001b[39m BUFFER_SIZE \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmin\u001b[39m(BUFFER_SIZE, dtype\u001b[38;5;241m.\u001b[39mitemsize)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.01 GiB for an array with shape (540000000,) and data type float32"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# 4. Load Dataset dan Splitting\n",
    "# ===========================================\n",
    "\n",
    "data_dir = \"../data/processed\"  # lokasi data hasil preprocessing\n",
    "label_json_path = \"../data/annotation/segmentation_masks/label_classes.json\"  # lokasi file label_classes.json\n",
    "\n",
    "label_map = load_label_mapping(label_json_path)\n",
    "print(f\"Jumlah total kelas: {len(label_map)}\")\n",
    "print(\"Contoh nama kelas:\", list(label_map.values())[:5])\n",
    "\n",
    "# Ambil sebagian file untuk uji awal agar tidak kehabisan memori\n",
    "N_FILES_TO_LOAD = 3   # tingkatkan kalau sudah stabil\n",
    "all_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".npz\")])[:N_FILES_TO_LOAD]\n",
    "print(f\"\\nTotal file ditemukan: {len(all_files)}\")\n",
    "\n",
    "# Tampilkan label unik per file (untuk verifikasi)\n",
    "for f in all_files:\n",
    "    data = np.load(f)\n",
    "    mask = data[\"y\"]\n",
    "    unique_labels = np.unique(mask)\n",
    "    print(f\"{os.path.basename(f)} -> Label unik: {unique_labels}\")\n",
    "\n",
    "# Buat dataset dan dataloader\n",
    "dataset = SeaweedDataset(all_files, label_map, tile_size=128)\n",
    "\n",
    "# Split train-val (80%-20%)\n",
    "n = len(dataset)\n",
    "idxs = list(range(n))\n",
    "random.shuffle(idxs)\n",
    "split = int(0.8 * n)\n",
    "train_idx, val_idx = idxs[:split], idxs[split:]\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=4, sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
    "val_loader   = DataLoader(dataset, batch_size=4, sampler=torch.utils.data.SubsetRandomSampler(val_idx))\n",
    "\n",
    "num_classes = len(label_map)\n",
    "print(f\"\\nJumlah tile train: {len(train_idx)}, val: {len(val_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82057837-2b53-467d-8fb5-508cdc66b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# 5. Model Fully Convolutional HybridSN (3D+2D CNN)\n",
    "# =================================================\n",
    "\n",
    "class FCHybridSN(nn.Module):\n",
    "    def __init__(self, in_bands=300, num_classes=41):\n",
    "        super().__init__()\n",
    "        self.conv3d_1 = nn.Conv3d(1, 16, (7,3,3), padding=(0,1,1))\n",
    "        self.bn3d_1 = nn.BatchNorm3d(16)\n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, (5,3,3), padding=(0,1,1))\n",
    "        self.bn3d_2 = nn.BatchNorm3d(32)\n",
    "        self.conv3d_3 = nn.Conv3d(32, 64, (3,3,3), padding=(0,1,1))\n",
    "        self.bn3d_3 = nn.BatchNorm3d(64)\n",
    "\n",
    "        self._out_spec = in_bands - 12\n",
    "        mid_ch = 256\n",
    "        self.conv2d_1 = nn.Conv2d(64 * max(1, self._out_spec), mid_ch, 3, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2d_2 = nn.Conv2d(mid_ch, 128, 3, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(128)\n",
    "        self.conv2d_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.bn2d_3 = nn.BatchNorm2d(64)\n",
    "        self.classifier = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, Bands, H, W = x.shape\n",
    "        x3 = x.unsqueeze(1)\n",
    "        x3 = F.relu(self.bn3d_1(self.conv3d_1(x3)))\n",
    "        x3 = F.relu(self.bn3d_2(self.conv3d_2(x3)))\n",
    "        x3 = F.relu(self.bn3d_3(self.conv3d_3(x3)))\n",
    "        B, C3, out_spec, H, W = x3.shape\n",
    "        x2 = x3.view(B, C3 * out_spec, H, W)\n",
    "        x2 = F.relu(self.bn2d_1(self.conv2d_1(x2)))\n",
    "        x2 = F.relu(self.bn2d_2(self.conv2d_2(x2)))\n",
    "        x2 = F.relu(self.bn2d_3(self.conv2d_3(x2)))\n",
    "        return self.classifier(x2)\n",
    "\n",
    "model = FCHybridSN(in_bands=300, num_classes=num_classes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b0701-198a-4987-8da8-2ee78970472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 6. Fungsi Training dan Evaluasi\n",
    "# ===========================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "def pixel_accuracy(pred, target):\n",
    "    valid = (target >= 0)\n",
    "    correct = (pred[valid] == target[valid]).sum()\n",
    "    total = valid.sum()\n",
    "    return (correct.float() / (total.float() + 1e-9)).item()\n",
    "\n",
    "def iou_per_class(pred, target, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_i = (pred == cls)\n",
    "        target_i = (target == cls)\n",
    "        inter = (pred_i & target_i).sum()\n",
    "        union = (pred_i | target_i).sum()\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append((inter.float() / union.float()).item())\n",
    "    return ious\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e619f6b-0bb7-4088-8458-deef4332bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 7. Loop Training Utama\n",
    "# ===========================================\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validasi\n",
    "    model.eval()\n",
    "    val_accs, val_ious = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb).argmax(dim=1)\n",
    "            val_accs.append(pixel_accuracy(preds, yb))\n",
    "            val_ious.extend(iou_per_class(preds, yb, num_classes))\n",
    "    mean_val_acc = np.nanmean(val_accs)\n",
    "    mean_iou = np.nanmean([v for v in val_ious if not np.isnan(v)])\n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Loss={avg_loss:.4f} | ValAcc={mean_val_acc:.4f} | mIoU={mean_iou:.4f}\")\n",
    "\n",
    "    if mean_val_acc > best_val_acc:\n",
    "        best_val_acc = mean_val_acc\n",
    "        torch.save(model.state_dict(), \"hybridsn_seg_best.pth\")\n",
    "        print(\"OK, Model terbaik disimpan.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909596e5-5dad-49f9-a3d2-2fb43456d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 8. Visualisasi Hasil Prediksi\n",
    "# ===========================================\n",
    "\n",
    "model.load_state_dict(torch.load(\"hybridsn_seg_best.pth\"))\n",
    "model.eval()\n",
    "xb, yb = next(iter(val_loader))\n",
    "xb, yb = xb.to(device), yb.to(device)\n",
    "with torch.no_grad():\n",
    "    preds = model(xb).argmax(dim=1).cpu().numpy()\n",
    "yb_np = yb.cpu().numpy()\n",
    "visualize_tile(xb[0], y_true=yb_np[0], y_pred=preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48711db-7b33-4dec-9a0d-203178d8b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 9. Inferensi Citra Penuh\n",
    "# ===========================================\n",
    "\n",
    "def infer_full_image(model, cube, tile_size=128, stride=96):\n",
    "    model.eval()\n",
    "    H, W, B = cube.shape\n",
    "    out_logits = np.zeros((num_classes, H, W), dtype=np.float32)\n",
    "    count = np.zeros((H, W), dtype=np.float32)\n",
    "    for i in range(0, max(1, H - tile_size + 1), stride):\n",
    "        for j in range(0, max(1, W - tile_size + 1), stride):\n",
    "            tile = cube[i:i+tile_size, j:j+tile_size, :]\n",
    "            if tile.shape[0] < tile_size or tile.shape[1] < tile_size:\n",
    "                continue\n",
    "            x = torch.from_numpy(np.transpose(tile, (2,0,1))).unsqueeze(0).to(device).float()\n",
    "            with torch.no_grad():\n",
    "                probs = F.softmax(model(x), dim=1).cpu().numpy()[0]\n",
    "            out_logits[:, i:i+tile_size, j:j+tile_size] += probs\n",
    "            count[i:i+tile_size, j:j+tile_size] += 1\n",
    "    count[count==0] = 1.0\n",
    "    pred_map = (out_logits / count[np.newaxis,...]).argmax(axis=0)\n",
    "    return pred_map\n",
    "\n",
    "p = all_files[0]\n",
    "cube = np.load(p, allow_pickle=True)[\"x\"]\n",
    "predmap = infer_full_image(model, cube)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(predmap, cmap='tab20')\n",
    "plt.title(f\"Hasil Inferensi: {os.path.basename(p)}\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
