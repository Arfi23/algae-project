{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2a8fb6-84b5-449d-9a39-dd9c8fe40e5f",
   "metadata": {},
   "source": [
    "### Ver5 Enhanced with Spectral Attention and Multi-Objective Loss\n",
    "\n",
    "Update di versi 4 :\n",
    "- Ada Spectral Attention\n",
    "- Ada Multi-Objective Loss\n",
    "- Ada mekanisme early stopping\n",
    "- Beberapa peningkatan di proses evaluasi\n",
    "- Perbaikan data splitting dll, class-balanced sampling, disable boundary loss, focal gamma 4.0, reduced dropout, warmup LR scheduler, fixed colormap loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df548c-63e2-4b21-bc3d-ae1335f4d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 1. Import Library dan Setup Environment\n",
    "# ===========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "# Gunakan GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device yang digunakan:\", device)\n",
    "\n",
    "# Monitor GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102f2a8-597a-4f3b-b507-0f240ab0f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 2. Fungsi Bantuan Umum + FIXED Colormap Loader\n",
    "# ===========================================\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Menetapkan seed random agar hasil eksperimen bisa direplikasi\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "def load_colormap_from_json(json_path, label_remap, num_classes):\n",
    "    \"\"\"\n",
    "    FIXED: Load colormap dari label_classes.json dengan mapping yang benar\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path ke label_classes.json\n",
    "        label_remap: Dictionary {original_label: new_index}\n",
    "        num_classes: Jumlah kelas setelah remapping\n",
    "    \n",
    "    Returns:\n",
    "        ListedColormap dengan warna yang sesuai\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(\"[WARNING] label_classes.json not found! Using default colors\")\n",
    "        return ListedColormap(['black'] + [plt.cm.tab20(i) for i in range(num_classes-1)])\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        label_info = json.load(f)\n",
    "    \n",
    "    # Reverse mapping: new_idx -> orig_label\n",
    "    reverse_map = {v: k for k, v in label_remap.items()}\n",
    "    \n",
    "    # Mapping nama kelas ke warna (berdasarkan original dataset indices)\n",
    "    # Original indices: {0, 8, 12, 13, 14, 18, 38}\n",
    "    name_to_color = {}\n",
    "    for cls_info in label_info:\n",
    "        name_to_color[cls_info['name']] = cls_info['color'][:7]\n",
    "    \n",
    "    # Build color list\n",
    "    colors = ['#000000']  # Background (index 0) always black\n",
    "    \n",
    "    # Manual mapping berdasarkan data Anda:\n",
    "    # 0 → 0: Deep water (hitam)\n",
    "    # 8 → 1: Sand (coklat muda)\n",
    "    # 12 → 2: Kelp (coklat)\n",
    "    # 13 → 3: Laminaria hyperborea (ungu)\n",
    "    # 14 → 4: Laminaria digitata (biru)\n",
    "    # 18 → 5: Rockweed (hijau)\n",
    "    # 38 → 6: Trawl track (merah)\n",
    "    \n",
    "    class_mapping = {\n",
    "        0: 'Deep water',\n",
    "        8: 'Sand',\n",
    "        12: 'Kelp',\n",
    "        13: 'Laminaria hyperborea',\n",
    "        14: 'Laminaria digitata',\n",
    "        18: 'Rockweed',\n",
    "        38: 'Trawl track'\n",
    "    }\n",
    "    \n",
    "    for new_idx in range(1, num_classes):\n",
    "        if new_idx not in reverse_map:\n",
    "            colors.append('#808080')  # Gray fallback\n",
    "            continue\n",
    "        \n",
    "        orig_label = reverse_map[new_idx]\n",
    "        class_name = class_mapping.get(orig_label, 'Unknown')\n",
    "        \n",
    "        color = name_to_color.get(class_name, '#808080')\n",
    "        colors.append(color)\n",
    "    \n",
    "    print(f\"[INFO] Loaded colormap with {len(colors)} colors\")\n",
    "    for i, (idx, color) in enumerate(zip(range(num_classes), colors)):\n",
    "        if idx in reverse_map:\n",
    "            orig = reverse_map[idx]\n",
    "            name = class_mapping.get(orig, 'Unknown')\n",
    "            print(f\"  Class {idx} ({name}): {color}\")\n",
    "    \n",
    "    return ListedColormap(colors)\n",
    "\n",
    "\n",
    "def visualize_tile(x_tile, y_true=None, y_pred=None, cmap=None, idx=0):\n",
    "    \"\"\"\n",
    "    UPDATED: Menampilkan citra tile dengan colormap yang benar\n",
    "    \"\"\"\n",
    "    if isinstance(x_tile, torch.Tensor):\n",
    "        x = x_tile.cpu().numpy()\n",
    "        x = np.transpose(x, (1,2,0))\n",
    "    else:\n",
    "        x = x_tile\n",
    "\n",
    "    # Pseudo-RGB\n",
    "    B = x.shape[2]\n",
    "    b1, b2, b3 = int(B*0.05), int(B*0.5), int(B*0.9)\n",
    "    rgb = x[..., [b1, b2, b3]]\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = \"tab20\"\n",
    "\n",
    "    # Visualisasi\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    axes[0].imshow(rgb_norm)\n",
    "    axes[0].set_title(\"Pseudo-RGB\", fontsize=11, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    if y_true is not None:\n",
    "        im1 = axes[1].imshow(y_true, cmap=cmap, vmin=0, vmax=6)\n",
    "        axes[1].set_title(\"Ground Truth\", fontsize=11, fontweight='bold')\n",
    "        axes[1].axis('off')\n",
    "        cbar1 = plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "        cbar1.set_label('Class', rotation=270, labelpad=15)\n",
    "    \n",
    "    if y_pred is not None:\n",
    "        im2 = axes[2].imshow(y_pred, cmap=cmap, vmin=0, vmax=6)\n",
    "        axes[2].set_title(\"Prediction\", fontsize=11, fontweight='bold')\n",
    "        axes[2].axis('off')\n",
    "        cbar2 = plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "        cbar2.set_label('Class', rotation=270, labelpad=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810eb07d-2006-49c3-a9b3-6edb995f13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 2b. [NEW] File 003 Splitting Functions\n",
    "# ===========================================\n",
    "\n",
    "def split_file_003_spatially(file_x, file_y, label_remap, \n",
    "                             train_ratio=0.6, val_ratio=0.2,\n",
    "                             output_dir=\"../data/npy_converted_split\",\n",
    "                             method='auto'):\n",
    "    \"\"\"\n",
    "    Split file 003 into train/val/test SPATIALLY\n",
    "    \n",
    "    Ensures that critical classes (14->4 and 18->5) appear in all splits.\n",
    "    Tries horizontal split first, then vertical if needed.\n",
    "    \n",
    "    Args:\n",
    "        file_x: Path to _x.npy (reflectance cube)\n",
    "        file_y: Path to _y.npy (label mask)\n",
    "        label_remap: Class remapping dict {original -> new_index}\n",
    "        train_ratio: Proportion for training (default 0.6)\n",
    "        val_ratio: Proportion for validation (default 0.2)\n",
    "        output_dir: Where to save split files\n",
    "        method: 'auto', 'horizontal', or 'vertical'\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'train': (x_path, y_path), 'val': (...), 'test': (...)}\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If unable to split with all classes in all sets\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SPATIAL SPLIT OF FILE 003 (CRITICAL CLASSES 4 & 5)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load full image\n",
    "    x_full = np.load(file_x)  # [H, W, Bands]\n",
    "    y_full = np.load(file_y)  # [H, W]\n",
    "    \n",
    "    H, W, B = x_full.shape\n",
    "    print(f\"\\nOriginal shape: {x_full.shape}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    unique_classes = np.unique(y_full)\n",
    "    print(f\"Classes in file 003: {unique_classes}\")\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        count = (y_full == cls).sum()\n",
    "        pct = count / y_full.size * 100\n",
    "        print(f\"  Class {cls}: {count:>7} pixels ({pct:>5.2f}%)\")\n",
    "    \n",
    "    # Helper function to check class presence\n",
    "    def check_classes_in_split(y_split, split_name):\n",
    "        unique = np.unique(y_split)\n",
    "        has_14 = 14 in unique\n",
    "        has_18 = 18 in unique\n",
    "        \n",
    "        if has_14 and has_18:\n",
    "            print(f\"  [OK] {split_name}: Classes 14 & 18 present\")\n",
    "        else:\n",
    "            missing = []\n",
    "            if not has_14:\n",
    "                missing.append('14')\n",
    "            if not has_18:\n",
    "                missing.append('18')\n",
    "            print(f\"  ✗ {split_name}: Missing class {', '.join(missing)}\")\n",
    "        \n",
    "        return has_14, has_18\n",
    "    \n",
    "    # Try horizontal split first\n",
    "    def try_horizontal_split():\n",
    "        print(\"\\n[Trying HORIZONTAL split (left-center-right)]\")\n",
    "        train_end = int(W * train_ratio)\n",
    "        val_end = int(W * (train_ratio + val_ratio))\n",
    "        \n",
    "        x_train = x_full[:, :train_end, :] # Left portion\n",
    "        y_train = y_full[:, :train_end]\n",
    "        \n",
    "        x_val = x_full[:, train_end:val_end, :] # Center portion\n",
    "        y_val = y_full[:, train_end:val_end]\n",
    "        \n",
    "        x_test = x_full[:, val_end:, :] # Right portion\n",
    "        y_test = y_full[:, val_end:]\n",
    "\n",
    "        print(f\"  Train: columns 0-{train_end} (shape {x_train.shape})\")\n",
    "        print(f\"  Val:   columns {train_end}-{val_end} (shape {x_val.shape})\")\n",
    "        print(f\"  Test:  columns {val_end}-{W} (shape {x_test.shape})\")\n",
    "        \n",
    "        # Verify all classes present\n",
    "        train_ok = check_classes_in_split(y_train, \"Train\")\n",
    "        val_ok = check_classes_in_split(y_val, \"Val\")\n",
    "        test_ok = check_classes_in_split(y_test, \"Test\")\n",
    "\n",
    "        # Success if all splits have both classes\n",
    "        if all([train_ok[0], train_ok[1], val_ok[0], val_ok[1], test_ok[0], test_ok[1]]):\n",
    "            print(\"  [OK] Horizontal split SUCCESSFUL!\")\n",
    "            return x_train, y_train, x_val, y_val, x_test, y_test, True\n",
    "        else:\n",
    "            print(\"  [WARNING] Horizontal split FAILED\")\n",
    "            return None, None, None, None, None, None, False\n",
    "    \n",
    "    # Try vertical split (top-center-bottom)\n",
    "    def try_vertical_split():\n",
    "        print(\"\\n[Trying VERTICAL split (top-center-bottom)]\")\n",
    "        train_end = int(H * train_ratio)\n",
    "        val_end = int(H * (train_ratio + val_ratio))\n",
    "        \n",
    "        x_train = x_full[:train_end, :, :] # Top portion\n",
    "        y_train = y_full[:train_end, :]\n",
    "        \n",
    "        x_val = x_full[train_end:val_end, :, :] # Center portion\n",
    "        y_val = y_full[train_end:val_end, :]\n",
    "        \n",
    "        x_test = x_full[val_end:, :, :] # Bottom portion\n",
    "        y_test = y_full[val_end:, :]\n",
    "\n",
    "        print(f\"  Train: rows 0-{train_end} (shape {x_train.shape})\")\n",
    "        print(f\"  Val:   rows {train_end}-{val_end} (shape {x_val.shape})\")\n",
    "        print(f\"  Test:  rows {val_end}-{H} (shape {x_test.shape})\")\n",
    "        \n",
    "        # Verify all classes present\n",
    "        train_ok = check_classes_in_split(y_train, \"Train\")\n",
    "        val_ok = check_classes_in_split(y_val, \"Val\")\n",
    "        test_ok = check_classes_in_split(y_test, \"Test\")\n",
    "\n",
    "        # Success if all splits have both classes\n",
    "        if all([train_ok[0], train_ok[1], val_ok[0], val_ok[1], test_ok[0], test_ok[1]]):\n",
    "            print(\"  [OK] Vertical split SUCCESSFUL!\")\n",
    "            return x_train, y_train, x_val, y_val, x_test, y_test, True\n",
    "        else:\n",
    "            print(\"  [WARNING] Vertical split FAILED\")\n",
    "            return None, None, None, None, None, None, False\n",
    "    \n",
    "    # Execute splitting based on method\n",
    "    success = False\n",
    "    \n",
    "    if method == 'horizontal':\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test, success = try_horizontal_split()\n",
    "    \n",
    "    elif method == 'vertical':\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test, success = try_vertical_split()\n",
    "    \n",
    "    elif method == 'auto':\n",
    "        # Try horizontal first, then vertical\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test, success = try_horizontal_split()\n",
    "        \n",
    "        if not success:\n",
    "            x_train, y_train, x_val, y_val, x_test, y_test, success = try_vertical_split()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method: {method}. Use 'auto', 'horizontal', or 'vertical'\")\n",
    "\n",
    "    # If both method fail, raise error\n",
    "    if not success:\n",
    "        raise ValueError(\n",
    "            \"❌ CRITICAL: Unable to split file 003 automatically!\\n\"\n",
    "            \"   Classes 14 & 18 are likely clustered in one region.\\n\"\n",
    "            \"   Solutions:\\n\"\n",
    "            \"   1. Run visualize_file003.ipynb to see distribution\\n\"\n",
    "            \"   2. Adjust split ratios (try 0.5/0.25/0.25 or 0.7/0.15/0.15)\\n\"\n",
    "            \"   3. Use manual polygon selection\\n\"\n",
    "            \"   4. Accept that class 4 & 5 won't be in test (report on val only)\\n\"\n",
    "        )\n",
    "    \n",
    "    # Save split files\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    base_name = \"massimal_smola_maholmen_202306211129-2_hsi_003\"\n",
    "    \n",
    "    # Save train\n",
    "    train_x_path = os.path.join(output_dir, f\"{base_name}_train_x.npy\")\n",
    "    train_y_path = os.path.join(output_dir, f\"{base_name}_train_y.npy\")\n",
    "    np.save(train_x_path, x_train)\n",
    "    np.save(train_y_path, y_train)\n",
    "    \n",
    "    # Save val\n",
    "    val_x_path = os.path.join(output_dir, f\"{base_name}_val_x.npy\")\n",
    "    val_y_path = os.path.join(output_dir, f\"{base_name}_val_y.npy\")\n",
    "    np.save(val_x_path, x_val)\n",
    "    np.save(val_y_path, y_val)\n",
    "    \n",
    "    # Save test\n",
    "    test_x_path = os.path.join(output_dir, f\"{base_name}_test_x.npy\")\n",
    "    test_y_path = os.path.join(output_dir, f\"{base_name}_test_y.npy\")\n",
    "    np.save(test_x_path, x_test)\n",
    "    np.save(test_y_path, y_test)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SPLIT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Files saved to: {output_dir}\")\n",
    "    print(f\"  Train: {os.path.basename(train_x_path)}\")\n",
    "    print(f\"  Val:   {os.path.basename(val_x_path)}\")\n",
    "    print(f\"  Test:  {os.path.basename(test_x_path)}\")\n",
    "    \n",
    "    return {\n",
    "        'train': (train_x_path, train_y_path),\n",
    "        'val': (val_x_path, val_y_path),\n",
    "        'test': (test_x_path, test_y_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9ad80-9f2d-45e0-90d6-8513d8e2efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 3. Dataset Loader + ADAPTIVE CLASS-BALANCED SAMPLING\n",
    "# ===========================================\n",
    "\n",
    "def load_label_mapping(json_path):\n",
    "    \"\"\"Membaca file label_classes.json untuk mapping id ke nama kelas\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    idx_to_name = {i: item[\"name\"] for i, item in enumerate(data)}\n",
    "    return idx_to_name\n",
    "\n",
    "def normalize_reflectance(cube):\n",
    "    \"\"\"\n",
    "    REMOVED: No additional normalization needed!\n",
    "    Reflectance is already 0-1 from preprocessing\n",
    "    \"\"\"\n",
    "    # Just ensure it's float32 and handle NaNs\n",
    "    if not cube.flags.writeable:\n",
    "        cube = cube.astype(np.float32, copy=True)\n",
    "\n",
    "    if cube.dtype != np.float32:\n",
    "        cube = cube.astype(np.float32, copy=False)\n",
    "\n",
    "    np.nan_to_num(cube, copy=False)\n",
    "\n",
    "    # NO MIN-MAX SCALING! Data is already 0-1\n",
    "\n",
    "    return cube\n",
    "\n",
    "\n",
    "class SeaweedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    IMPROVED: Dataset dengan class-balanced sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, data_files, label_map, tile_size=64, normalize=True, label_remap=None, augment=False):\n",
    "        self.data_files = data_files\n",
    "        self.label_map = label_map\n",
    "        self.tile_size = tile_size\n",
    "        self.normalize = normalize\n",
    "        self.label_remap = label_remap\n",
    "        self.augment = augment\n",
    "\n",
    "        # Daftar pasangan (file_x, file_y)\n",
    "        self.pairs = []\n",
    "        for f in data_files:\n",
    "            if f.endswith(\"_x.npy\"):\n",
    "                fy = f.replace(\"_x.npy\", \"_y.npy\")\n",
    "                if os.path.exists(fy):\n",
    "                    self.pairs.append((f, fy))\n",
    "        \n",
    "        # Pre-filter tile kosong\n",
    "        self.index = []\n",
    "        print(\"[INFO] Pre-filtering empty tiles...\")\n",
    "        \n",
    "        empty_count = 0\n",
    "        valid_count = 0\n",
    "        \n",
    "        for file_idx, (fx, fy) in enumerate(self.pairs):\n",
    "            x = np.load(fx, mmap_mode=\"r\")\n",
    "            y = np.load(fy, mmap_mode=\"r\")\n",
    "            H, W, _ = x.shape\n",
    "            \n",
    "            for i in range(0, H - tile_size + 1, tile_size):\n",
    "                for j in range(0, W - tile_size + 1, tile_size):\n",
    "                    y_tile = y[i:i+tile_size, j:j+tile_size]\n",
    "                    \n",
    "                    if np.any(y_tile > 0):\n",
    "                        self.index.append((file_idx, i, j))\n",
    "                        valid_count += 1\n",
    "                    else:\n",
    "                        empty_count += 1\n",
    "            del x, y\n",
    "            \n",
    "        print(f\"[INFO] Total tile valid: {valid_count}\")\n",
    "        print(f\"[INFO] Total tile empty (filtered): {empty_count}\")\n",
    "        print(f\"[INFO] Ratio valid/total: {valid_count/(valid_count+empty_count)*100:.2f}%\")\n",
    "        \n",
    "        # IMPROVEMENT: Adaptive Class-balanced sampling\n",
    "        if self.augment:\n",
    "            print(\"[INFO] Applying ADAPTIVE class-balanced sampling...\")\n",
    "            \n",
    "            # Group tiles by dominant class\n",
    "            class_tiles = defaultdict(list)\n",
    "            \n",
    "            for idx, (file_idx, i, j) in enumerate(self.index):\n",
    "                fy = self.pairs[file_idx][1]\n",
    "                y = np.load(fy, mmap_mode=\"r\")[i:i+tile_size, j:j+tile_size]\n",
    "                \n",
    "                # Remap\n",
    "                if self.label_remap:\n",
    "                    y_remap = np.zeros_like(y, dtype=np.int64)\n",
    "                    for orig, new in self.label_remap.items():\n",
    "                        y_remap[y == orig] = new\n",
    "                    y = y_remap\n",
    "                \n",
    "                # Dominant class (excluding background)\n",
    "                valid_mask = (y > 0)\n",
    "                if valid_mask.sum() > 0:\n",
    "                    dominant_class = np.bincount(y[valid_mask].flatten()).argmax()\n",
    "                else:\n",
    "                    dominant_class = 0\n",
    "                \n",
    "                class_tiles[dominant_class].append((file_idx, i, j))\n",
    "            \n",
    "            # IMPROVED: Adaptive target berdasarkan kelangkaan kelas.\n",
    "            # Pendekatan sampling sebelumnya adalah dengan menetapkan semua kelas harus berjumlah 200 sampel seragam.\n",
    "            # Pendekatan ini bersifat lebih adaptif, tidak default 200 seperti sebelumnya.\n",
    "            balanced_index = []\n",
    "            RARE_THRESHOLD = 100\n",
    "            \n",
    "            for cls in range(1, len(self.label_remap)):  # Skip background\n",
    "                if cls not in class_tiles:\n",
    "                    continue\n",
    "\n",
    "                tiles = class_tiles[cls]\n",
    "                n_tiles = len(tiles)\n",
    "\n",
    "\n",
    "                # Adaptive target:\n",
    "                # - Rare classes (<100): aggressive oversample to 300\n",
    "                # - Medium classes (100-500): sample 250\n",
    "                # - Common classes (>500): undersample to 200\n",
    "\n",
    "                if n_tiles < RARE_THRESHOLD:\n",
    "                    # RARE: Aggressive oversampling\n",
    "                    target = min(n_tiles * 6, 400)  # 6x oversample, cap at 400\n",
    "                    print(f\"  Class {cls} [RARE]: {n_tiles} tiles -> {target} (6x oversample)\")\n",
    "                elif n_tiles < 500:\n",
    "                    # MEDIUM: Moderate sampling\n",
    "                    target = 250\n",
    "                    print(f\"  Class {cls} [MED]:  {n_tiles} tiles → {target}\")\n",
    "                else:\n",
    "                    # COMMON: Light undersampling\n",
    "                    target = 200\n",
    "                    print(f\"  Class {cls} [COM]:  {n_tiles} tiles → {target}\")\n",
    "\n",
    "                # Ensure minimum representation\n",
    "                target = max(target, min(50, n_tiles))\n",
    "                \n",
    "                # Sample\n",
    "                if n_tiles < target:\n",
    "                    # Oversample with replacement\n",
    "                    sampled_idx = np.random.choice(len(tiles), target, replace=True)\n",
    "                else:\n",
    "                    # Undersample\n",
    "                    sampled_idx = np.random.choice(len(tiles), target, replace=False)\n",
    "                \n",
    "                balanced_index.extend([tiles[i] for i in sampled_idx])\n",
    "\n",
    "            \n",
    "            # Add some background tiles (20% of total)\n",
    "            if 0 in class_tiles:\n",
    "                bg_tiles = class_tiles[0]\n",
    "                n_bg = len(balanced_index) // 5  # 20%\n",
    "                n_bg = min(n_bg, len(bg_tiles))\n",
    "                sampled_idx = np.random.choice(len(bg_tiles), n_bg, replace=False)\n",
    "                balanced_index.extend([bg_tiles[i] for i in sampled_idx])\n",
    "                print(f\"  Class 0 (background): {len(bg_tiles)} tiles -> {n_bg}\")\n",
    "            \n",
    "            self.index = balanced_index\n",
    "            print(f\"[INFO] After adaptive balancing: {len(self.index)} tiles\")\n",
    "        \n",
    "        print(f\"[INFO] Total tile terdaftar: {len(self.index)} dari {len(self.pairs)} file\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, i, j = self.index[idx]\n",
    "        fx, fy = self.pairs[file_idx]\n",
    "        \n",
    "        # Memuat tile menggunakan mmap\n",
    "        x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "        y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "        if self.normalize:\n",
    "            x = normalize_reflectance(x)\n",
    "\n",
    "        # REMAP label\n",
    "        if self.label_remap is not None:\n",
    "            y_remap = np.zeros_like(y, dtype=np.int64)\n",
    "            for orig_label, new_idx in self.label_remap.items():\n",
    "                y_remap[y == orig_label] = new_idx\n",
    "            y = y_remap\n",
    "        else:\n",
    "            y = y.astype(np.int64)\n",
    "\n",
    "        # Konversi ke tensor\n",
    "        x_tensor = torch.tensor(x.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        # Data Augmentation\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if random.random() > 0.5:\n",
    "                x_tensor = torch.flip(x_tensor, [2])\n",
    "                y_tensor = torch.flip(y_tensor, [1])\n",
    "            \n",
    "            # Random vertical flip\n",
    "            if random.random() > 0.5:\n",
    "                x_tensor = torch.flip(x_tensor, [1])\n",
    "                y_tensor = torch.flip(y_tensor, [0])\n",
    "            \n",
    "            # Random rotation 90/180/270\n",
    "            k = random.choice([0, 1, 2, 3])\n",
    "            if k > 0:\n",
    "                x_tensor = torch.rot90(x_tensor, k, [1, 2])\n",
    "                y_tensor = torch.rot90(y_tensor, k, [0, 1])\n",
    "        \n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "\n",
    "def detect_actual_classes(pairs):\n",
    "    \"\"\"Scan semua file y.npy untuk mendeteksi kelas yang benar-benar ada\"\"\"\n",
    "    found = set()\n",
    "    for _, fy in pairs:\n",
    "        y = np.load(fy, mmap_mode=\"r\")\n",
    "        found |= set(np.unique(y))\n",
    "    found = sorted(list(found))\n",
    "    print(f\"[INFO] Kelas AKTUAL yang ditemukan di dataset: {found}\")\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e24a3-c23a-4288-a472-a07bfe82d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 4. NOVELTY 1: Spectral Self-Attention Module\n",
    "# ===========================================\n",
    "\n",
    "class SpectralAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral Self-Attention Module\n",
    "    Adaptively learns importance weights for spectral channels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        \n",
    "        avg_out = self.avg_pool(x).view(B, C)\n",
    "        max_out = self.max_pool(x).view(B, C)\n",
    "        \n",
    "        avg_weight = self.fc(avg_out)\n",
    "        max_weight = self.fc(max_out)\n",
    "        \n",
    "        attention_weights = self.sigmoid(avg_weight + max_weight)\n",
    "        \n",
    "        out = x * attention_weights.view(B, C, 1, 1)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe9307-2176-4dfd-b04e-44e7a3e7e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 5. NOVELTY 2: Multi-Objective Loss (with Focal Loss)\n",
    "# ===========================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing class imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=4.0, ignore_index=0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = F.cross_entropy(pred, target, weight=self.alpha, \n",
    "                                  ignore_index=self.ignore_index, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        \n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for region-level consistency\"\"\"\n",
    "    def __init__(self, smooth=1.0, ignore_index=0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        target_one_hot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        if self.ignore_index is not None:\n",
    "            mask = (target != self.ignore_index).unsqueeze(1).float()\n",
    "            pred = pred * mask\n",
    "            target_one_hot = target_one_hot * mask\n",
    "        \n",
    "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
    "        \n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"Boundary Loss for edge precision (DISABLED in this version)\"\"\"\n",
    "    def __init__(self, ignore_index=0):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Return zero loss (disabled)\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    IMPROVED: Multi-Objective Hybrid Loss\n",
    "    - Focal gamma increased to 5.0\n",
    "    - Boundary loss disabled\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_ce=1.0, weight_dice=1.0, weight_boundary=0.0, \n",
    "                 class_weights=None, ignore_index=0, focal_gamma=5.0):\n",
    "        super().__init__()\n",
    "        self.weight_ce = weight_ce\n",
    "        self.weight_dice = weight_dice\n",
    "        self.weight_boundary = weight_boundary\n",
    "        \n",
    "        self.ce_loss = FocalLoss(\n",
    "            alpha=class_weights, \n",
    "            gamma=focal_gamma,\n",
    "            ignore_index=ignore_index\n",
    "        )\n",
    "        self.dice_loss = DiceLoss(ignore_index=ignore_index)\n",
    "        self.boundary_loss = BoundaryLoss(ignore_index=ignore_index)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        loss_ce = self.ce_loss(pred, target)\n",
    "        loss_dice = self.dice_loss(pred, target)\n",
    "        loss_boundary = self.boundary_loss(pred, target)\n",
    "        \n",
    "        total_loss = (\n",
    "            self.weight_ce * loss_ce +\n",
    "            self.weight_dice * loss_dice +\n",
    "            self.weight_boundary * loss_boundary\n",
    "        )\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total': total_loss.item(),\n",
    "            'focal': loss_ce.item(),\n",
    "            'dice': loss_dice.item(),\n",
    "            'boundary': loss_boundary.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ade7a-79f4-409e-8c7b-1b2b47a707f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 6. Enhanced HybridSN + REDUCED Dropout\n",
    "# ===========================================\n",
    "\n",
    "class SpectralAttentionHybridSN(nn.Module):\n",
    "    \"\"\"\n",
    "    IMPROVED: Reduced dropout from 0.3 to 0.2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_bands=300, num_classes=7, attention_reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3D Convolution layers\n",
    "        self.conv3d_1 = nn.Conv3d(1, 16, (7,3,3), padding=(0,1,1))\n",
    "        self.bn3d_1 = nn.BatchNorm3d(16)\n",
    "        \n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, (5,3,3), padding=(0,1,1))\n",
    "        self.bn3d_2 = nn.BatchNorm3d(32)\n",
    "        \n",
    "        self.conv3d_3 = nn.Conv3d(32, 64, (3,3,3), padding=(0,1,1))\n",
    "        self.bn3d_3 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        self.spectral_pool = nn.AdaptiveAvgPool3d((8, None, None))\n",
    "        \n",
    "        self.flatten_channels = 64 * 8\n",
    "        \n",
    "        # Spectral Attention\n",
    "        self.spectral_attention = SpectralAttention(\n",
    "            in_channels=self.flatten_channels,\n",
    "            reduction=attention_reduction\n",
    "        )\n",
    "        \n",
    "        # 2D Convolution layers (REDUCED DROPOUT)\n",
    "        self.conv2d_1 = nn.Conv2d(self.flatten_channels, 256, 3, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(256)\n",
    "        self.dropout1 = nn.Dropout2d(0.2)  # REDUCED from 0.3\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(128)\n",
    "        self.dropout2 = nn.Dropout2d(0.2)  # REDUCED from 0.3\n",
    "        \n",
    "        self.conv2d_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.bn2d_3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.classifier = nn.Conv2d(64, num_classes, 1)\n",
    "        \n",
    "        self.last_attention_weights = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, Bands, H, W = x.shape\n",
    "        \n",
    "        # 3D CNN\n",
    "        x3 = x.unsqueeze(1)\n",
    "        x3 = F.relu(self.bn3d_1(self.conv3d_1(x3)))\n",
    "        x3 = F.relu(self.bn3d_2(self.conv3d_2(x3)))\n",
    "        x3 = F.relu(self.bn3d_3(self.conv3d_3(x3)))\n",
    "        \n",
    "        x3 = self.spectral_pool(x3)\n",
    "        \n",
    "        # Reshape\n",
    "        B, C3, reduced_spec, H, W = x3.shape\n",
    "        x2 = x3.view(B, C3 * reduced_spec, H, W)\n",
    "        \n",
    "        # Spectral Attention\n",
    "        x2, attention_weights = self.spectral_attention(x2)\n",
    "        self.last_attention_weights = attention_weights\n",
    "        \n",
    "        # 2D CNN\n",
    "        x2 = self.dropout1(F.relu(self.bn2d_1(self.conv2d_1(x2))))\n",
    "        x2 = self.dropout2(F.relu(self.bn2d_2(self.conv2d_2(x2))))\n",
    "        x2 = F.relu(self.bn2d_3(self.conv2d_3(x2)))\n",
    "        \n",
    "        out = self.classifier(x2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_attention_weights(self):\n",
    "        return self.last_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65df8a-b176-4f11-9fe3-a1a92e031919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 7. STRATIFIED Data Split (FIXED) + Separate Treatment for 003 + Tuned Class Weights\n",
    "# ===========================================\n",
    "\n",
    "data_dir = \"../data/npy_converted\"\n",
    "split_dir = \"../data/npy_converted_split\"\n",
    "label_json_path = \"../data/annotation/segmentation_masks/label_classes.json\"\n",
    "\n",
    "label_map = load_label_mapping(label_json_path)\n",
    "print(f\"Jumlah total kelas di JSON: {len(label_map)}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 1 : Ambil semua file kecuali file 003\n",
    "# ============================================\n",
    "\n",
    "all_x_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\"_x.npy\")])\n",
    "\n",
    "file_003_name = \"massimal_smola_maholmen_202306211129-2_hsi_003_processed\" # ada \"_processed\"-nya di akhir\n",
    "pairs_without_003 = []\n",
    "\n",
    "for fx in all_x_files:\n",
    "    if file_003_name not in fx:  # Exclude file 003\n",
    "        fy = fx.replace(\"_x.npy\", \"_y.npy\")\n",
    "        if os.path.exists(fy):\n",
    "            pairs_without_003.append((fx, fy))\n",
    "\n",
    "print(f\"\\nTotal files (excluding 003): {len(pairs_without_003)}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: Split File 003 Spatially\n",
    "# ============================================\n",
    "\n",
    "file_003_x = os.path.join(data_dir, f\"{file_003_name}_x.npy\")\n",
    "file_003_y = os.path.join(data_dir, f\"{file_003_name}_y.npy\")\n",
    "\n",
    "# Check if split already exists\n",
    "split_exists = all([\n",
    "    os.path.exists(os.path.join(split_dir, f\"{file_003_name}_train_x.npy\")),\n",
    "    os.path.exists(os.path.join(split_dir, f\"{file_003_name}_val_x.npy\")),\n",
    "    os.path.exists(os.path.join(split_dir, f\"{file_003_name}_test_x.npy\"))\n",
    "])\n",
    "\n",
    "if not split_exists:\n",
    "    print(\"\\n[INFO] Splitting file 003...\")\n",
    "    \n",
    "    # Create temporary label_remap for splitting (will be updated later)\n",
    "    temp_remap = {0:0, 8:1, 12:2, 13:3, 14:4, 18:5, 38:6}\n",
    "    \n",
    "    split_files_003 = split_file_003_spatially(\n",
    "        file_003_x, \n",
    "        file_003_y,\n",
    "        label_remap=temp_remap,\n",
    "        train_ratio=0.5,\n",
    "        val_ratio=0.25,\n",
    "        output_dir=split_dir,\n",
    "        method='auto'  # Try horizontal first, then vertical\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n[INFO] Split files already exist, loading...\")\n",
    "    split_files_003 = {\n",
    "        'train': (\n",
    "            os.path.join(split_dir, f\"{file_003_name}_train_x.npy\"),\n",
    "            os.path.join(split_dir, f\"{file_003_name}_train_y.npy\")\n",
    "        ),\n",
    "        'val': (\n",
    "            os.path.join(split_dir, f\"{file_003_name}_val_x.npy\"),\n",
    "            os.path.join(split_dir, f\"{file_003_name}_val_y.npy\")\n",
    "        ),\n",
    "        'test': (\n",
    "            os.path.join(split_dir, f\"{file_003_name}_test_x.npy\"),\n",
    "            os.path.join(split_dir, f\"{file_003_name}_test_y.npy\")\n",
    "        )\n",
    "    }\n",
    "    print(\"  [OK] File 003 train split found\")\n",
    "    print(\"  [OK] File 003 val split found\")\n",
    "    print(\"  [OK] File 003 test split found\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: Create Train/Val/Test Splits\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING TRAIN/VAL/TEST SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "#***************\n",
    "# TRAIN: 10 files + file_003_train\n",
    "train_pairs = [\n",
    "    split_files_003['train'],\n",
    "    pairs_without_003[0],    # 004\n",
    "    pairs_without_003[1],    # 008\n",
    "    pairs_without_003[3],    # 013\n",
    "    pairs_without_003[4],    # 1228-004\n",
    "    pairs_without_003[6],    # 1228-006\n",
    "    pairs_without_003[8],    # 1228-012\n",
    "    pairs_without_003[9],    # 1228-013\n",
    "    pairs_without_003[11],   # 1228-015\n",
    "    pairs_without_003[12],   # 1228-016\n",
    "    pairs_without_003[16],   # 1228-021\n",
    "]\n",
    "\n",
    "# VAL: 4 files + file_003_val\n",
    "val_pairs = [\n",
    "    split_files_003['val'],\n",
    "    pairs_without_003[2],    # 009\n",
    "    pairs_without_003[7],    # 1228-009\n",
    "    pairs_without_003[10],   # 1228-014\n",
    "    pairs_without_003[13],   # 1228-018\n",
    "]\n",
    "\n",
    "# TEST: 2 files + file_003_test\n",
    "test_pairs = [\n",
    "    split_files_003['test'],\n",
    "    pairs_without_003[5],    # 1228-005\n",
    "    pairs_without_003[14],   # 1228-019\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"Train : {len(train_pairs)} files\")\n",
    "print(f\"Val   : {len(val_pairs)} files\")\n",
    "print(f\"Test  : {len(test_pairs)} files\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: Detect Classes & Create Remap\n",
    "# ============================================\n",
    "\n",
    "actual_classes = detect_actual_classes(train_pairs + val_pairs + test_pairs)\n",
    "orig_classes = [int(x) for x in actual_classes]\n",
    "label_remap = {orig: idx for idx, orig in enumerate(orig_classes)}\n",
    "print(f\"\\n[INFO] Label remap: {label_remap}\")\n",
    "\n",
    "# Verify coverage\n",
    "def verify_coverage(split_pairs, split_name):\n",
    "    all_classes = set()\n",
    "    for _, fy in split_pairs:\n",
    "        y = np.load(fy, mmap_mode='r')\n",
    "        all_classes |= set(np.unique(y))\n",
    "    \n",
    "    remapped = {label_remap[c] for c in all_classes if c in label_remap}\n",
    "    print(f\"\\n{split_name}: {sorted(list(remapped))}\")\n",
    "    return remapped\n",
    "\n",
    "train_remapped = verify_coverage(train_pairs, \"TRAIN\")\n",
    "val_remapped = verify_coverage(val_pairs, \"VAL\")\n",
    "test_remapped = verify_coverage(test_pairs, \"TEST\")\n",
    "\n",
    "# Detect actual classes\n",
    "actual_classes = detect_actual_classes(train_pairs + val_pairs + test_pairs)\n",
    "orig_classes = [int(x) for x in actual_classes]\n",
    "label_remap = {orig: idx for idx, orig in enumerate(orig_classes)}\n",
    "print(f\"\\n[INFO] Label remap: {label_remap}\")\n",
    "\n",
    "# Check remapped coverage\n",
    "train_remapped = {label_remap[x] for x in train_orig if x in label_remap}\n",
    "val_remapped = {label_remap[x] for x in val_orig if x in label_remap}\n",
    "test_remapped = {label_remap[x] for x in test_orig if x in label_remap}\n",
    "\n",
    "# Check if class 4 & 5 (critical) are present\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CRITICAL CLASS VERIFICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for split_name, classes in [(\"Train\", train_remapped), (\"Val\", val_remapped), (\"Test\", test_remapped)]:\n",
    "    has_4 = 4 in classes\n",
    "    has_5 = 5 in classes\n",
    "    status_4 = \"✅\" if has_4 else \"❌\"\n",
    "    status_5 = \"✅\" if has_5 else \"❌\"\n",
    "    print(f\"{split_name}: Class 4 (L. digitata) {status_4}, Class 5 (Rockweed) {status_5}\")\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: Create Datasets\n",
    "# ============================================\n",
    "\n",
    "TILE_SIZE = 64  # Keep at 64\n",
    "\n",
    "train_dataset = SeaweedDataset(\n",
    "    [p[0] for p in train_pairs], \n",
    "    label_map, \n",
    "    tile_size=TILE_SIZE, \n",
    "    label_remap=label_remap, \n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = SeaweedDataset(\n",
    "    [p[0] for p in val_pairs], \n",
    "    label_map, \n",
    "    tile_size=TILE_SIZE, \n",
    "    label_remap=label_remap, \n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = SeaweedDataset(\n",
    "    [p[0] for p in test_pairs], \n",
    "    label_map, \n",
    "    tile_size=TILE_SIZE, \n",
    "    label_remap=label_remap, \n",
    "    normalize=True,  # FIXED: Enable normalization!\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: TUNED Class Weights (Power 0.75)\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPUTING CLASS WEIGHTS (TUNED)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Calculate class weights\n",
    "counter = Counter()\n",
    "for _, fy in train_pairs:\n",
    "    y = np.load(fy, mmap_mode=\"r\")\n",
    "    for orig, new in label_remap.items():\n",
    "        cnt = int((y == orig).sum())\n",
    "        counter[new] += cnt\n",
    "\n",
    "print(f\"\\n[INFO] Pixel counts per class: {dict(counter)}\")\n",
    "\n",
    "counts = np.array([counter.get(i, 0) for i in range(len(label_remap))], dtype=np.float64)\n",
    "eps = 1e-6\n",
    "\n",
    "# Inverse frequency\n",
    "inv_freq = 1.0 / (counts + eps)\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "\n",
    "# TUNED: Power 0.75 (was 0.5 for sqrt)\n",
    "class_weights_np = np.power(inv_freq, 0.75).astype(np.float32) # was np.sqrt(inv_freq).astype(np.float32)\n",
    "class_weights_np[0] = 0.0\n",
    "\n",
    "print(f\"Class weights (power=0.75): {class_weights_np}\")\n",
    "print(f\"\\nWeight ratios (vs class 1):\")\n",
    "for i in range(1, len(class_weights_np)):\n",
    "    ratio = class_weights_np[i] / class_weights_np[1]\n",
    "    pixels = counter.get(i, 0)\n",
    "    print(f\"  Class {i}: {class_weights_np[i]:.4f} ({ratio:.2f}x) - {pixels:>8,} pixels\")\n",
    "\n",
    "weight_tensor = torch.from_numpy(class_weights_np).to(device) # untuk masuk sebagai arg value pada pemanggilan criterion HybridLoss di cell 9\n",
    "\n",
    "num_classes_actual = len(label_remap)\n",
    "print(f\"\\nTotal TILE train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba0294-b31d-4c02-9e82-048ef3114b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 8. Initialize Model\n",
    "# ===========================================\n",
    "\n",
    "sample_x = np.load(train_pairs[0][0], mmap_mode=\"r\")\n",
    "in_bands_actual = sample_x.shape[2]\n",
    "print(f\"Band input aktual: {in_bands_actual}\")\n",
    "\n",
    "model = SpectralAttentionHybridSN(\n",
    "    in_bands=in_bands_actual, \n",
    "    num_classes=num_classes_actual,\n",
    "    attention_reduction=16\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODEL ARCHITECTURE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b338e-ca52-4578-a2bb-bb37939c0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 9. Setup Training (IMPROVED)\n",
    "# ===========================================\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 1\n",
    "ACCUMULATION_STEPS = 4\n",
    "CLIP_NORM = 0.5\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# IMPROVED: Multi-Objective Hybrid Loss\n",
    "criterion = HybridLoss(\n",
    "    weight_ce=1.0,          \n",
    "    weight_dice=1.0,        \n",
    "    weight_boundary=0.0,    # DISABLED (was 0.1)\n",
    "    class_weights=weight_tensor,\n",
    "    ignore_index=0,\n",
    "    focal_gamma=5.0         # INCREASED from 4.0 to 5.0\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOSS FUNCTION CONFIGURATION (IMPROVED)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  - Focal Loss: gamma=5.0 (increased from 4.0) [CHECKED]\")\n",
    "print(f\"  - Dice Loss: weight=1.0 [CHECKED]\")\n",
    "print(f\"  - Boundary Loss: DISABLED [CHECKED]\")\n",
    "print(f\"  - Class weights: sqrt-smoothed [CHECKED]\")\n",
    "\n",
    "# Optimizer & Warmup Scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-8)\n",
    "\n",
    "# Warmup: lr ramps from lr/10 to lr over 5 epochs\n",
    "def warmup_lambda(epoch):\n",
    "    warmup_epochs = 5\n",
    "    if epoch < warmup_epochs:\n",
    "        return (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "main_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True, min_lr=1e-7)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCHEDULER CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Warmup: 5 epochs (lr: {LR/10:.2e} -> {LR:.2e})\")\n",
    "print(f\"Main: ReduceLROnPlateau (patience=5)\")\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0.001, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, current_score, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            if current_score > self.best_score + self.min_delta:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "                self.best_epoch = epoch\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "            \n",
    "        return self.early_stop\n",
    "\n",
    "early_stopping = EarlyStopping(patience=15, min_delta=0.001, mode='max')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVEMENTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"[OK] File 003 split (class 4 & 5 in all sets)\")\n",
    "print(f\"[OK] Stratified split (all classes in test)\")\n",
    "print(f\"[OK] Adaptive class-balanced sampling (6x for rare)\")\n",
    "print(f\"[OK] Focal gamma: 5.0 (was 4.0)\")\n",
    "print(f\"[OK] Boundary loss: DISABLED\")\n",
    "print(f\"[OK] Class weights: power 0.75 (was 0.5)\")\n",
    "print(f\"[OK] Normalization fixed (test set enabled)\")\n",
    "print(f\"[OK] Dropout: 0.2 (was 0.3)\")\n",
    "print(f\"[OK] Warmup LR scheduler\")\n",
    "print(f\"[OK] Fixed colormap loading\")\n",
    "\n",
    "\n",
    "# Metrics\n",
    "class SegmentationMetrics:\n",
    "    def __init__(self, num_classes, ignore_index=0):\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total_intersection = torch.zeros(self.num_classes)\n",
    "        self.total_union = torch.zeros(self.num_classes)\n",
    "        self.total_correct = 0\n",
    "        self.total_pixels = 0\n",
    "    \n",
    "    def update(self, pred, target):\n",
    "        valid = (target != self.ignore_index)\n",
    "        pred = pred[valid]\n",
    "        target = target[valid]\n",
    "        \n",
    "        self.total_correct += (pred == target).sum().item()\n",
    "        self.total_pixels += valid.sum().item()\n",
    "        \n",
    "        for cls in range(self.num_classes):\n",
    "            pred_i = (pred == cls)\n",
    "            target_i = (target == cls)\n",
    "            intersection = (pred_i & target_i).sum().item()\n",
    "            union = (pred_i | target_i).sum().item()\n",
    "            \n",
    "            self.total_intersection[cls] += intersection\n",
    "            self.total_union[cls] += union\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        pixel_acc = self.total_correct / (self.total_pixels + 1e-9)\n",
    "        \n",
    "        iou_per_class = self.total_intersection / (self.total_union + 1e-9)\n",
    "        valid_ious = []\n",
    "        for i in range(1, self.num_classes):\n",
    "            if self.total_union[i] > 0:\n",
    "                valid_ious.append(iou_per_class[i].item())\n",
    "        \n",
    "        mean_iou = np.mean(valid_ious) if valid_ious else 0.0\n",
    "        \n",
    "        return pixel_acc, mean_iou, iou_per_class.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47957486-8d6a-4475-a86c-b4731761a291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 10. Training Loop (IMPROVED)\n",
    "# ===========================================\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "START_EPOCH = 1\n",
    "NUM_EPOCHS = 50\n",
    "best_val_miou = 0.0\n",
    "\n",
    "checkpoint_path = \"hybridsn_sgmt_ver5_FIXED_checkpoint.pth\"\n",
    "best_model_path = \"hybridsn_sgmt_ver5_FIXED_best_model.pth\"\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    START_EPOCH = checkpoint[\"epoch\"] + 1\n",
    "    best_val_miou = checkpoint.get(\"best_val_miou\", 0.0)\n",
    "    \n",
    "    if \"early_stopping_state\" in checkpoint:\n",
    "        early_stopping.counter = checkpoint[\"early_stopping_state\"][\"counter\"]\n",
    "        early_stopping.best_score = checkpoint[\"early_stopping_state\"][\"best_score\"]\n",
    "        early_stopping.best_epoch = checkpoint[\"early_stopping_state\"][\"best_epoch\"]\n",
    "    \n",
    "    print(f\"[INFO] Resume dari epoch {START_EPOCH}\")\n",
    "else:\n",
    "    print(\"[INFO] Training dari awal\")\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [], \"val_loss\": [], \n",
    "    \"train_acc\": [], \"val_acc\": [], \n",
    "    \"val_miou\": [],\n",
    "    \"train_loss_focal\": [], \"train_loss_dice\": [], \"train_loss_boundary\": [],\n",
    "    \"val_loss_focal\": [], \"val_loss_dice\": [], \"val_loss_boundary\": []\n",
    "}\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, metrics, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_components = {'focal': 0.0, 'dice': 0.0, 'boundary': 0.0}\n",
    "    metrics.reset()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    nan_count = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    for i, (xb, yb) in enumerate(pbar):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        if torch.all(yb == 0):\n",
    "            continue\n",
    "        \n",
    "        logits = model(xb)\n",
    "        loss, loss_dict = criterion(logits, yb)\n",
    "        loss = loss / ACCUMULATION_STEPS\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            nan_count += 1\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item() * ACCUMULATION_STEPS * xb.size(0)\n",
    "        running_loss_components['focal'] += loss_dict['focal'] * xb.size(0)\n",
    "        running_loss_components['dice'] += loss_dict['dice'] * xb.size(0)\n",
    "        running_loss_components['boundary'] += loss_dict['boundary'] * xb.size(0)\n",
    "        valid_batches += xb.size(0)\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        metrics.update(preds, yb)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item()*ACCUMULATION_STEPS:.4f}\",\n",
    "            \"focal\": f\"{loss_dict['focal']:.3f}\",\n",
    "            \"dice\": f\"{loss_dict['dice']:.3f}\"\n",
    "        })\n",
    "        \n",
    "    if nan_count > 0:\n",
    "        print(f\"  [INFO] Skipped {nan_count} NaN batches\")\n",
    "    \n",
    "    avg_loss = running_loss / valid_batches if valid_batches > 0 else 0.0\n",
    "    avg_loss_focal = running_loss_components['focal'] / valid_batches if valid_batches > 0 else 0.0\n",
    "    avg_loss_dice = running_loss_components['dice'] / valid_batches if valid_batches > 0 else 0.0\n",
    "    avg_loss_boundary = running_loss_components['boundary'] / valid_batches if valid_batches > 0 else 0.0\n",
    "    \n",
    "    pixel_acc, _, _ = metrics.get_metrics()\n",
    "    \n",
    "    return avg_loss, pixel_acc, avg_loss_focal, avg_loss_dice, avg_loss_boundary\n",
    "\n",
    "def validate(model, loader, criterion, metrics, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_loss_components = {'focal': 0.0, 'dice': 0.0, 'boundary': 0.0}\n",
    "    valid_batches = 0\n",
    "    metrics.reset()\n",
    "\n",
    "    nan_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            if torch.all(yb == 0):\n",
    "                continue\n",
    "            \n",
    "            logits = model(xb)\n",
    "            loss, loss_dict = criterion(logits, yb)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                nan_count += 1\n",
    "                continue\n",
    "            \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            running_loss_components['focal'] += loss_dict['focal'] * xb.size(0)\n",
    "            running_loss_components['dice'] += loss_dict['dice'] * xb.size(0)\n",
    "            running_loss_components['boundary'] += loss_dict['boundary'] * xb.size(0)\n",
    "            valid_batches += xb.size(0)\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            metrics.update(preds, yb)\n",
    "\n",
    "    if nan_count > 0:\n",
    "        print(f\"  [INFO] Skipped {nan_count} NaN batches\")\n",
    "    \n",
    "    if valid_batches > 0:\n",
    "        avg_loss = running_loss / valid_batches\n",
    "        avg_loss_focal = running_loss_components['focal'] / valid_batches\n",
    "        avg_loss_dice = running_loss_components['dice'] / valid_batches\n",
    "        avg_loss_boundary = running_loss_components['boundary'] / valid_batches\n",
    "    else:\n",
    "        avg_loss = float('nan')\n",
    "        avg_loss_focal = avg_loss_dice = avg_loss_boundary = float('nan')\n",
    "    \n",
    "    pixel_acc, mean_iou, iou_per_class = metrics.get_metrics()\n",
    "    \n",
    "    return avg_loss, pixel_acc, mean_iou, iou_per_class, avg_loss_focal, avg_loss_dice, avg_loss_boundary\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING (IMPROVED VERSION)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Max epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "stopped_early = False\n",
    "\n",
    "for epoch in range(START_EPOCH, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated\")\n",
    "    \n",
    "    # Training\n",
    "    train_metrics = SegmentationMetrics(num_classes_actual, ignore_index=0)\n",
    "    train_loss, train_acc, train_focal, train_dice, train_bnd = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, train_metrics, device\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = SegmentationMetrics(num_classes_actual, ignore_index=0)\n",
    "    val_loss, val_acc, val_miou, val_iou_per_class, val_focal, val_dice, val_bnd = validate(\n",
    "        model, val_loader, criterion, val_metrics, device\n",
    "    )\n",
    "    \n",
    "    # Scheduler step (WARMUP AWARE)\n",
    "    if epoch <= 5:\n",
    "        warmup_scheduler.step()\n",
    "        print(f\"  [Warmup] LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    else:\n",
    "        main_scheduler.step(val_miou)\n",
    "    \n",
    "    # Logging\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"    ├─ Focal: {train_focal:.4f} | Dice: {train_dice:.4f}\")\n",
    "    print(f\"  Val Loss  : {val_loss:.4f} | Val Acc  : {val_acc:.4f}\")\n",
    "    print(f\"    ├─ Focal: {val_focal:.4f} | Dice: {val_dice:.4f}\")\n",
    "    print(f\"  Val mIoU  : {val_miou:.4f}\")\n",
    "    print(f\"  Time      : {elapsed/60:.2f} min\")\n",
    "    print(f\"  IoU per class: {val_iou_per_class[1:]}\")\n",
    "    \n",
    "    # Save history\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_miou\"].append(val_miou)\n",
    "    history[\"train_loss_focal\"].append(train_focal)\n",
    "    history[\"train_loss_dice\"].append(train_dice)\n",
    "    history[\"train_loss_boundary\"].append(train_bnd)\n",
    "    history[\"val_loss_focal\"].append(val_focal)\n",
    "    history[\"val_loss_dice\"].append(val_dice)\n",
    "    history[\"val_loss_boundary\"].append(val_bnd)\n",
    "    \n",
    "    # Checkpoint\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"warmup_scheduler_state\": warmup_scheduler.state_dict(),\n",
    "        \"main_scheduler_state\": main_scheduler.state_dict(),\n",
    "        \"best_val_miou\": best_val_miou,\n",
    "        \"history\": history,\n",
    "        \"early_stopping_state\": {\n",
    "            \"counter\": early_stopping.counter,\n",
    "            \"best_score\": early_stopping.best_score,\n",
    "            \"best_epoch\": early_stopping.best_epoch\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_miou > best_val_miou:\n",
    "        best_val_miou = val_miou\n",
    "        torch.save(checkpoint, best_model_path)\n",
    "        print(f\"[OK] Best model saved! (mIoU: {best_val_miou:.4f})\")\n",
    "    \n",
    "    # Early Stopping\n",
    "    if early_stopping(val_miou, epoch):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EARLY STOPPING TRIGGERED!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Best mIoU: {early_stopping.best_score:.4f} at epoch {early_stopping.best_epoch}\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "    else:\n",
    "        if early_stopping.counter > 0:\n",
    "            print(f\"[Early Stop] No improvement: {early_stopping.counter}/{early_stopping.patience}\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if stopped_early:\n",
    "    print(\"Training stopped early!\")\n",
    "else:\n",
    "    print(\"Training completed!\")\n",
    "print(f\"Best validation mIoU: {best_val_miou:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342319e-c5fc-4546-b16e-d620cbce21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load colormap ONCE for all visualizations\n",
    "global_cmap = load_colormap_from_json(label_json_path, label_remap, num_classes_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459542c9-cda1-4d1b-9c18-5b76109e5c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 11. Plot Training History (Enhanced)\n",
    "# ===========================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history[\"train_loss\"], label=\"Train Loss\", marker='o')\n",
    "axes[0, 0].plot(history[\"val_loss\"], label=\"Val Loss\", marker='s')\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Training vs Validation Loss\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history[\"train_acc\"], label=\"Train Acc\", marker='o')\n",
    "axes[0, 1].plot(history[\"val_acc\"], label=\"Val Acc\", marker='s')\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Pixel Accuracy\")\n",
    "axes[0, 1].set_title(\"Training vs Validation Accuracy\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# mIoU\n",
    "axes[0, 2].plot(history[\"val_miou\"], label=\"Val mIoU\", marker='d', color='green')\n",
    "axes[0, 2].set_xlabel(\"Epoch\")\n",
    "axes[0, 2].set_ylabel(\"Mean IoU\")\n",
    "axes[0, 2].set_title(\"Validation mIoU\")\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True)\n",
    "\n",
    "# Loss Components - Training\n",
    "axes[1, 0].plot(history[\"train_loss_focal\"], label=\"Focal Loss\", marker='o')\n",
    "axes[1, 0].plot(history[\"train_loss_dice\"], label=\"Dice Loss\", marker='s')\n",
    "axes[1, 0].plot(history[\"train_loss_boundary\"], label=\"Boundary Loss\", marker='^')\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"Loss Value\")\n",
    "axes[1, 0].set_title(\"Training Loss Components\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Loss Components - Validation\n",
    "axes[1, 1].plot(history[\"val_loss_focal\"], label=\"Focal Loss\", marker='o')\n",
    "axes[1, 1].plot(history[\"val_loss_dice\"], label=\"Dice Loss\", marker='s')\n",
    "axes[1, 1].plot(history[\"val_loss_boundary\"], label=\"Boundary Loss\", marker='^')\n",
    "axes[1, 1].set_xlabel(\"Epoch\")\n",
    "axes[1, 1].set_ylabel(\"Loss Value\")\n",
    "axes[1, 1].set_title(\"Validation Loss Components\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# Loss ratio analysis\n",
    "if len(history[\"val_loss_focal\"]) > 0:\n",
    "    total = np.array(history[\"val_loss_focal\"]) + np.array(history[\"val_loss_dice\"]) + np.array(history[\"val_loss_boundary\"])\n",
    "    focal_ratio = np.array(history[\"val_loss_focal\"]) / (total + 1e-9)\n",
    "    dice_ratio = np.array(history[\"val_loss_dice\"]) / (total + 1e-9)\n",
    "    bnd_ratio = np.array(history[\"val_loss_boundary\"]) / (total + 1e-9)\n",
    "    \n",
    "    axes[1, 2].plot(focal_ratio, label=\"Focal Ratio\", marker='o')\n",
    "    axes[1, 2].plot(dice_ratio, label=\"Dice Ratio\", marker='s')\n",
    "    axes[1, 2].plot(bnd_ratio, label=\"Boundary Ratio\", marker='^')\n",
    "    axes[1, 2].set_xlabel(\"Epoch\")\n",
    "    axes[1, 2].set_ylabel(\"Proportion\")\n",
    "    axes[1, 2].set_title(\"Loss Component Ratios (Validation)\")\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_history_ver5_enhanced.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de16a85-baa5-46e8-9135-f9f07237d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 12. Spectral Attention Visualization\n",
    "# ===========================================\n",
    "\n",
    "def visualize_spectral_attention(model, loader, num_samples=3):\n",
    "    \"\"\"Visualize learned spectral attention weights\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    attention_weights_list = []\n",
    "    sample_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (xb, yb) in enumerate(loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            xb = xb.to(device)\n",
    "            _ = model(xb)  # Forward pass\n",
    "            \n",
    "            # Get attention weights\n",
    "            attn_weights = model.get_attention_weights()\n",
    "            if attn_weights is not None:\n",
    "                attention_weights_list.append(attn_weights[0].cpu().numpy())\n",
    "                sample_images.append(xb[0])\n",
    "    \n",
    "    if not attention_weights_list:\n",
    "        print(\"No attention weights captured!\")\n",
    "        return\n",
    "    \n",
    "    # Plot attention weights\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(14, 4*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Attention weights\n",
    "        attn = attention_weights_list[i]\n",
    "        axes[i, 0].bar(range(len(attn)), attn, color='steelblue', alpha=0.7)\n",
    "        axes[i, 0].set_xlabel(\"Spectral Channel (after 3D CNN)\")\n",
    "        axes[i, 0].set_ylabel(\"Attention Weight\")\n",
    "        axes[i, 0].set_title(f\"Sample {i+1}: Learned Spectral Attention\")\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Top-k important channels\n",
    "        top_k = 10\n",
    "        top_indices = np.argsort(attn)[-top_k:]\n",
    "        axes[i, 0].bar(top_indices, attn[top_indices], color='red', alpha=0.7, \n",
    "                       label=f'Top-{top_k} channels')\n",
    "        axes[i, 0].legend()\n",
    "        \n",
    "        # Original hyperspectral signature (averaged)\n",
    "        img = sample_images[i].cpu().numpy()  # [Bands, H, W]\n",
    "        mean_spectrum = img.mean(axis=(1, 2))\n",
    "        \n",
    "        axes[i, 1].plot(mean_spectrum, color='darkgreen', linewidth=1.5)\n",
    "        axes[i, 1].set_xlabel(\"Original Spectral Band\")\n",
    "        axes[i, 1].set_ylabel(\"Mean Reflectance\")\n",
    "        axes[i, 1].set_title(f\"Sample {i+1}: Mean Spectral Signature\")\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"spectral_attention_weights_ver5.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SPECTRAL ATTENTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    for i, attn in enumerate(attention_weights_list):\n",
    "        top_5_idx = np.argsort(attn)[-5:]\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  Mean attention: {attn.mean():.4f}\")\n",
    "        print(f\"  Std attention: {attn.std():.4f}\")\n",
    "        print(f\"  Max attention: {attn.max():.4f}\")\n",
    "        print(f\"  Top-5 channels: {top_5_idx} with weights {attn[top_5_idx]}\")\n",
    "\n",
    "# Visualize attention on validation set\n",
    "print(\"\\nVisualizing spectral attention weights...\")\n",
    "visualize_spectral_attention(model, val_loader, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44479017-395c-41f9-ab81-10d14fb5a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 13. Testing dan Visualisasi Output Segmentation\n",
    "# ===========================================\n",
    "\n",
    "# Load best model\n",
    "best_checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(best_checkpoint[\"model_state\"])\n",
    "print(f\"Loaded best model from epoch {best_checkpoint['epoch']}\")\n",
    "\n",
    "# Test evaluation\n",
    "test_metrics = SegmentationMetrics(num_classes_actual, ignore_index=0)\n",
    "test_loss, test_acc, test_miou, test_iou_per_class, test_focal, test_dice, test_bnd = validate(\n",
    "    model, test_loader, criterion, test_metrics, device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss     : {test_loss:.4f}\")\n",
    "print(f\"  ├─ Focal Loss  : {test_focal:.4f}\") # rename ce menjadi focal \n",
    "print(f\"  ├─ Dice Loss: {test_dice:.4f}\")\n",
    "print(f\"  └─ Bnd Loss : {test_bnd:.4f}\")\n",
    "print(f\"Test Accuracy : {test_acc:.4f}\")\n",
    "print(f\"Test mIoU     : {test_miou:.4f}\")\n",
    "print(f\"IoU per class : {test_iou_per_class[1:]}\")\n",
    "\n",
    "# Visualisasi beberapa prediksi SEGMENTATION MAP\n",
    "model.eval()\n",
    "num_vis = 5\n",
    "vis_samples = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SEGMENTATION MAPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xb, yb) in enumerate(test_loader):\n",
    "        if i >= num_vis:\n",
    "            break\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1)  # [B, H, W] - SEGMENTATION MAP\n",
    "        \n",
    "        vis_samples.append((xb[0], yb[0], preds[0]))\n",
    "        print(f\"Sample {i+1}: Input shape {xb.shape}, Output shape {preds.shape}\")\n",
    "\n",
    "# Plot visualisasi segmentation maps\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "for i, (x, y_true, y_pred) in enumerate(vis_samples):\n",
    "    print(f\"Visualizing sample {i+1}/{num_vis}\")\n",
    "    visualize_tile(x, y_true.cpu().numpy(), y_pred.cpu().numpy(), \n",
    "                   cmap=global_cmap, idx=i)\n",
    "\n",
    "print(\"\\n[OK] Output: SEGMENTATION MAPS\")\n",
    "print(\"  Format: [H, W] dengan setiap pixel berisi class label (0-6)\")\n",
    "print(\"  Visualisasi: Setiap warna merepresentasikan habitat class berbeda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b505d0-478f-491f-82c3-bf547cc14927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 14. Confusion Matrix\n",
    "# ===========================================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Collect predictions untuk confusion matrix\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(test_loader, desc=\"Computing CM\"):\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        \n",
    "        # Flatten dan filter valid pixels\n",
    "        preds_flat = preds.cpu().numpy().flatten()\n",
    "        targets_flat = yb.numpy().flatten()\n",
    "        \n",
    "        valid = targets_flat != 0  # Exclude background\n",
    "        all_preds.extend(preds_flat[valid])\n",
    "        all_targets.extend(targets_flat[valid])\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds, labels=list(range(1, num_classes_actual)))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(1, num_classes_actual),\n",
    "            yticklabels=range(1, num_classes_actual))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Test Set - Excluding Background)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_ver5.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac2973-e670-4e48-940c-bf75ce3e17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 15. Full Image Segmentation with Sliding Window\n",
    "# ===========================================\n",
    "\n",
    "def segment_full_image(model, image_path, mask_path=None, tile_size=64, stride=32, device='cuda'):\n",
    "    \"\"\"\n",
    "    Segmentasi full image dengan sliding window approach\n",
    "    \n",
    "    Args:\n",
    "        model: trained segmentation model\n",
    "        image_path: path ke file _x.npy\n",
    "        mask_path: path ke file _y.npy (optional, untuk comparison)\n",
    "        tile_size: ukuran tile untuk inference\n",
    "        stride: step size untuk sliding window (stride < tile_size untuk smooth overlap)\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        prediction_map: segmentation result [H, W]\n",
    "        full_image: original image [H, W, Bands]\n",
    "        ground_truth: GT mask if mask_path provided, else None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"[INFO] Loading full image from: {image_path}\")\n",
    "    \n",
    "    # Load full image\n",
    "    x_full = np.load(image_path, mmap_mode='r')  # [H, W, Bands]\n",
    "    H, W, B = x_full.shape\n",
    "    print(f\"[INFO] Image shape: {x_full.shape}\")\n",
    "    \n",
    "    # Normalize\n",
    "    x_full_norm = normalize_reflectance(x_full.copy())\n",
    "    \n",
    "    # Prepare output with soft voting\n",
    "    prediction_accumulator = np.zeros((num_classes_actual, H, W), dtype=np.float32)\n",
    "    count_map = np.zeros((H, W), dtype=np.float32)\n",
    "    \n",
    "    # Calculate number of tiles\n",
    "    n_tiles_h = (H - tile_size) // stride + 1\n",
    "    n_tiles_w = (W - tile_size) // stride + 1\n",
    "    total_tiles = n_tiles_h * n_tiles_w\n",
    "    \n",
    "    print(f\"[INFO] Processing {total_tiles} tiles ({n_tiles_h}x{n_tiles_w})...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tile_count = 0\n",
    "        for i in range(0, H - tile_size + 1, stride):\n",
    "            for j in range(0, W - tile_size + 1, stride):\n",
    "                # Extract tile\n",
    "                tile = x_full_norm[i:i+tile_size, j:j+tile_size, :]\n",
    "                tile_tensor = torch.from_numpy(tile.transpose(2,0,1)).unsqueeze(0).float().to(device)\n",
    "                \n",
    "                # Predict\n",
    "                logits = model(tile_tensor)  # [1, C, H, W]\n",
    "                probs = F.softmax(logits, dim=1).squeeze(0).cpu().numpy()  # [C, H, W]\n",
    "                \n",
    "                # Accumulate predictions (soft voting)\n",
    "                prediction_accumulator[:, i:i+tile_size, j:j+tile_size] += probs\n",
    "                count_map[i:i+tile_size, j:j+tile_size] += 1\n",
    "                \n",
    "                tile_count += 1\n",
    "                if tile_count % 100 == 0:\n",
    "                    print(f\"  Processed {tile_count}/{total_tiles} tiles ({tile_count/total_tiles*100:.1f}%)\")\n",
    "    \n",
    "    # Average overlapping predictions and get argmax\n",
    "    prediction_accumulator /= (count_map[np.newaxis, :, :] + 1e-9)\n",
    "    final_pred = prediction_accumulator.argmax(axis=0).astype(np.int32)\n",
    "    \n",
    "    print(f\"[INFO] Segmentation complete!\")\n",
    "    print(f\"[INFO] Unique classes predicted: {np.unique(final_pred)}\")\n",
    "    \n",
    "    # Load ground truth if provided\n",
    "    ground_truth = None\n",
    "    if mask_path and os.path.exists(mask_path):\n",
    "        ground_truth = np.load(mask_path)\n",
    "        print(f\"[INFO] Ground truth loaded. Unique classes: {np.unique(ground_truth)}\")\n",
    "        \n",
    "        # Remap ground truth\n",
    "        if label_remap is not None:\n",
    "            gt_remap = np.zeros_like(ground_truth, dtype=np.int32)\n",
    "            for orig_label, new_idx in label_remap.items():\n",
    "                gt_remap[ground_truth == orig_label] = new_idx\n",
    "            ground_truth = gt_remap\n",
    "    \n",
    "    return final_pred, x_full, ground_truth\n",
    "\n",
    "\n",
    "# Visualize full image segmentation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL IMAGE SEGMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select a test image\n",
    "test_file_idx = 0  # Change this to visualize different images\n",
    "test_file_x = test_pairs[test_file_idx][0]\n",
    "test_file_y = test_pairs[test_file_idx][1]\n",
    "\n",
    "print(f\"Selected image: {os.path.basename(test_file_x)}\")\n",
    "\n",
    "# Perform segmentation\n",
    "full_pred, full_img, full_gt = segment_full_image(\n",
    "    model, \n",
    "    test_file_x, \n",
    "    test_file_y,\n",
    "    tile_size=TILE_SIZE, \n",
    "    stride=TILE_SIZE // 2,  # 50% overlap\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Calculate metrics for full image\n",
    "if full_gt is not None:\n",
    "    valid_mask = (full_gt != 0)\n",
    "    if valid_mask.sum() > 0:\n",
    "        valid_pred = full_pred[valid_mask]\n",
    "        valid_gt = full_gt[valid_mask]\n",
    "        \n",
    "        full_acc = (valid_pred == valid_gt).sum() / len(valid_pred)\n",
    "        \n",
    "        # Calculate IoU per class\n",
    "        full_iou_per_class = []\n",
    "        for cls in range(1, num_classes_actual):\n",
    "            pred_i = (valid_pred == cls)\n",
    "            gt_i = (valid_gt == cls)\n",
    "            intersection = (pred_i & gt_i).sum()\n",
    "            union = (pred_i | gt_i).sum()\n",
    "            iou = intersection / (union + 1e-9) if union > 0 else 0.0\n",
    "            full_iou_per_class.append(iou)\n",
    "        \n",
    "        full_miou = np.mean([iou for iou in full_iou_per_class if iou > 0])\n",
    "        \n",
    "        print(f\"\\n[METRICS] Full Image:\")\n",
    "        print(f\"  Pixel Accuracy: {full_acc:.4f}\")\n",
    "        print(f\"  Mean IoU: {full_miou:.4f}\")\n",
    "        print(f\"  IoU per class: {full_iou_per_class}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Pseudo-RGB\n",
    "rgb_idx = [int(full_img.shape[2]*0.05), int(full_img.shape[2]*0.5), int(full_img.shape[2]*0.9)]\n",
    "rgb = full_img[..., rgb_idx]\n",
    "rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "\n",
    "# Load colormap\n",
    "if os.path.exists(label_json_path):\n",
    "    with open(label_json_path, \"r\") as f:\n",
    "        label_info = json.load(f)\n",
    "    custom_colors = [c[\"color\"][:7] for c in label_info]\n",
    "    cmap = ListedColormap(custom_colors)\n",
    "else:\n",
    "    cmap = \"tab20\"\n",
    "\n",
    "axes[0].imshow(rgb_norm)\n",
    "axes[0].set_title(f\"Full Image (Pseudo-RGB)\\nShape: {full_img.shape[:2]}\", fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "if full_gt is not None:\n",
    "    axes[1].imshow(full_gt, cmap=cmap, vmin=0, vmax=num_classes_actual-1)\n",
    "    axes[1].set_title(\"Ground Truth Segmentation\", fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(full_pred, cmap=cmap, vmin=0, vmax=num_classes_actual-1)\n",
    "axes[2].set_title(f\"Predicted Segmentation\\nmIoU: {full_miou:.3f}\" if full_gt is not None else \"Predicted Segmentation\", fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_image_segmentation_ver5.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n[OK] Full image segmentation saved!\")\n",
    "print(f\"  Resolution: {full_pred.shape}\")\n",
    "print(f\"  File: full_image_segmentation_ver5.png\")\n",
    "\n",
    "\n",
    "# Optional: Visualize multiple test images\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BATCH FULL IMAGE SEGMENTATION (ALL TEST IMAGES)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, (fx, fy) in enumerate(test_pairs):\n",
    "    print(f\"\\nProcessing image {idx+1}/{len(test_pairs)}: {os.path.basename(fx)}\")\n",
    "    \n",
    "    pred, img, gt = segment_full_image(\n",
    "        model, fx, fy,\n",
    "        tile_size=TILE_SIZE,\n",
    "        stride=TILE_SIZE // 2,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    rgb_idx = [int(img.shape[2]*0.05), int(img.shape[2]*0.5), int(img.shape[2]*0.9)]\n",
    "    rgb = img[..., rgb_idx]\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "    \n",
    "    axes[0].imshow(rgb_norm)\n",
    "    axes[0].set_title(f\"Image {idx+1}: Pseudo-RGB\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    if gt is not None:\n",
    "        axes[1].imshow(gt, cmap=cmap)\n",
    "        axes[1].set_title(\"Ground Truth\")\n",
    "        axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(pred, cmap=cmap)\n",
    "    axes[2].set_title(\"Prediction\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'full_image_seg_test_{idx+1}.png', dpi=120, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  [OK] Saved: full_image_seg_test_{idx+1}.png\")\n",
    "\n",
    "print(\"\\n[OK] All test images processed!\")\n",
    "print(f\"[OK] Generated {len(test_pairs)} full segmentation maps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb79d49-d3b6-41ca-9f89-4fb691cded70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 15b. Dataset Distribution Analysis (untuk solusi test set issue)\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def analyze_file_distribution(pairs, split_name):\n",
    "    \"\"\"Analisis distribusi kelas per file\"\"\"\n",
    "    print(f\"\\n{split_name} Split Analysis:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    file_class_info = []\n",
    "    \n",
    "    for idx, (fx, fy) in enumerate(pairs):\n",
    "        y = np.load(fy, mmap_mode='r')\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        # Convert to dict\n",
    "        class_dist = dict(zip(unique.tolist(), counts.tolist()))\n",
    "        \n",
    "        # Get remapped classes\n",
    "        remapped_dist = {}\n",
    "        for orig, cnt in class_dist.items():\n",
    "            if orig in label_remap:\n",
    "                new_idx = label_remap[orig]\n",
    "                remapped_dist[new_idx] = cnt\n",
    "        \n",
    "        file_class_info.append({\n",
    "            'file': os.path.basename(fx),\n",
    "            'distribution': remapped_dist,\n",
    "            'classes_present': sorted(remapped_dist.keys())\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nFile {idx+1}: {os.path.basename(fx)}\")\n",
    "        print(f\"  Classes present: {sorted(remapped_dist.keys())}\")\n",
    "        print(f\"  Pixel counts: {remapped_dist}\")\n",
    "    \n",
    "    # Summary\n",
    "    all_classes = set()\n",
    "    for info in file_class_info:\n",
    "        all_classes.update(info['classes_present'])\n",
    "    \n",
    "    print(f\"\\n{split_name} Summary:\")\n",
    "    print(f\"  Total files: {len(pairs)}\")\n",
    "    print(f\"  Classes covered: {sorted(all_classes)}\")\n",
    "    print(f\"  Missing classes: {set(range(num_classes_actual)) - all_classes}\")\n",
    "    \n",
    "    return file_class_info\n",
    "\n",
    "# Analyze all splits\n",
    "train_analysis = analyze_file_distribution(train_pairs, \"TRAIN\")\n",
    "val_analysis = analyze_file_distribution(val_pairs, \"VALIDATION\")\n",
    "test_analysis = analyze_file_distribution(test_pairs, \"TEST\")\n",
    "\n",
    "# Check coverage\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL COVERAGE CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_train_classes = set()\n",
    "all_val_classes = set()\n",
    "all_test_classes = set()\n",
    "\n",
    "for info in train_analysis:\n",
    "    all_train_classes.update(info['classes_present'])\n",
    "for info in val_analysis:\n",
    "    all_val_classes.update(info['classes_present'])\n",
    "for info in test_analysis:\n",
    "    all_test_classes.update(info['classes_present'])\n",
    "\n",
    "print(f\"\\nClasses in Train: {sorted(all_train_classes)}\")\n",
    "print(f\"Classes in Val:   {sorted(all_val_classes)}\")\n",
    "print(f\"Classes in Test:  {sorted(all_test_classes)}\")\n",
    "\n",
    "missing_in_test = all_train_classes - all_test_classes\n",
    "if missing_in_test:\n",
    "    print(f\"\\n [WARNING]: Classes {sorted(missing_in_test)} are in TRAIN but NOT in TEST!\")\n",
    "    print(f\"   This means we cannot evaluate model performance on these classes.\")\n",
    "    \n",
    "    # Suggest rebalancing\n",
    "    print(f\"\\n💡 RECOMMENDATION:\")\n",
    "    print(f\"   Option 1: Move files from train/val to test that contain classes {sorted(missing_in_test)}\")\n",
    "    \n",
    "    # Find files in train that have missing classes\n",
    "    candidate_files = []\n",
    "    for info in train_analysis:\n",
    "        if missing_in_test & set(info['classes_present']):\n",
    "            candidate_files.append((info['file'], info['classes_present']))\n",
    "    \n",
    "    if candidate_files:\n",
    "        print(f\"\\n   Candidate files to move to test set:\")\n",
    "        for fname, classes in candidate_files[:3]:  # Show top 3\n",
    "            print(f\"     - {fname}: has classes {sorted(classes)}\")\n",
    "else:\n",
    "    print(f\"\\n✓ All classes are represented in test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7d889-8c20-43dc-8fb4-71b2715bf4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 17. Save Results Summary\n",
    "# ===========================================\n",
    "\n",
    "results_summary = {\n",
    "    \"model\": \"SpectralAttentionHybridSN\",\n",
    "    \"version\": \"v4_improved\",\n",
    "    \"novelties\": [\"Spectral Self-Attention\", \"Multi-Objective Loss (Focal+Dice+Boundary)\"],\n",
    "    \"improvements\": [\n",
    "        \"Learning rate: 5e-6 → 1e-4\",\n",
    "        \"Tile size: 32 → 64\",\n",
    "        \"Boundary loss weight: 0.5 → 0.1\",\n",
    "        \"Data augmentation: flip + rotation\",\n",
    "        \"Focal loss replacing CE\",\n",
    "        \"Early stopping patience: 10 → 15\"\n",
    "    ],\n",
    "    \"hyperparameters\": {\n",
    "        \"learning_rate\": LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"accumulation_steps\": ACCUMULATION_STEPS,\n",
    "        \"tile_size\": TILE_SIZE,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"attention_reduction\": 16,\n",
    "        \"loss_weights\": {\"focal\": 1.0, \"dice\": 1.0, \"boundary\": 0.1},\n",
    "        \"focal_gamma\": 2.0,\n",
    "        \"early_stopping_patience\": 15\n",
    "    },\n",
    "    \"test_results\": {\n",
    "        \"pixel_accuracy\": float(test_acc),\n",
    "        \"mean_iou\": float(test_miou),\n",
    "        \"loss_total\": float(test_loss),\n",
    "        \"loss_focal\": float(test_focal),\n",
    "        \"loss_dice\": float(test_dice),\n",
    "        \"loss_boundary\": float(test_bnd),\n",
    "        \"iou_per_class\": test_iou_per_class.tolist()\n",
    "    },\n",
    "    \"training_history\": history\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"results_ver5_summary.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(results_summary[\"test_results\"], indent=2))\n",
    "print(\"\\nResults saved to: results_ver5_summary.json\")\n",
    "print(\"\\n[OK]Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
