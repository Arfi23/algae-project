{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba550fc2-7c28-4aaf-88dc-d674f3645334",
   "metadata": {},
   "source": [
    "### Versi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fc193c-5510-46f6-8950-cef44e0e096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device yang digunakan: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 1. Import Library dan Setup Environment\n",
    "# ===========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Gunakan GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device yang digunakan:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47021b9-620b-47b4-b014-7f8f942937b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 2. Fungsi Bantuan Umum\n",
    "# ===========================================\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Menetapkan seed random agar hasil eksperimen bisa direplikasi\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "def visualize_tile(x_tile, y_true=None, y_pred=None, json_path=None,class_names=None, idx=0):\n",
    "    \"\"\"\n",
    "    Menampilkan citra tile beserta mask ground-truth dan prediksi\n",
    "    \"\"\"\n",
    "    if isinstance(x_tile, torch.Tensor):\n",
    "        x = x_tile.cpu().numpy()\n",
    "        x = np.transpose(x, (1,2,0))  # ubah dari [B,H,W] -> [H,W,B]\n",
    "    else:\n",
    "        x = x_tile\n",
    "\n",
    "    # menampilkan pseudo-RGB (karena data hyperspectral)\n",
    "    B = x.shape[2]\n",
    "    b1, b2, b3 = int(B*0.05), int(B*0.5), int(B*0.9)\n",
    "    rgb = x[..., [b1, b2, b3]]\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "\n",
    "    # Coba baca colormap dari file JSON\n",
    "    if json_path and os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            label_info = json.load(f)\n",
    "        custom_colors = [c[\"color\"][:7] for c in label_info]\n",
    "        cmap = ListedColormap(custom_colors)\n",
    "    else:\n",
    "        print(\"File json tidak terbaca, menggunakan cmap tab20\")\n",
    "        cmap = \"tab20\"  # fallback\n",
    "\n",
    "        \n",
    "\n",
    "    # Visualisasi\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.imshow(rgb_norm); plt.title(\"Citra (Pseudo-RGB)\")\n",
    "    if y_true is not None:\n",
    "        plt.subplot(1,3,2); plt.imshow(y_true, cmap=cmap); plt.title(\"Ground Truth\")\n",
    "    if y_pred is not None:\n",
    "        plt.subplot(1,3,3); plt.imshow(y_pred, cmap=cmap); plt.title(\"Prediksi\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad9e39d-a3a4-4af2-9cca-44e1836dac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Cell 3. Dataset Loader (SeaweedDataset) dan Label Mapping\n",
    "# =========================================================\n",
    "\n",
    "def load_label_mapping(json_path):\n",
    "    \"\"\"Membaca file label_classes.json untuk mapping id ke nama kelas\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    idx_to_name = {i: item[\"name\"] for i, item in enumerate(data)}\n",
    "    return idx_to_name\n",
    "\n",
    "# def normalize_reflectance(cube):\n",
    "#     \"\"\"Menormalkan nilai reflektansi ke rentang 0-1 per tile\"\"\"\n",
    "#     cube = np.nan_to_num(cube).astype(np.float32)\n",
    "#     min_val = np.nanmin(cube)\n",
    "#     max_val = np.nanmax(cube)\n",
    "#     if max_val > min_val:\n",
    "#         cube = (cube - min_val) / (max_val - min_val)\n",
    "#     return cube\n",
    "\n",
    "def normalize_reflectance(cube):\n",
    "    \"\"\"Menormalkan reflektansi 0–1 per tile, hemat RAM, aman untuk mmap read-only.\"\"\"\n",
    "    # Jika hasil np.load mmap, array biasanya read-only → buat copy ringan\n",
    "    if not cube.flags.writeable:\n",
    "        cube = cube.astype(np.float32, copy=True)  # hanya salin tile, bukan file besar\n",
    "\n",
    "    # Pastikan float32\n",
    "    if cube.dtype != np.float32:\n",
    "        cube = cube.astype(np.float32, copy=False)\n",
    "\n",
    "    # Ganti NaN / Inf in-place\n",
    "    np.nan_to_num(cube, copy=False)\n",
    "\n",
    "    # Normalisasi min-max\n",
    "    min_val = np.nanmin(cube)\n",
    "    max_val = np.nanmax(cube)\n",
    "    if max_val > min_val:\n",
    "        cube -= min_val\n",
    "        cube /= (max_val - min_val + 1e-8)\n",
    "\n",
    "    return cube\n",
    "\n",
    "\n",
    "class SeaweedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset hemat memori berbasis file .npy hasil konversi.\n",
    "    Membaca tile langsung dari disk menggunakan mmap_mode=\"r\".\n",
    "    \"\"\"\n",
    "    def __init__(self, data_files, label_map, tile_size=128, normalize=True, label_remap=None):\n",
    "        self.data_files = data_files\n",
    "        self.label_map = label_map\n",
    "        self.tile_size = tile_size\n",
    "        self.normalize = normalize\n",
    "        # label_remap: dict {orig_label: new_index}, if None -> identity mapping\n",
    "        self.label_remap = label_remap\n",
    "\n",
    "        # Daftar pasangan (file_x, file_y)\n",
    "        self.pairs = []\n",
    "        for f in data_files:\n",
    "            if f.endswith(\"_x.npy\"):\n",
    "                fy = f.replace(\"_x.npy\", \"_y.npy\")\n",
    "                if os.path.exists(fy):\n",
    "                    self.pairs.append((f, fy))\n",
    "        \n",
    "        # Hanya menyimpan indeks tile berdasarkan ukuran file .npy\n",
    "        self.index = []  \n",
    "        for file_idx, (fx, fy) in enumerate(self.pairs):\n",
    "            x = np.load(fx, mmap_mode=\"r\")\n",
    "            H, W, _ = x.shape\n",
    "            for i in range(0, H - tile_size + 1, tile_size):\n",
    "                for j in range(0, W - tile_size + 1, tile_size):\n",
    "                    self.index.append((file_idx, i, j))\n",
    "            del x  # bebaskan referensi memori\n",
    "\n",
    "        print(f\"[INFO] Total tile terdaftar: {len(self.index)} dari {len(self.pairs)} file\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, i, j = self.index[idx]\n",
    "        fx, fy = self.pairs[file_idx]\n",
    "        \n",
    "        # Memuat tile menggunakan mmap\n",
    "        x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "        y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "        # Abaikan tile kosong (semua 0) dengan fail-safe agar tidak infinite recursion\n",
    "        # kalau tetap kosong setelah 3 percobaan → biarkan saja y tetap kosong (model akan skip naturally karena weight=0 utk bg)\n",
    "        for _ in range(3):  # coba maksimal 3 kali\n",
    "            if np.any(y > 0):\n",
    "                break\n",
    "            file_idx, i, j = self.index[np.random.randint(0, len(self.index))]\n",
    "            fx, fy = self.pairs[file_idx]\n",
    "            x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "            y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "\n",
    "        if self.normalize:\n",
    "            x = normalize_reflectance(x)\n",
    "\n",
    "        # REMAP label bila mapping diberikan\n",
    "        if self.label_remap is not None:\n",
    "            # buat array output dengan nilai default 0 (background) atau -1 jika ingin ignore\n",
    "            y_remap = np.zeros_like(y, dtype=np.int64)\n",
    "            # set default to 0 (background) then map others\n",
    "            for orig_label, new_idx in self.label_remap.items():\n",
    "                # gunakan boolean mask assignment (efisien)\n",
    "                if orig_label == 0:\n",
    "                    # background -> keep 0 (or explicitly assign)\n",
    "                    y_remap[y == orig_label] = new_idx\n",
    "                else:\n",
    "                    y_remap[y == orig_label] = new_idx\n",
    "            y = y_remap\n",
    "        else:\n",
    "            y = y.astype(np.int64)\n",
    "\n",
    "        # Konversi ke tensor\n",
    "        x_tensor = torch.tensor(x.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "\n",
    "def detect_actual_classes(pairs):\n",
    "    \"\"\"Scan semua file y.npy untuk mendeteksi kelas yang benar-benar ada\"\"\"\n",
    "    found = set()\n",
    "    for _, fy in pairs:\n",
    "        y = np.load(fy, mmap_mode=\"r\")\n",
    "        found |= set(np.unique(y))\n",
    "    found = sorted(list(found))\n",
    "    print(f\"[INFO] Kelas AKTUAL yang ditemukan di dataset: {found}\")\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ce7f40-73e1-4678-9b6b-61fe3ed57384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total kelas di JSON: 41\n",
      "Total pasangan file X-Y ditemukan: 18 (expected: 18)\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_003_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_003_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_004_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_004_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_008_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_008_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_009_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_009_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_013_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_013_processed_y.npy\n",
      "\n",
      "=== FINAL SPLIT PER FILE ===\n",
      "Train : 11\n",
      "Val   : 5\n",
      "Test  : 2\n",
      "[INFO] Kelas AKTUAL yang ditemukan di dataset: [np.int32(0), np.int32(8), np.int32(12), np.int32(13), np.int32(14), np.int32(18), np.int32(38)]\n",
      "[INFO] Label remap (orig -> new): {0: 0, 8: 1, 12: 2, 13: 3, 14: 4, 18: 5, 38: 6}\n",
      "[INFO] Total tile terdaftar: 1099 dari 11 file\n",
      "[INFO] Total tile terdaftar: 518 dari 5 file\n",
      "[INFO] Total tile terdaftar: 161 dari 2 file\n",
      "[INFO] Pixel counts per (remapped) class (train set): {0: 14185638, 1: 840140, 2: 1566138, 3: 808104, 4: 36978, 5: 139337, 6: 1297565}\n",
      "[INFO] Class weights (remapped, sum-normalized, bg=0): [0.         0.2185292  0.11722793 0.22719244 4.96498242 1.31763365\n",
      " 0.14149204]\n",
      "\n",
      "Total TILE train: 1099, val: 518, test: 161\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Cell 4. Load Dataset dan FILE-LEVEL Splitting\n",
    "# ==============================================\n",
    "\n",
    "data_dir = \"../data/npy_converted\"\n",
    "label_json_path = \"../data/annotation/segmentation_masks/label_classes.json\"\n",
    "\n",
    "label_map = load_label_mapping(label_json_path)\n",
    "print(f\"Jumlah total kelas di JSON: {len(label_map)}\")\n",
    "\n",
    "# Ambil semua file _x.npy\n",
    "all_x_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\"_x.npy\")])\n",
    "pairs = [(fx, fx.replace(\"_x.npy\", \"_y.npy\")) for fx in all_x_files if os.path.exists(fx.replace(\"_x.npy\", \"_y.npy\"))]\n",
    "\n",
    "print(f\"Total pasangan file X-Y ditemukan: {len(pairs)} (expected: 18)\")\n",
    "for p in pairs[:5]:\n",
    "    print(\"Contoh:\", os.path.basename(p[0]), \"<->\", os.path.basename(p[1]))\n",
    "\n",
    "# Split deterministik berbasis urutan nama (11 train, 5 val, 3 test)\n",
    "train_pairs = pairs[:11]\n",
    "val_pairs   = pairs[11:16]\n",
    "test_pairs  = pairs[16:]\n",
    "\n",
    "print(\"\\n=== FINAL SPLIT PER FILE ===\")\n",
    "print(f\"Train : {len(train_pairs)}\")\n",
    "print(f\"Val   : {len(val_pairs)}\")\n",
    "print(f\"Test  : {len(test_pairs)}\")\n",
    "\n",
    "# DETEKSI kelas aktual yang benar-benar muncul (bukan ambil dari JSON)\n",
    "actual_classes = detect_actual_classes(train_pairs + val_pairs + test_pairs)\n",
    "\n",
    "# actual_classes adalah list of numpy ints e.g. [0,8,12,13,14,18,38]\n",
    "orig_classes = [int(x) for x in actual_classes]  # cast to python ints\n",
    "# Buat mapping orig_label -> contiguous idx 0..(C-1)\n",
    "label_remap = {orig: idx for idx, orig in enumerate(orig_classes)}\n",
    "print(f\"[INFO] Label remap (orig -> new): {label_remap}\")\n",
    "\n",
    "# Update datasets: pass label_remap ke SeaweedDataset\n",
    "train_dataset = SeaweedDataset([p[0] for p in train_pairs], label_map, tile_size=128, label_remap=label_remap)\n",
    "val_dataset   = SeaweedDataset([p[0] for p in val_pairs], label_map, tile_size=128, label_remap=label_remap)\n",
    "test_dataset  = SeaweedDataset([p[0] for p in test_pairs], label_map, tile_size=128, label_remap=label_remap, normalize=False)\n",
    "\n",
    "\n",
    "# Hitung frekuensi kelas dari TRAIN dataset (hitung dari file y, lebih efisien)\n",
    "'''\n",
    "Menghitung distribusi jumlah pixel dari setiap kelas di TRAIN SET\n",
    "Lalu menghitung bobot loss yang adil (class weights) berdasarkan distribusi ini\n",
    "Supaya kelas langka tidak tertindas / diabaikan oleh model, \n",
    "karena dataset Imbalance besar (background 0 paling dominan)\n",
    "'''\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for _, fy in train_pairs:\n",
    "    y = np.load(fy, mmap_mode=\"r\")\n",
    "    # remap using label_remap quickly:\n",
    "    for orig, new in label_remap.items():\n",
    "        cnt = int((y == orig).sum())\n",
    "        counter[new] += cnt\n",
    "\n",
    "print(f\"[INFO] Pixel counts per (remapped) class (train set): {dict(counter)}\")\n",
    "\n",
    "# Buat class weights (inverse frequency), dan set weight[0]=0 karena ignore_index=0\n",
    "counts = np.array([counter.get(i, 0) for i in range(len(label_remap))], dtype=np.float64)\n",
    "eps = 1e-6\n",
    "inv_freq = 1.0 / (counts + eps)\n",
    "# optional normalization so that mean weight = 1\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "# set background index weight to 0 (ignored)\n",
    "inv_freq[0] = 0.0\n",
    "\n",
    "print(f\"[INFO] Class weights (remapped, sum-normalized, bg=0): {inv_freq}\")\n",
    "\n",
    "# Simpan nilai-nilai penting ke variabel global untuk digunakan Cell6\n",
    "num_classes_actual = len(label_remap)\n",
    "label_remap_global = label_remap\n",
    "# class_weights_np = inv_freq.astype(np.float32)\n",
    "\n",
    "# Untuk stabilisasi awal training — pure uniform weights\n",
    "class_weights_np = np.ones(7, dtype=np.float32)\n",
    "\n",
    "print(f\"\\nTotal TILE train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c445cb-15d0-4575-8dbc-d5928ae6384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL DEBUG (BUKAN CELL URUTAN ===\n",
    "\n",
    "# print(np.load(train_pairs[0][0], mmap_mode=\"r\").shape)\n",
    "# print(np.load(train_pairs[1][0], mmap_mode=\"r\").shape)\n",
    "# print(np.load(train_pairs[-1][0], mmap_mode=\"r\").shape)\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3ce270-784b-48f3-b800-dadaf7207919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band input aktual terdeteksi: 300\n",
      "FCHybridSN(\n",
      "  (conv3d_1): Conv3d(1, 16, kernel_size=(7, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3d_2): Conv3d(16, 32, kernel_size=(5, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3d_3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_1): Conv2d(18432, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Conv2d(64, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Cell 5. Model Fully Convolutional HybridSN (3D+2D CNN)\n",
    "# ======================================================\n",
    "\n",
    "class FCHybridSN(nn.Module):\n",
    "    def __init__(self, in_bands=300, num_classes=7): # nilai num_classes dijadikan default 7\n",
    "        super().__init__()\n",
    "        self.conv3d_1 = nn.Conv3d(1, 16, (7,3,3), padding=(0,1,1))\n",
    "        self.bn3d_1 = nn.BatchNorm3d(16)\n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, (5,3,3), padding=(0,1,1))\n",
    "        self.bn3d_2 = nn.BatchNorm3d(32)\n",
    "        self.conv3d_3 = nn.Conv3d(32, 64, (3,3,3), padding=(0,1,1))\n",
    "        self.bn3d_3 = nn.BatchNorm3d(64)\n",
    "\n",
    "        self._out_spec = in_bands - 12\n",
    "        mid_ch = 256\n",
    "        self.conv2d_1 = nn.Conv2d(64 * max(1, self._out_spec), mid_ch, 3, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2d_2 = nn.Conv2d(mid_ch, 128, 3, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(128)\n",
    "        self.conv2d_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.bn2d_3 = nn.BatchNorm2d(64)\n",
    "        self.classifier = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, Bands, H, W = x.shape\n",
    "        x3 = x.unsqueeze(1)\n",
    "        x3 = F.relu(self.bn3d_1(self.conv3d_1(x3)))\n",
    "        x3 = F.relu(self.bn3d_2(self.conv3d_2(x3)))\n",
    "        x3 = F.relu(self.bn3d_3(self.conv3d_3(x3)))\n",
    "        B, C3, out_spec, H, W = x3.shape\n",
    "        x2 = x3.view(B, C3 * out_spec, H, W)\n",
    "        x2 = F.relu(self.bn2d_1(self.conv2d_1(x2)))\n",
    "        x2 = F.relu(self.bn2d_2(self.conv2d_2(x2)))\n",
    "        x2 = F.relu(self.bn2d_3(self.conv2d_3(x2)))\n",
    "        return self.classifier(x2)\n",
    "\n",
    "# Ambil jumlah band langsung dari data TRAIN pertama\n",
    "sample_x = np.load(train_pairs[0][0], mmap_mode=\"r\")\n",
    "in_bands_actual = sample_x.shape[2]  # ambil jumlah band asli dari npy\n",
    "\n",
    "print(f\"Band input aktual terdeteksi: {in_bands_actual}\")\n",
    "\n",
    "# GUNAKAN jumlah kelas AKTUAL (bukan 41 dari JSON!)\n",
    "model = FCHybridSN(in_bands=in_bands_actual, num_classes=num_classes_actual).to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb2e104e-e26f-4f8e-bbf5-b0a26de99191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\algae\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 6. Keperluan Evaluasi & Loss / Optimizer (diperbarui)\n",
    "# ===========================================\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# jika class_weights_np sudah uniform ones, tidak perlu pakai weights here.\n",
    "# (jika nanti ingin pakai safe clipped weights, bisa diubah di sini)\n",
    "# weight_tensor = torch.from_numpy(class_weights_np)\n",
    "\n",
    "# enable cudnn benchmark untuk kecepatan (bagus ketika input sizes konstan)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# hyperparameters (adjustable)\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 1   \n",
    "\n",
    "# ==== Dataloaders ====\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Criterion: uniform (tanpa class weights) - stable start\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Optimizer & scheduler (setelah model available)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Scaler ini digunakan saat nanti menggunakan autocast\n",
    "scaler = None  # placeholder; dan juga tidak digunakan\n",
    "\n",
    "\n",
    "def pixel_accuracy(pred, target):\n",
    "    valid = (target >= 0)\n",
    "    correct = (pred[valid] == target[valid]).sum()\n",
    "    total = valid.sum()\n",
    "    return (correct.float() / (total.float() + 1e-9)).item()\n",
    "\n",
    "def iou_per_class(pred, target, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_i = (pred == cls)\n",
    "        target_i = (target == cls)\n",
    "        inter = (pred_i & target_i).sum()\n",
    "        union = (pred_i | target_i).sum()\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append((inter.float() / union.float()).item())\n",
    "    return ious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dbb8b84-32d0-41c8-92ee-fcdd5433e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CUDA available: True\n",
      "num_classes_actual = 7\n",
      "label_remap = {0: 0, 8: 1, 12: 2, 13: 3, 14: 4, 18: 5, 38: 6}\n",
      "LR = 0.0001\n",
      "Criterion: CrossEntropyLoss()\n",
      "scaler is None (AMP off)?: True\n"
     ]
    }
   ],
   "source": [
    "# Cell periksa 1\n",
    "import torch, numpy as np\n",
    "print(\"Device:\", device)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"num_classes_actual =\", num_classes_actual)\n",
    "print(\"label_remap =\", label_remap)\n",
    "print(\"LR =\", LR)\n",
    "print(\"Criterion:\", criterion)\n",
    "print(\"scaler is None (AMP off)?:\", scaler is None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9e3e28-1e6d-4039-aa6b-69a90533cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=0 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 6] | max=6 | min=0\n",
      "idx=1 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 3 6] | max=6 | min=0\n",
      "idx=2 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 3 6] | max=6 | min=0\n",
      "idx=3 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 3 6] | max=6 | min=0\n",
      "idx=4 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 3 6] | max=6 | min=0\n",
      "idx=5 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 6] | max=6 | min=0\n",
      "idx=6 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 3] | max=3 | min=0\n",
      "idx=7 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 3 6] | max=6 | min=0\n",
      "idx=8 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 6] | max=6 | min=0\n",
      "idx=9 | x.shape=torch.Size([300, 128, 128]) | y.shape=(128, 128) | unique labels=[0 3 6] | max=6 | min=0\n"
     ]
    }
   ],
   "source": [
    "# Cell periksa 2\n",
    "# ambil beberapa sample dari train_dataset (menggunakan __getitem__ yang sudah meremap)\n",
    "for idx in range(10):\n",
    "    x, y = train_dataset[idx]   # x: Tensor [Bands, H, W], y: Tensor [H,W]\n",
    "    y_np = y.numpy()\n",
    "    print(f\"idx={idx} | x.shape={x.shape} | y.shape={y_np.shape} | unique labels={np.unique(y_np)} | max={y_np.max()} | min={y_np.min()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95fe1a37-4893-458d-a6f3-2c3f0ad09241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "massimal_smola_maholmen_202306211129-2_hsi_003_processed_y.npy unique raw labels (orig): [ 0 13 14 18 38]\n",
      "massimal_smola_maholmen_202306211129-2_hsi_004_processed_y.npy unique raw labels (orig): [ 0 12]\n",
      "massimal_smola_maholmen_202306211129-2_hsi_008_processed_y.npy unique raw labels (orig): [ 0 12]\n",
      "massimal_smola_maholmen_202306211129-2_hsi_009_processed_y.npy unique raw labels (orig): [ 0 12]\n",
      "massimal_smola_maholmen_202306211129-2_hsi_013_processed_y.npy unique raw labels (orig): [0 8]\n"
     ]
    }
   ],
   "source": [
    "# Cell periksa 3\n",
    "import numpy as np\n",
    "for fx, fy in train_pairs[:5]:\n",
    "    y = np.load(fy, mmap_mode=\"r\")\n",
    "    uniq = np.unique(y)\n",
    "    print(os.path.basename(fy), \"unique raw labels (orig):\", uniq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aaa7175-6d24-4ba4-9df8-8ea2163d38de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss = 2.180907, logits finite? True\n",
      "batch 1: loss = 1.996143, logits finite? True\n",
      "batch 2: loss = 2.259423, logits finite? True\n",
      "batch 3: loss = 2.156544, logits finite? True\n",
      "batch 4: loss = 1.995491, logits finite? True\n",
      "batch 5: loss = 2.009948, logits finite? True\n",
      "batch 6: loss = 1.997626, logits finite? True\n",
      "batch 7: loss = 2.000330, logits finite? True\n",
      "batch 8: loss = 2.218812, logits finite? True\n",
      "batch 9: loss = 1.801834, logits finite? True\n",
      "batch 10: loss = 1.993381, logits finite? True\n",
      "batch 11: loss = 2.180167, logits finite? True\n",
      "batch 12: loss = 2.247280, logits finite? True\n",
      "batch 13: loss = 1.997035, logits finite? True\n",
      "batch 14: loss = 2.246928, logits finite? True\n",
      "batch 15: loss = 2.156216, logits finite? True\n",
      "batch 16: loss = 1.996756, logits finite? True\n",
      "batch 17: loss = 2.247705, logits finite? True\n",
      "batch 18: loss = 1.993167, logits finite? True\n",
      "batch 19: loss = 2.159674, logits finite? True\n",
      "dry-run done\n"
     ]
    }
   ],
   "source": [
    "# Cell periksa 4\n",
    "# dry-run: ambil 20 batch pertama dan cek loss nan\n",
    "n_batches_check = 20\n",
    "cnt = 0\n",
    "for xb, yb in train_loader:\n",
    "    xb = xb.to(device); yb = yb.to(device)\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "    print(f\"batch {cnt}: loss = {float(loss):.6f}, logits finite? {torch.isfinite(logits).all().item()}\")\n",
    "    if torch.isnan(loss) or not torch.isfinite(loss):\n",
    "        print(\"FOUND NAN/INF LOSS in batch\", cnt)\n",
    "        # print some diagnostics\n",
    "        print(\" - preds unique:\", torch.unique(logits.argmax(dim=1)).cpu().numpy())\n",
    "        print(\" - labels unique:\", torch.unique(yb).cpu().numpy())\n",
    "        break\n",
    "    cnt += 1\n",
    "    if cnt >= n_batches_check:\n",
    "        break\n",
    "print(\"dry-run done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38cd3466-8fb2-42c4-adc3-158f34196d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all-background batches in first 200 batches: 34\n"
     ]
    }
   ],
   "source": [
    "# Cell periksa 5\n",
    "from collections import Counter\n",
    "cnt_bg_batches = 0\n",
    "for i, (xb, yb) in enumerate(train_loader):\n",
    "    y_np = yb.numpy()\n",
    "    if np.all(y_np == 0):\n",
    "        cnt_bg_batches += 1\n",
    "    if i >= 200: break\n",
    "print(\"Number of all-background batches in first 200 batches:\", cnt_bg_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce62664-0656-468e-b294-464c42f7e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 7. Loop Training Utama (+ checkpoint) — no AMP + uniform class weights\n",
    "# ===========================================\n",
    "import os, time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.amp import autocast\n",
    "\n",
    "START_EPOCH = 1     # mulai default 1 (kamu sebelumnya sempat pakai 10)\n",
    "NUM_EPOCHS = 6     # boleh disesuaikan; start conservative\n",
    "CLIP_NORM = 5.0 # grad clip untuk stabilitas\n",
    "best_val_acc = 0.0\n",
    "\n",
    "checkpoint_path = \"hybridsn_sgmt_ver2_checkpoint_stable.pth\"\n",
    "best_model_path = \"hybridsn_sgmt_ver2_best_model_stable.pth\"\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Load checkpoint jika ada (including scaler & scheduler)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    scaler.load_state_dict(checkpoint.get(\"scaler_state\", {}))\n",
    "    START_EPOCH = checkpoint[\"epoch\"] + 1\n",
    "    best_val_acc = checkpoint.get(\"best_val_acc\", 0.0)\n",
    "    print(f\"[INFO] Memuat checkpoint {checkpoint_path}, resume epoch {START_EPOCH}\")\n",
    "else:\n",
    "    print(\"[INFO] Tidak ditemukan checkpoint. Mulai training dari awal.\")\n",
    "\n",
    "\n",
    "# Container untuk plotting/logging\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_miou\": []\n",
    "}\n",
    "\n",
    "# Safety: try to reduce fragmentation (optional)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128\")\n",
    "\n",
    "\n",
    "for epoch in range(START_EPOCH, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ---------- TRAIN ----------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", leave=True)\n",
    "    for xb, yb in pbar:\n",
    "        try:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Gunakan autocast untuk forward agar hemat memory (FP16)\n",
    "            with autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "                logits = model(xb)               # [B, C, H, W]\n",
    "                loss = criterion(logits, yb)\n",
    "        \n",
    "\n",
    "            # backward + grad clip + step (no AMP)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "            optimizer.step()\n",
    "        \n",
    "            # akumulasi\n",
    "            bs = xb.shape[0]\n",
    "            running_loss += (loss.item() * bs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            valid = (yb >= 0)\n",
    "            running_correct += (preds[valid] == yb[valid]).sum().item()\n",
    "            running_total += valid.sum().item()\n",
    "        \n",
    "            pbar.set_postfix({\"TrainLoss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            # Meng-handle potential OOM pada level batch: clear cache and skip batch\n",
    "            if 'out of memory' in str(e).lower():\n",
    "                print(\"[WARN] OOM terjadi saat batch forward/backward. Clear cache dan skip batch.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "            else:\n",
    "                # kirim exceptions lain\n",
    "                raise\n",
    "    avg_train_loss = running_loss / max(1, len(train_loader.dataset))\n",
    "    train_acc = running_correct / (running_total + 1e-9)\n",
    "\n",
    "    # ---------- VALIDATION ----------\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    val_running_total = 0\n",
    "    val_ious = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            \n",
    "            # autocast untuk validasi forward\n",
    "            with autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            \n",
    "            val_running_loss += loss.item() * xb.shape[0]\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            valid = (yb >= 0)\n",
    "            val_running_correct += (preds[valid] == yb[valid]).sum().item()\n",
    "            val_running_total += valid.sum().item()\n",
    "            val_ious.extend(iou_per_class(preds, yb, num_classes_actual))\n",
    "\n",
    "    avg_val_loss = val_running_loss / max(1, len(val_loader.dataset))\n",
    "    val_acc = val_running_correct / (val_running_total + 1e-9)\n",
    "    mean_iou = np.nanmean([v for v in val_ious if not np.isnan(v)])\n",
    "    \n",
    "    # Scheduler step (menggunakan val_acc sebagai metric)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # Logging & checkpoint\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_miou\"].append(mean_iou)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | TrainLoss={avg_train_loss:.4f} | ValLoss={avg_val_loss:.4f} | TrainAcc={train_acc:.4f} | ValAcc={val_acc:.4f} | mIoU={mean_iou:.4f} | Time={elapsed/3600:.2f} jam\")\n",
    "\n",
    "    # Checkpoint (tanpa simpan scaler state)\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # Save best model by val_acc\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(checkpoint, best_model_path)\n",
    "        print(\"[INFO] Model terbaik disimpan.\")\n",
    "\n",
    "# ---------- AFTER ALL EPOCHS: PLOT LOSS ----------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3ea49-ca7b-4589-9a4f-fd7f6428345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
