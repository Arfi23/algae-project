{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba550fc2-7c28-4aaf-88dc-d674f3645334",
   "metadata": {},
   "source": [
    "### Versi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fc193c-5510-46f6-8950-cef44e0e096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device yang digunakan: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 1. Import Library dan Setup Environment\n",
    "# ===========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Gunakan GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device yang digunakan:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47021b9-620b-47b4-b014-7f8f942937b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 2. Fungsi Bantuan Umum\n",
    "# ===========================================\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Menetapkan seed random agar hasil eksperimen bisa direplikasi\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "def visualize_tile(x_tile, y_true=None, y_pred=None, json_path=None,class_names=None, idx=0):\n",
    "    \"\"\"\n",
    "    Menampilkan citra tile beserta mask ground-truth dan prediksi\n",
    "    \"\"\"\n",
    "    if isinstance(x_tile, torch.Tensor):\n",
    "        x = x_tile.cpu().numpy()\n",
    "        x = np.transpose(x, (1,2,0))  # ubah dari [B,H,W] -> [H,W,B]\n",
    "    else:\n",
    "        x = x_tile\n",
    "\n",
    "    # menampilkan pseudo-RGB (karena data hyperspectral)\n",
    "    B = x.shape[2]\n",
    "    b1, b2, b3 = int(B*0.05), int(B*0.5), int(B*0.9)\n",
    "    rgb = x[..., [b1, b2, b3]]\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "\n",
    "    # Coba baca colormap dari file JSON\n",
    "    if json_path and os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            label_info = json.load(f)\n",
    "        custom_colors = [c[\"color\"][:7] for c in label_info]\n",
    "        cmap = ListedColormap(custom_colors)\n",
    "    else:\n",
    "        print(\"File json tidak terbaca, menggunakan cmap tab20\")\n",
    "        cmap = \"tab20\"  # fallback\n",
    "\n",
    "        \n",
    "\n",
    "    # Visualisasi\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.imshow(rgb_norm); plt.title(\"Citra (Pseudo-RGB)\")\n",
    "    if y_true is not None:\n",
    "        plt.subplot(1,3,2); plt.imshow(y_true, cmap=cmap); plt.title(\"Ground Truth\")\n",
    "    if y_pred is not None:\n",
    "        plt.subplot(1,3,3); plt.imshow(y_pred, cmap=cmap); plt.title(\"Prediksi\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad9e39d-a3a4-4af2-9cca-44e1836dac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Cell 3. Dataset Loader (SeaweedDataset) dan Label Mapping\n",
    "# =========================================================\n",
    "\n",
    "def load_label_mapping(json_path):\n",
    "    \"\"\"Membaca file label_classes.json untuk mapping id ke nama kelas\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    idx_to_name = {i: item[\"name\"] for i, item in enumerate(data)}\n",
    "    return idx_to_name\n",
    "\n",
    "# def normalize_reflectance(cube):\n",
    "#     \"\"\"Menormalkan nilai reflektansi ke rentang 0-1 per tile\"\"\"\n",
    "#     cube = np.nan_to_num(cube).astype(np.float32)\n",
    "#     min_val = np.nanmin(cube)\n",
    "#     max_val = np.nanmax(cube)\n",
    "#     if max_val > min_val:\n",
    "#         cube = (cube - min_val) / (max_val - min_val)\n",
    "#     return cube\n",
    "\n",
    "def normalize_reflectance(cube):\n",
    "    \"\"\"Menormalkan reflektansi 0–1 per tile, hemat RAM, aman untuk mmap read-only.\"\"\"\n",
    "    # Jika hasil np.load mmap, array biasanya read-only → buat copy ringan\n",
    "    if not cube.flags.writeable:\n",
    "        cube = cube.astype(np.float32, copy=True)  # hanya salin tile, bukan file besar\n",
    "\n",
    "    # Pastikan float32\n",
    "    if cube.dtype != np.float32:\n",
    "        cube = cube.astype(np.float32, copy=False)\n",
    "\n",
    "    # Ganti NaN / Inf in-place\n",
    "    np.nan_to_num(cube, copy=False)\n",
    "\n",
    "    # Normalisasi min-max\n",
    "    min_val = np.nanmin(cube)\n",
    "    max_val = np.nanmax(cube)\n",
    "    if max_val > min_val:\n",
    "        cube -= min_val\n",
    "        cube /= (max_val - min_val + 1e-8)\n",
    "\n",
    "    return cube\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SeaweedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset hemat memori berbasis file .npy hasil konversi.\n",
    "    Membaca tile langsung dari disk menggunakan mmap_mode=\"r\".\n",
    "    \"\"\"\n",
    "    def __init__(self, data_files, label_map, tile_size=128, normalize=True, label_remap=None):\n",
    "        self.data_files = data_files\n",
    "        self.label_map = label_map\n",
    "        self.tile_size = tile_size\n",
    "        self.normalize = normalize\n",
    "        # label_remap: dict {orig_label: new_index}, if None -> identity mapping\n",
    "        self.label_remap = label_remap\n",
    "\n",
    "        # Daftar pasangan (file_x, file_y)\n",
    "        self.pairs = []\n",
    "        for f in data_files:\n",
    "            if f.endswith(\"_x.npy\"):\n",
    "                fy = f.replace(\"_x.npy\", \"_y.npy\")\n",
    "                if os.path.exists(fy):\n",
    "                    self.pairs.append((f, fy))\n",
    "        \n",
    "        # Hanya menyimpan indeks tile berdasarkan ukuran file .npy\n",
    "        self.index = []  \n",
    "        for file_idx, (fx, fy) in enumerate(self.pairs):\n",
    "            x = np.load(fx, mmap_mode=\"r\")\n",
    "            H, W, _ = x.shape\n",
    "            for i in range(0, H - tile_size + 1, tile_size):\n",
    "                for j in range(0, W - tile_size + 1, tile_size):\n",
    "                    self.index.append((file_idx, i, j))\n",
    "            del x  # bebaskan referensi memori\n",
    "\n",
    "        print(f\"[INFO] Total tile terdaftar: {len(self.index)} dari {len(self.pairs)} file\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, i, j = self.index[idx]\n",
    "        fx, fy = self.pairs[file_idx]\n",
    "        \n",
    "        # Memuat tile menggunakan mmap\n",
    "        x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "        y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "        # Abaikan tile kosong (semua 0) dengan fail-safe agar tidak infinite recursion\n",
    "        # kalau tetap kosong setelah 3 percobaan → biarkan saja y tetap kosong (model akan skip naturally karena weight=0 utk bg)\n",
    "        for _ in range(3):  # coba maksimal 3 kali\n",
    "            if np.any(y > 0):\n",
    "                break\n",
    "            file_idx, i, j = self.index[np.random.randint(0, len(self.index))]\n",
    "            fx, fy = self.pairs[file_idx]\n",
    "            x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "            y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "\n",
    "        if self.normalize:\n",
    "            x = normalize_reflectance(x)\n",
    "\n",
    "        # REMAP label bila mapping diberikan\n",
    "        if self.label_remap is not None:\n",
    "            # buat array output dengan nilai default 0 (background) atau -1 jika ingin ignore\n",
    "            y_remap = np.zeros_like(y, dtype=np.int64)\n",
    "            # set default to 0 (background) then map others\n",
    "            for orig_label, new_idx in self.label_remap.items():\n",
    "                # gunakan boolean mask assignment (efisien)\n",
    "                if orig_label == 0:\n",
    "                    # background -> keep 0 (or explicitly assign)\n",
    "                    y_remap[y == orig_label] = new_idx\n",
    "                else:\n",
    "                    y_remap[y == orig_label] = new_idx\n",
    "            y = y_remap\n",
    "        else:\n",
    "            y = y.astype(np.int64)\n",
    "\n",
    "        # Konversi ke tensor\n",
    "        x_tensor = torch.tensor(x.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "\n",
    "def detect_actual_classes(pairs):\n",
    "    \"\"\"Scan semua file y.npy untuk mendeteksi kelas yang benar-benar ada\"\"\"\n",
    "    found = set()\n",
    "    for _, fy in pairs:\n",
    "        y = np.load(fy, mmap_mode=\"r\")\n",
    "        found |= set(np.unique(y))\n",
    "    found = sorted(list(found))\n",
    "    print(f\"[INFO] Kelas AKTUAL yang ditemukan di dataset: {found}\")\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ce7f40-73e1-4678-9b6b-61fe3ed57384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total kelas di JSON: 41\n",
      "Total pasangan file X-Y ditemukan: 18 (expected: 18)\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_003_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_003_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_004_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_004_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_008_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_008_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_009_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_009_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_013_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_013_processed_y.npy\n",
      "\n",
      "=== FINAL SPLIT PER FILE ===\n",
      "Train : 11\n",
      "Val   : 5\n",
      "Test  : 2\n",
      "[INFO] Kelas AKTUAL yang ditemukan di dataset: [np.int32(0), np.int32(8), np.int32(12), np.int32(13), np.int32(14), np.int32(18), np.int32(38)]\n",
      "[INFO] Label remap (orig -> new): {0: 0, 8: 1, 12: 2, 13: 3, 14: 4, 18: 5, 38: 6}\n",
      "[INFO] Total tile terdaftar: 1099 dari 11 file\n",
      "[INFO] Total tile terdaftar: 518 dari 5 file\n",
      "[INFO] Total tile terdaftar: 161 dari 2 file\n",
      "[INFO] Pixel counts per (remapped) class (train set): {0: 14185638, 1: 840140, 2: 1566138, 3: 808104, 4: 36978, 5: 139337, 6: 1297565}\n",
      "[INFO] Class weights (remapped, sum-normalized, bg=0): [0.         0.2185292  0.11722793 0.22719244 4.96498242 1.31763365\n",
      " 0.14149204]\n",
      "\n",
      "Total TILE train: 1099, val: 518, test: 161\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Cell 4. Load Dataset dan FILE-LEVEL Splitting\n",
    "# ==============================================\n",
    "\n",
    "data_dir = \"../data/npy_converted\"\n",
    "label_json_path = \"../data/annotation/segmentation_masks/label_classes.json\"\n",
    "\n",
    "label_map = load_label_mapping(label_json_path)\n",
    "print(f\"Jumlah total kelas di JSON: {len(label_map)}\")\n",
    "\n",
    "# Ambil semua file _x.npy\n",
    "all_x_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\"_x.npy\")])\n",
    "pairs = [(fx, fx.replace(\"_x.npy\", \"_y.npy\")) for fx in all_x_files if os.path.exists(fx.replace(\"_x.npy\", \"_y.npy\"))]\n",
    "\n",
    "print(f\"Total pasangan file X-Y ditemukan: {len(pairs)} (expected: 18)\")\n",
    "for p in pairs[:5]:\n",
    "    print(\"Contoh:\", os.path.basename(p[0]), \"<->\", os.path.basename(p[1]))\n",
    "\n",
    "# Split deterministik berbasis urutan nama (11 train, 5 val, 3 test)\n",
    "train_pairs = pairs[:11]\n",
    "val_pairs   = pairs[11:16]\n",
    "test_pairs  = pairs[16:]\n",
    "\n",
    "print(\"\\n=== FINAL SPLIT PER FILE ===\")\n",
    "print(f\"Train : {len(train_pairs)}\")\n",
    "print(f\"Val   : {len(val_pairs)}\")\n",
    "print(f\"Test  : {len(test_pairs)}\")\n",
    "\n",
    "# DETEKSI kelas aktual yang benar-benar muncul (bukan ambil dari JSON)\n",
    "actual_classes = detect_actual_classes(train_pairs + val_pairs + test_pairs)\n",
    "\n",
    "# actual_classes adalah list of numpy ints e.g. [0,8,12,13,14,18,38]\n",
    "orig_classes = [int(x) for x in actual_classes]  # cast to python ints\n",
    "# Buat mapping orig_label -> contiguous idx 0..(C-1)\n",
    "label_remap = {orig: idx for idx, orig in enumerate(orig_classes)}\n",
    "print(f\"[INFO] Label remap (orig -> new): {label_remap}\")\n",
    "\n",
    "# Update datasets: pass label_remap ke SeaweedDataset\n",
    "train_dataset = SeaweedDataset([p[0] for p in train_pairs], label_map, tile_size=128, label_remap=label_remap)\n",
    "val_dataset   = SeaweedDataset([p[0] for p in val_pairs], label_map, tile_size=128, label_remap=label_remap)\n",
    "test_dataset  = SeaweedDataset([p[0] for p in test_pairs], label_map, tile_size=128, label_remap=label_remap, normalize=False)\n",
    "\n",
    "\n",
    "# Hitung frekuensi kelas dari TRAIN dataset (hitung dari file y, lebih efisien)\n",
    "'''\n",
    "Menghitung distribusi jumlah pixel dari setiap kelas di TRAIN SET\n",
    "Lalu menghitung bobot loss yang adil (class weights) berdasarkan distribusi ini\n",
    "Supaya kelas langka tidak tertindas / diabaikan oleh model, \n",
    "karena dataset Imbalance besar (background 0 paling dominan)\n",
    "'''\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for _, fy in train_pairs:\n",
    "    y = np.load(fy, mmap_mode=\"r\")\n",
    "    # remap using label_remap quickly:\n",
    "    for orig, new in label_remap.items():\n",
    "        cnt = int((y == orig).sum())\n",
    "        counter[new] += cnt\n",
    "\n",
    "print(f\"[INFO] Pixel counts per (remapped) class (train set): {dict(counter)}\")\n",
    "\n",
    "# Buat class weights (inverse frequency), dan set weight[0]=0 karena ignore_index=0\n",
    "counts = np.array([counter.get(i, 0) for i in range(len(label_remap))], dtype=np.float64)\n",
    "eps = 1e-6\n",
    "inv_freq = 1.0 / (counts + eps)\n",
    "# optional normalization so that mean weight = 1\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "# set background index weight to 0 (ignored)\n",
    "inv_freq[0] = 0.0\n",
    "\n",
    "print(f\"[INFO] Class weights (remapped, sum-normalized, bg=0): {inv_freq}\")\n",
    "\n",
    "# Simpan nilai-nilai penting ke variabel global untuk digunakan Cell6\n",
    "num_classes_actual = len(label_remap)\n",
    "label_remap_global = label_remap\n",
    "class_weights_np = inv_freq.astype(np.float32)\n",
    "\n",
    "print(f\"\\nTotal TILE train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c445cb-15d0-4575-8dbc-d5928ae6384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "# === CELL DEBUG (BUKAN CELL URUTAN ===\n",
    "\n",
    "# print(np.load(train_pairs[0][0], mmap_mode=\"r\").shape)\n",
    "# print(np.load(train_pairs[1][0], mmap_mode=\"r\").shape)\n",
    "# print(np.load(train_pairs[-1][0], mmap_mode=\"r\").shape)\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f3ce270-784b-48f3-b800-dadaf7207919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band input aktual terdeteksi: 300\n",
      "FCHybridSN(\n",
      "  (conv3d_1): Conv3d(1, 16, kernel_size=(7, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3d_2): Conv3d(16, 32, kernel_size=(5, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3d_3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_1): Conv2d(18432, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Conv2d(64, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Cell 5. Model Fully Convolutional HybridSN (3D+2D CNN)\n",
    "# ======================================================\n",
    "\n",
    "class FCHybridSN(nn.Module):\n",
    "    def __init__(self, in_bands=300, num_classes=7): # nilai num_classes dijadikan default 7\n",
    "        super().__init__()\n",
    "        self.conv3d_1 = nn.Conv3d(1, 16, (7,3,3), padding=(0,1,1))\n",
    "        self.bn3d_1 = nn.BatchNorm3d(16)\n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, (5,3,3), padding=(0,1,1))\n",
    "        self.bn3d_2 = nn.BatchNorm3d(32)\n",
    "        self.conv3d_3 = nn.Conv3d(32, 64, (3,3,3), padding=(0,1,1))\n",
    "        self.bn3d_3 = nn.BatchNorm3d(64)\n",
    "\n",
    "        self._out_spec = in_bands - 12\n",
    "        mid_ch = 256\n",
    "        self.conv2d_1 = nn.Conv2d(64 * max(1, self._out_spec), mid_ch, 3, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2d_2 = nn.Conv2d(mid_ch, 128, 3, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(128)\n",
    "        self.conv2d_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.bn2d_3 = nn.BatchNorm2d(64)\n",
    "        self.classifier = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, Bands, H, W = x.shape\n",
    "        x3 = x.unsqueeze(1)\n",
    "        x3 = F.relu(self.bn3d_1(self.conv3d_1(x3)))\n",
    "        x3 = F.relu(self.bn3d_2(self.conv3d_2(x3)))\n",
    "        x3 = F.relu(self.bn3d_3(self.conv3d_3(x3)))\n",
    "        B, C3, out_spec, H, W = x3.shape\n",
    "        x2 = x3.view(B, C3 * out_spec, H, W)\n",
    "        x2 = F.relu(self.bn2d_1(self.conv2d_1(x2)))\n",
    "        x2 = F.relu(self.bn2d_2(self.conv2d_2(x2)))\n",
    "        x2 = F.relu(self.bn2d_3(self.conv2d_3(x2)))\n",
    "        return self.classifier(x2)\n",
    "\n",
    "# Ambil jumlah band langsung dari data TRAIN pertama\n",
    "sample_x = np.load(train_pairs[0][0], mmap_mode=\"r\")\n",
    "in_bands_actual = sample_x.shape[2]  # ambil jumlah band asli dari npy\n",
    "\n",
    "print(f\"Band input aktual terdeteksi: {in_bands_actual}\")\n",
    "\n",
    "# GUNAKAN jumlah kelas AKTUAL (bukan 41 dari JSON!)\n",
    "model = FCHybridSN(in_bands=in_bands_actual, num_classes=num_classes_actual).to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2e104e-e26f-4f8e-bbf5-b0a26de99191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 6. Keperluan Evaluasi & Loss / Optimizer (diperbarui)\n",
    "# ===========================================\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Menggunakan class weights yang sudah dihitung\n",
    "# pastikan untuk memindahkan weight ke device nanti saat membangun criterion (lakukan setelah model.to(device))\n",
    "weight_tensor = torch.from_numpy(class_weights_np)\n",
    "\n",
    "# enable cudnn benchmark untuk kecepatan (bagus ketika input sizes konstan)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# hyperparameters (adjustable)\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 1   # Jika OOM, turunkan ke 2 atau 1. Coba 2 jika masih OOM.\n",
    "\n",
    "# DataLoaders menggunakan pin_memory dan num_workers untuk GPU\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "# test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ==== Dataloaders alternatif dengan mematikan multiprocessing\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# model to device sudah dilakukan lebih awal; pastikan criterion menggunakan weights on device\n",
    "criterion = nn.CrossEntropyLoss(weight=None, ignore_index=0)  # placeholder weight = 0, akan di-set setelah move model to device\n",
    "\n",
    "# Optimizer & scheduler (setelah model available)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Mixed precision scaler\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda')) # penggunaan \"enabled=\" sudah deprecated, ganti dengan yang bawah\n",
    "scaler = torch.amp.GradScaler(device.type)\n",
    "\n",
    "def pixel_accuracy(pred, target):\n",
    "    valid = (target >= 0)\n",
    "    correct = (pred[valid] == target[valid]).sum()\n",
    "    total = valid.sum()\n",
    "    return (correct.float() / (total.float() + 1e-9)).item()\n",
    "\n",
    "def iou_per_class(pred, target, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_i = (pred == cls)\n",
    "        target_i = (target == cls)\n",
    "        inter = (pred_i & target_i).sum()\n",
    "        union = (pred_i | target_i).sum()\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append((inter.float() / union.float()).item())\n",
    "    return ious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce62664-0656-468e-b294-464c42f7e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tidak ditemukan checkpoint. Mulai training dari awal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|                                                                             | 0/1099 [00:00<?, ?it/s]C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13764\\2700568697.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
      "Epoch 1/50: 100%|████████████████████████████████████████████████| 1099/1099 [04:09<00:00,  4.40it/s, TrainLoss=1.0853]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_13764\\2700568697.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss=nan | ValAcc=0.2537 | mIoU=0.0542 | Time=0.09 jam\n",
      "[INFO] Model terbaik disimpan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|████████████████████████████████████████████████| 1099/1099 [04:08<00:00,  4.43it/s, TrainLoss=1.3037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Loss=nan | ValAcc=0.1669 | mIoU=0.0369 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|████████████████████████████████████████████████| 1099/1099 [04:08<00:00,  4.42it/s, TrainLoss=0.5017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Loss=nan | ValAcc=0.0421 | mIoU=0.0101 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|████████████████████████████████████████████████| 1099/1099 [04:07<00:00,  4.43it/s, TrainLoss=0.9231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Loss=nan | ValAcc=0.0251 | mIoU=0.0066 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|████████████████████████████████████████████████| 1099/1099 [04:07<00:00,  4.45it/s, TrainLoss=2.3375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Loss=nan | ValAcc=0.2134 | mIoU=0.0473 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|████████████████████████████████████████████████| 1099/1099 [04:08<00:00,  4.43it/s, TrainLoss=0.5272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Loss=nan | ValAcc=0.1575 | mIoU=0.0382 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|███████████████████████████████████████████████████| 1099/1099 [04:08<00:00,  4.43it/s, TrainLoss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Loss=nan | ValAcc=0.3626 | mIoU=0.0774 | Time=0.09 jam\n",
      "[INFO] Model terbaik disimpan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|████████████████████████████████████████████████| 1099/1099 [04:06<00:00,  4.46it/s, TrainLoss=0.7678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Loss=nan | ValAcc=0.3471 | mIoU=0.0787 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|████████████████████████████████████████████████| 1099/1099 [04:08<00:00,  4.42it/s, TrainLoss=0.6674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Loss=nan | ValAcc=0.2190 | mIoU=0.0455 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:08<00:00,  4.43it/s, TrainLoss=0.3892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Loss=nan | ValAcc=0.2885 | mIoU=0.0669 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.39it/s, TrainLoss=0.0284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Loss=nan | ValAcc=0.1665 | mIoU=0.0433 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:09<00:00,  4.40it/s, TrainLoss=0.0050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Loss=nan | ValAcc=0.2302 | mIoU=0.0649 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:19<00:00,  4.23it/s, TrainLoss=0.1815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Loss=nan | ValAcc=0.2500 | mIoU=0.0587 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:11<00:00,  4.37it/s, TrainLoss=0.3227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Loss=nan | ValAcc=0.1908 | mIoU=0.0509 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:21<00:00,  4.20it/s, TrainLoss=0.2082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Loss=nan | ValAcc=0.1230 | mIoU=0.0342 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:07<00:00,  4.45it/s, TrainLoss=0.0676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Loss=nan | ValAcc=0.0396 | mIoU=0.0139 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:15<00:00,  4.31it/s, TrainLoss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Loss=nan | ValAcc=0.2202 | mIoU=0.0554 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:13<00:00,  4.33it/s, TrainLoss=0.1795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Loss=nan | ValAcc=0.2958 | mIoU=0.0665 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:13<00:00,  4.34it/s, TrainLoss=0.3959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Loss=nan | ValAcc=0.2213 | mIoU=0.0627 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.38it/s, TrainLoss=1.3800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | Loss=nan | ValAcc=0.2370 | mIoU=0.0709 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:14<00:00,  4.32it/s, TrainLoss=0.5360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | Loss=nan | ValAcc=0.2410 | mIoU=0.0700 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████████████████████████████████████████████| 1099/1099 [04:12<00:00,  4.35it/s, TrainLoss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | Loss=nan | ValAcc=0.2309 | mIoU=0.0664 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:11<00:00,  4.37it/s, TrainLoss=0.2040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | Loss=nan | ValAcc=0.2307 | mIoU=0.0687 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:11<00:00,  4.37it/s, TrainLoss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | Loss=nan | ValAcc=0.2177 | mIoU=0.0588 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:08<00:00,  4.42it/s, TrainLoss=0.0957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | Loss=nan | ValAcc=0.2471 | mIoU=0.0649 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:09<00:00,  4.40it/s, TrainLoss=0.0073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | Loss=nan | ValAcc=0.1419 | mIoU=0.0396 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.39it/s, TrainLoss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | Loss=nan | ValAcc=0.1871 | mIoU=0.0484 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.39it/s, TrainLoss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | Loss=nan | ValAcc=0.0875 | mIoU=0.0251 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.39it/s, TrainLoss=0.2615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | Loss=nan | ValAcc=0.1610 | mIoU=0.0316 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:11<00:00,  4.38it/s, TrainLoss=0.1499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | Loss=nan | ValAcc=0.1112 | mIoU=0.0264 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:05<00:00,  4.48it/s, TrainLoss=0.1569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | Loss=nan | ValAcc=0.2392 | mIoU=0.0583 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:06<00:00,  4.45it/s, TrainLoss=0.3327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | Loss=nan | ValAcc=0.2350 | mIoU=0.0713 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:13<00:00,  4.34it/s, TrainLoss=0.3819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | Loss=nan | ValAcc=0.1540 | mIoU=0.0397 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:07<00:00,  4.44it/s, TrainLoss=0.2826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | Loss=nan | ValAcc=0.1899 | mIoU=0.0486 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:09<00:00,  4.40it/s, TrainLoss=0.0182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | Loss=nan | ValAcc=0.1987 | mIoU=0.0429 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:11<00:00,  4.36it/s, TrainLoss=0.0503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | Loss=nan | ValAcc=0.2681 | mIoU=0.0716 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.38it/s, TrainLoss=0.0293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 | Loss=nan | ValAcc=0.1048 | mIoU=0.0290 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████████████████████████████████████████████| 1099/1099 [04:08<00:00,  4.41it/s, TrainLoss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 | Loss=nan | ValAcc=0.2004 | mIoU=0.0563 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.39it/s, TrainLoss=0.2577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 | Loss=nan | ValAcc=0.2125 | mIoU=0.0533 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.39it/s, TrainLoss=1.5584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 | Loss=nan | ValAcc=0.2146 | mIoU=0.0561 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:11<00:00,  4.38it/s, TrainLoss=0.2655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 | Loss=nan | ValAcc=0.2879 | mIoU=0.0656 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.39it/s, TrainLoss=0.0833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 | Loss=nan | ValAcc=0.1213 | mIoU=0.0303 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.38it/s, TrainLoss=0.2276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 | Loss=nan | ValAcc=0.2277 | mIoU=0.0659 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.38it/s, TrainLoss=0.7541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 | Loss=nan | ValAcc=0.1079 | mIoU=0.0246 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:10<00:00,  4.38it/s, TrainLoss=1.1751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 | Loss=nan | ValAcc=0.2344 | mIoU=0.0577 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:13<00:00,  4.34it/s, TrainLoss=0.1517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 | Loss=nan | ValAcc=0.2094 | mIoU=0.0576 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:13<00:00,  4.34it/s, TrainLoss=0.1236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 | Loss=nan | ValAcc=0.2132 | mIoU=0.0609 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:13<00:00,  4.33it/s, TrainLoss=0.0059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 | Loss=nan | ValAcc=0.1153 | mIoU=0.0314 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:14<00:00,  4.32it/s, TrainLoss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 | Loss=nan | ValAcc=0.2246 | mIoU=0.0587 | Time=0.09 jam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|███████████████████████████████████████████████| 1099/1099 [04:13<00:00,  4.34it/s, TrainLoss=0.2190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 | Loss=nan | ValAcc=0.2554 | mIoU=0.0774 | Time=0.09 jam\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 7. Loop Training Utama (+ checkpoint) — AMP & class weights\n",
    "# ===========================================\n",
    "import os, time\n",
    "from tqdm import tqdm\n",
    "\n",
    "START_EPOCH = 1     # mulai default 1 (kamu sebelumnya sempat pakai 10)\n",
    "NUM_EPOCHS = 50     # boleh disesuaikan; start conservative\n",
    "best_val_acc = 0.0\n",
    "\n",
    "checkpoint_path = \"hybridsn_sgmt_ver2_checkpoint.pth\"\n",
    "\n",
    "# Pastikan criterion menggunakan weights pada device\n",
    "weight_tensor_device = torch.from_numpy(class_weights_np).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor_device, ignore_index=0)\n",
    "\n",
    "# Load checkpoint jika ada (including scaler & scheduler)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    scaler.load_state_dict(checkpoint.get(\"scaler_state\", {}))\n",
    "    START_EPOCH = checkpoint[\"epoch\"] + 1\n",
    "    best_val_acc = checkpoint.get(\"best_val_acc\", 0.0)\n",
    "    print(f\"[INFO] Memuat checkpoint {checkpoint_path}, resume epoch {START_EPOCH}\")\n",
    "else:\n",
    "    print(\"[INFO] Tidak ditemukan checkpoint. Mulai training dari awal.\")\n",
    "\n",
    "for epoch in range(START_EPOCH, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", leave=True)\n",
    "    for xb, yb in pbar:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "            logits = model(xb)               # [B, C, H, W]\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        # scaler backward + step\n",
    "        if device.type == 'cuda':\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"TrainLoss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = running_loss / max(1, len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_accs = []\n",
    "    val_ious = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "                logits = model(xb)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_accs.append(pixel_accuracy(preds, yb))\n",
    "            val_ious.extend(iou_per_class(preds, yb, num_classes_actual))\n",
    "\n",
    "    mean_val_acc = np.nanmean(val_accs)\n",
    "    mean_iou = np.nanmean([v for v in val_ious if not np.isnan(v)])\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | Loss={avg_loss:.4f} | ValAcc={mean_val_acc:.4f} | mIoU={mean_iou:.4f} | Time={elapsed/3600:.2f} jam\")\n",
    "\n",
    "    # Scheduler step (ReduceLROnPlateau uses validation metric)\n",
    "    scheduler.step(mean_val_acc)\n",
    "\n",
    "    # Checkpoint (simpan juga scaler state)\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"scaler_state\": scaler.state_dict() if device.type == 'cuda' else {}\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    if mean_val_acc > best_val_acc:\n",
    "        best_val_acc = mean_val_acc\n",
    "        torch.save(checkpoint, \"hybridsn_sgmt_ver2_best.pth\")\n",
    "        print(\"[INFO] Model terbaik disimpan.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdccdce3-4ce4-4d41-934f-b7bac23102d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.2185292  0.11722793 0.22719245 4.9649825  1.3176336\n",
      " 0.14149204]\n"
     ]
    }
   ],
   "source": [
    "print(class_weights_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
