{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba550fc2-7c28-4aaf-88dc-d674f3645334",
   "metadata": {},
   "source": [
    "### Versi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fc193c-5510-46f6-8950-cef44e0e096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device yang digunakan: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 1. Import Library dan Setup Environment\n",
    "# ===========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Gunakan GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device yang digunakan:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47021b9-620b-47b4-b014-7f8f942937b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 2. Fungsi Bantuan Umum\n",
    "# ===========================================\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Menetapkan seed random agar hasil eksperimen bisa direplikasi\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "def visualize_tile(x_tile, y_true=None, y_pred=None, json_path=None,class_names=None, idx=0):\n",
    "    \"\"\"\n",
    "    Menampilkan citra tile beserta mask ground-truth dan prediksi\n",
    "    \"\"\"\n",
    "    if isinstance(x_tile, torch.Tensor):\n",
    "        x = x_tile.cpu().numpy()\n",
    "        x = np.transpose(x, (1,2,0))  # ubah dari [B,H,W] -> [H,W,B]\n",
    "    else:\n",
    "        x = x_tile\n",
    "\n",
    "    # menampilkan pseudo-RGB (karena data hyperspectral)\n",
    "    B = x.shape[2]\n",
    "    b1, b2, b3 = int(B*0.05), int(B*0.5), int(B*0.9)\n",
    "    rgb = x[..., [b1, b2, b3]]\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "\n",
    "    # Coba baca colormap dari file JSON\n",
    "    if json_path and os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            label_info = json.load(f)\n",
    "        custom_colors = [c[\"color\"][:7] for c in label_info]\n",
    "        cmap = ListedColormap(custom_colors)\n",
    "    else:\n",
    "        print(\"File json tidak terbaca, menggunakan cmap tab20\")\n",
    "        cmap = \"tab20\"  # fallback\n",
    "\n",
    "        \n",
    "\n",
    "    # Visualisasi\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.imshow(rgb_norm); plt.title(\"Citra (Pseudo-RGB)\")\n",
    "    if y_true is not None:\n",
    "        plt.subplot(1,3,2); plt.imshow(y_true, cmap=cmap); plt.title(\"Ground Truth\")\n",
    "    if y_pred is not None:\n",
    "        plt.subplot(1,3,3); plt.imshow(y_pred, cmap=cmap); plt.title(\"Prediksi\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad9e39d-a3a4-4af2-9cca-44e1836dac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Cell 3. Dataset Loader (SeaweedDataset) dan Label Mapping\n",
    "# =========================================================\n",
    "\n",
    "def load_label_mapping(json_path):\n",
    "    \"\"\"Membaca file label_classes.json untuk mapping id ke nama kelas\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    idx_to_name = {i: item[\"name\"] for i, item in enumerate(data)}\n",
    "    return idx_to_name\n",
    "\n",
    "# def normalize_reflectance(cube):\n",
    "#     \"\"\"Menormalkan nilai reflektansi ke rentang 0-1 per tile\"\"\"\n",
    "#     cube = np.nan_to_num(cube).astype(np.float32)\n",
    "#     min_val = np.nanmin(cube)\n",
    "#     max_val = np.nanmax(cube)\n",
    "#     if max_val > min_val:\n",
    "#         cube = (cube - min_val) / (max_val - min_val)\n",
    "#     return cube\n",
    "\n",
    "def normalize_reflectance(cube):\n",
    "    \"\"\"Menormalkan reflektansi 0–1 per tile, hemat RAM, aman untuk mmap read-only.\"\"\"\n",
    "    # Jika hasil np.load mmap, array biasanya read-only → buat copy ringan\n",
    "    if not cube.flags.writeable:\n",
    "        cube = cube.astype(np.float32, copy=True)  # hanya salin tile, bukan file besar\n",
    "\n",
    "    # Pastikan float32\n",
    "    if cube.dtype != np.float32:\n",
    "        cube = cube.astype(np.float32, copy=False)\n",
    "\n",
    "    # Ganti NaN / Inf in-place\n",
    "    np.nan_to_num(cube, copy=False)\n",
    "\n",
    "    # Normalisasi min-max\n",
    "    min_val = np.nanmin(cube)\n",
    "    max_val = np.nanmax(cube)\n",
    "    if max_val > min_val:\n",
    "        cube -= min_val\n",
    "        cube /= (max_val - min_val + 1e-8)\n",
    "\n",
    "    return cube\n",
    "\n",
    "\n",
    "class SeaweedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset hemat memori berbasis file .npy hasil konversi.\n",
    "    Membaca tile langsung dari disk menggunakan mmap_mode=\"r\".\n",
    "    \"\"\"\n",
    "    def __init__(self, data_files, label_map, tile_size=128, normalize=True, label_remap=None):\n",
    "        self.data_files = data_files\n",
    "        self.label_map = label_map\n",
    "        self.tile_size = tile_size\n",
    "        self.normalize = normalize\n",
    "        # label_remap: dict {orig_label: new_index}, if None -> identity mapping\n",
    "        self.label_remap = label_remap\n",
    "\n",
    "        # Daftar pasangan (file_x, file_y)\n",
    "        self.pairs = []\n",
    "        for f in data_files:\n",
    "            if f.endswith(\"_x.npy\"):\n",
    "                fy = f.replace(\"_x.npy\", \"_y.npy\")\n",
    "                if os.path.exists(fy):\n",
    "                    self.pairs.append((f, fy))\n",
    "        \n",
    "        # Hanya menyimpan indeks tile berdasarkan ukuran file .npy\n",
    "        self.index = []  \n",
    "        for file_idx, (fx, fy) in enumerate(self.pairs):\n",
    "            x = np.load(fx, mmap_mode=\"r\")\n",
    "            H, W, _ = x.shape\n",
    "            for i in range(0, H - tile_size + 1, tile_size):\n",
    "                for j in range(0, W - tile_size + 1, tile_size):\n",
    "                    self.index.append((file_idx, i, j))\n",
    "            del x  # bebaskan referensi memori\n",
    "\n",
    "        print(f\"[INFO] Total tile terdaftar: {len(self.index)} dari {len(self.pairs)} file\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, i, j = self.index[idx]\n",
    "        fx, fy = self.pairs[file_idx]\n",
    "        \n",
    "        # Memuat tile menggunakan mmap\n",
    "        x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "        y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "        # Abaikan tile kosong (semua 0) dengan fail-safe agar tidak infinite recursion\n",
    "        # kalau tetap kosong setelah 3 percobaan → biarkan saja y tetap kosong (model akan skip naturally karena weight=0 utk bg)\n",
    "        for _ in range(3):  # coba maksimal 3 kali\n",
    "            if np.any(y > 0):\n",
    "                break\n",
    "            file_idx, i, j = self.index[np.random.randint(0, len(self.index))]\n",
    "            fx, fy = self.pairs[file_idx]\n",
    "            x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "            y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "\n",
    "        if self.normalize:\n",
    "            x = normalize_reflectance(x)\n",
    "\n",
    "        # REMAP label bila mapping diberikan\n",
    "        if self.label_remap is not None:\n",
    "            # buat array output dengan nilai default 0 (background) atau -1 jika ingin ignore\n",
    "            y_remap = np.zeros_like(y, dtype=np.int64)\n",
    "            # set default to 0 (background) then map others\n",
    "            for orig_label, new_idx in self.label_remap.items():\n",
    "                # gunakan boolean mask assignment (efisien)\n",
    "                if orig_label == 0:\n",
    "                    # background -> keep 0 (or explicitly assign)\n",
    "                    y_remap[y == orig_label] = new_idx\n",
    "                else:\n",
    "                    y_remap[y == orig_label] = new_idx\n",
    "            y = y_remap\n",
    "        else:\n",
    "            y = y.astype(np.int64)\n",
    "\n",
    "        # Konversi ke tensor\n",
    "        x_tensor = torch.tensor(x.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "\n",
    "def detect_actual_classes(pairs):\n",
    "    \"\"\"Scan semua file y.npy untuk mendeteksi kelas yang benar-benar ada\"\"\"\n",
    "    found = set()\n",
    "    for _, fy in pairs:\n",
    "        y = np.load(fy, mmap_mode=\"r\")\n",
    "        found |= set(np.unique(y))\n",
    "    found = sorted(list(found))\n",
    "    print(f\"[INFO] Kelas AKTUAL yang ditemukan di dataset: {found}\")\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ce7f40-73e1-4678-9b6b-61fe3ed57384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total kelas di JSON: 41\n",
      "Total pasangan file X-Y ditemukan: 18 (expected: 18)\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_003_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_003_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_004_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_004_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_008_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_008_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_009_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_009_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_013_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_013_processed_y.npy\n",
      "\n",
      "=== FINAL SPLIT PER FILE ===\n",
      "Train : 11\n",
      "Val   : 5\n",
      "Test  : 2\n",
      "[INFO] Kelas AKTUAL yang ditemukan di dataset: [np.int32(0), np.int32(8), np.int32(12), np.int32(13), np.int32(14), np.int32(18), np.int32(38)]\n",
      "[INFO] Label remap (orig -> new): {0: 0, 8: 1, 12: 2, 13: 3, 14: 4, 18: 5, 38: 6}\n",
      "[INFO] Total tile terdaftar: 1099 dari 11 file\n",
      "[INFO] Total tile terdaftar: 518 dari 5 file\n",
      "[INFO] Total tile terdaftar: 161 dari 2 file\n",
      "[INFO] Pixel counts per (remapped) class (train set): {0: 14185638, 1: 840140, 2: 1566138, 3: 808104, 4: 36978, 5: 139337, 6: 1297565}\n",
      "[INFO] Class weights (remapped, sum-normalized, bg=0): [0.         0.2185292  0.11722793 0.22719244 4.96498242 1.31763365\n",
      " 0.14149204]\n",
      "\n",
      "Total TILE train: 1099, val: 518, test: 161\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Cell 4. Load Dataset dan FILE-LEVEL Splitting\n",
    "# ==============================================\n",
    "\n",
    "data_dir = \"../data/npy_converted\"\n",
    "label_json_path = \"../data/annotation/segmentation_masks/label_classes.json\"\n",
    "\n",
    "label_map = load_label_mapping(label_json_path)\n",
    "print(f\"Jumlah total kelas di JSON: {len(label_map)}\")\n",
    "\n",
    "# Ambil semua file _x.npy\n",
    "all_x_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\"_x.npy\")])\n",
    "pairs = [(fx, fx.replace(\"_x.npy\", \"_y.npy\")) for fx in all_x_files if os.path.exists(fx.replace(\"_x.npy\", \"_y.npy\"))]\n",
    "\n",
    "print(f\"Total pasangan file X-Y ditemukan: {len(pairs)} (expected: 18)\")\n",
    "for p in pairs[:5]:\n",
    "    print(\"Contoh:\", os.path.basename(p[0]), \"<->\", os.path.basename(p[1]))\n",
    "\n",
    "# Split deterministik berbasis urutan nama (11 train, 5 val, 3 test)\n",
    "train_pairs = pairs[:11]\n",
    "val_pairs   = pairs[11:16]\n",
    "test_pairs  = pairs[16:]\n",
    "\n",
    "print(\"\\n=== FINAL SPLIT PER FILE ===\")\n",
    "print(f\"Train : {len(train_pairs)}\")\n",
    "print(f\"Val   : {len(val_pairs)}\")\n",
    "print(f\"Test  : {len(test_pairs)}\")\n",
    "\n",
    "# DETEKSI kelas aktual yang benar-benar muncul (bukan ambil dari JSON)\n",
    "actual_classes = detect_actual_classes(train_pairs + val_pairs + test_pairs)\n",
    "\n",
    "# actual_classes adalah list of numpy ints e.g. [0,8,12,13,14,18,38]\n",
    "orig_classes = [int(x) for x in actual_classes]  # cast to python ints\n",
    "# Buat mapping orig_label -> contiguous idx 0..(C-1)\n",
    "label_remap = {orig: idx for idx, orig in enumerate(orig_classes)}\n",
    "print(f\"[INFO] Label remap (orig -> new): {label_remap}\")\n",
    "\n",
    "# Update datasets: pass label_remap ke SeaweedDataset\n",
    "train_dataset = SeaweedDataset([p[0] for p in train_pairs], label_map, tile_size=128, label_remap=label_remap)\n",
    "val_dataset   = SeaweedDataset([p[0] for p in val_pairs], label_map, tile_size=128, label_remap=label_remap)\n",
    "test_dataset  = SeaweedDataset([p[0] for p in test_pairs], label_map, tile_size=128, label_remap=label_remap, normalize=False)\n",
    "\n",
    "\n",
    "# Hitung frekuensi kelas dari TRAIN dataset (hitung dari file y, lebih efisien)\n",
    "'''\n",
    "Menghitung distribusi jumlah pixel dari setiap kelas di TRAIN SET\n",
    "Lalu menghitung bobot loss yang adil (class weights) berdasarkan distribusi ini\n",
    "Supaya kelas langka tidak tertindas / diabaikan oleh model, \n",
    "karena dataset Imbalance besar (background 0 paling dominan)\n",
    "'''\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for _, fy in train_pairs:\n",
    "    y = np.load(fy, mmap_mode=\"r\")\n",
    "    # remap using label_remap quickly:\n",
    "    for orig, new in label_remap.items():\n",
    "        cnt = int((y == orig).sum())\n",
    "        counter[new] += cnt\n",
    "\n",
    "print(f\"[INFO] Pixel counts per (remapped) class (train set): {dict(counter)}\")\n",
    "\n",
    "# Buat class weights (inverse frequency), dan set weight[0]=0 karena ignore_index=0\n",
    "counts = np.array([counter.get(i, 0) for i in range(len(label_remap))], dtype=np.float64)\n",
    "eps = 1e-6\n",
    "inv_freq = 1.0 / (counts + eps)\n",
    "# optional normalization so that mean weight = 1\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "# set background index weight to 0 (ignored)\n",
    "inv_freq[0] = 0.0\n",
    "\n",
    "print(f\"[INFO] Class weights (remapped, sum-normalized, bg=0): {inv_freq}\")\n",
    "\n",
    "# Simpan nilai-nilai penting ke variabel global untuk digunakan Cell6\n",
    "num_classes_actual = len(label_remap)\n",
    "label_remap_global = label_remap\n",
    "# class_weights_np = inv_freq.astype(np.float32)\n",
    "\n",
    "# Untuk stabilisasi awal training — pure uniform weights\n",
    "class_weights_np = np.ones(7, dtype=np.float32)\n",
    "\n",
    "print(f\"\\nTotal TILE train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c445cb-15d0-4575-8dbc-d5928ae6384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL DEBUG (BUKAN CELL URUTAN ===\n",
    "\n",
    "# print(np.load(train_pairs[0][0], mmap_mode=\"r\").shape)\n",
    "# print(np.load(train_pairs[1][0], mmap_mode=\"r\").shape)\n",
    "# print(np.load(train_pairs[-1][0], mmap_mode=\"r\").shape)\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3ce270-784b-48f3-b800-dadaf7207919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band input aktual terdeteksi: 300\n",
      "FCHybridSN(\n",
      "  (conv3d_1): Conv3d(1, 16, kernel_size=(7, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3d_2): Conv3d(16, 32, kernel_size=(5, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3d_3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      "  (bn3d_3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_1): Conv2d(18432, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2d_3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2d_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Conv2d(64, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Cell 5. Model Fully Convolutional HybridSN (3D+2D CNN)\n",
    "# ======================================================\n",
    "\n",
    "class FCHybridSN(nn.Module):\n",
    "    def __init__(self, in_bands=300, num_classes=7): # nilai num_classes dijadikan default 7\n",
    "        super().__init__()\n",
    "        self.conv3d_1 = nn.Conv3d(1, 16, (7,3,3), padding=(0,1,1))\n",
    "        self.bn3d_1 = nn.BatchNorm3d(16)\n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, (5,3,3), padding=(0,1,1))\n",
    "        self.bn3d_2 = nn.BatchNorm3d(32)\n",
    "        self.conv3d_3 = nn.Conv3d(32, 64, (3,3,3), padding=(0,1,1))\n",
    "        self.bn3d_3 = nn.BatchNorm3d(64)\n",
    "\n",
    "        self._out_spec = in_bands - 12\n",
    "        mid_ch = 256\n",
    "        self.conv2d_1 = nn.Conv2d(64 * max(1, self._out_spec), mid_ch, 3, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2d_2 = nn.Conv2d(mid_ch, 128, 3, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(128)\n",
    "        self.conv2d_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.bn2d_3 = nn.BatchNorm2d(64)\n",
    "        self.classifier = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, Bands, H, W = x.shape\n",
    "        x3 = x.unsqueeze(1)\n",
    "        x3 = F.relu(self.bn3d_1(self.conv3d_1(x3)))\n",
    "        x3 = F.relu(self.bn3d_2(self.conv3d_2(x3)))\n",
    "        x3 = F.relu(self.bn3d_3(self.conv3d_3(x3)))\n",
    "        B, C3, out_spec, H, W = x3.shape\n",
    "        x2 = x3.view(B, C3 * out_spec, H, W)\n",
    "        x2 = F.relu(self.bn2d_1(self.conv2d_1(x2)))\n",
    "        x2 = F.relu(self.bn2d_2(self.conv2d_2(x2)))\n",
    "        x2 = F.relu(self.bn2d_3(self.conv2d_3(x2)))\n",
    "        return self.classifier(x2)\n",
    "\n",
    "# Ambil jumlah band langsung dari data TRAIN pertama\n",
    "sample_x = np.load(train_pairs[0][0], mmap_mode=\"r\")\n",
    "in_bands_actual = sample_x.shape[2]  # ambil jumlah band asli dari npy\n",
    "\n",
    "print(f\"Band input aktual terdeteksi: {in_bands_actual}\")\n",
    "\n",
    "# GUNAKAN jumlah kelas AKTUAL (bukan 41 dari JSON!)\n",
    "model = FCHybridSN(in_bands=in_bands_actual, num_classes=num_classes_actual).to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2e104e-e26f-4f8e-bbf5-b0a26de99191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_6428\\1628421838.py:57: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=False) # AMP dinon-aktifkan\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 6. Keperluan Evaluasi & Loss / Optimizer (diperbarui)\n",
    "# ===========================================\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Menggunakan class weights yang sudah dihitung\n",
    "# pastikan untuk memindahkan weight ke device nanti saat membangun criterion (lakukan setelah model.to(device))\n",
    "weight_tensor = torch.from_numpy(class_weights_np)\n",
    "\n",
    "# enable cudnn benchmark untuk kecepatan (bagus ketika input sizes konstan)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# hyperparameters (adjustable)\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 1   # Jika OOM, turunkan ke 2 atau 1. Coba 2 jika masih OOM.\n",
    "\n",
    "CLIP_NORM = 5.0 # grad clip untuk stabilitas\n",
    "\n",
    "# DataLoaders menggunakan pin_memory dan num_workers untuk GPU\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "# test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ==== Dataloaders alternatif dengan mematikan multiprocessing\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# model to device sudah dilakukan lebih awal; pastikan criterion menggunakan weights on device\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # uniform weights (semua 1)\n",
    "\n",
    "# Optimizer & scheduler (setelah model available)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Mixed precision scaler\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda')) # penggunaan \"enabled=\" sudah deprecated, ganti dengan yang bawah\n",
    "# scaler = torch.amp.GradScaler(device.type) \n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False) # AMP dinon-aktifkan\n",
    "\n",
    "\n",
    "def pixel_accuracy(pred, target):\n",
    "    valid = (target >= 0)\n",
    "    correct = (pred[valid] == target[valid]).sum()\n",
    "    total = valid.sum()\n",
    "    return (correct.float() / (total.float() + 1e-9)).item()\n",
    "\n",
    "def iou_per_class(pred, target, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_i = (pred == cls)\n",
    "        target_i = (target == cls)\n",
    "        inter = (pred_i & target_i).sum()\n",
    "        union = (pred_i | target_i).sum()\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append((inter.float() / union.float()).item())\n",
    "    return ious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce62664-0656-468e-b294-464c42f7e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tidak ditemukan checkpoint. Mulai training dari awal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:   0%|                                                                             | 0/1099 [00:57<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 580.00 MiB. GPU 0 has a total capacity of 15.99 GiB of which 8.34 GiB is free. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 1023.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 56\u001b[0m\n\u001b[0;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')): # AMP dinon-aktifkan dulu\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#     logits = model(xb)               # [B, C, H, W]\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#     loss = criterion(logits, yb)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m               \u001b[38;5;66;03m# [B, C, H, W]\u001b[39;00m\n\u001b[0;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, yb)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# # scaler backward + step               # dikomen dulu diganti yang tanpa AMP \u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# if device.type == 'cuda':\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m#     scaler.scale(loss).backward()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# backward + grad clip + step (no AMP)\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\algae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\algae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mFCHybridSN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m x3 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m x3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3d_1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3d_1(x3)))\n\u001b[1;32m---> 29\u001b[0m x3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3d_2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m x3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3d_3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3d_3(x3)))\n\u001b[0;32m     31\u001b[0m B, C3, out_spec, H, W \u001b[38;5;241m=\u001b[39m x3\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\algae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\algae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\algae\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\algae\\lib\\site-packages\\torch\\nn\\functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 580.00 MiB. GPU 0 has a total capacity of 15.99 GiB of which 8.34 GiB is free. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 1023.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 7. Loop Training Utama (+ checkpoint) — no AMP + uniform class weights\n",
    "# ===========================================\n",
    "import os, time\n",
    "from tqdm import tqdm\n",
    "\n",
    "START_EPOCH = 1     # mulai default 1 (kamu sebelumnya sempat pakai 10)\n",
    "NUM_EPOCHS = 25     # boleh disesuaikan; start conservative\n",
    "best_val_acc = 0.0\n",
    "\n",
    "checkpoint_path = \"hybridsn_sgmt_ver2_checkpoint_stable.pth\"\n",
    "best_model_path = \"hybridsn_sgmt_ver2_best_model_stable.pth\"\n",
    "\n",
    "# Load checkpoint jika ada (including scaler & scheduler)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    scaler.load_state_dict(checkpoint.get(\"scaler_state\", {}))\n",
    "    START_EPOCH = checkpoint[\"epoch\"] + 1\n",
    "    best_val_acc = checkpoint.get(\"best_val_acc\", 0.0)\n",
    "    print(f\"[INFO] Memuat checkpoint {checkpoint_path}, resume epoch {START_EPOCH}\")\n",
    "else:\n",
    "    print(\"[INFO] Tidak ditemukan checkpoint. Mulai training dari awal.\")\n",
    "\n",
    "\n",
    "# Containers for plotting / logging\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_miou\": []\n",
    "}\n",
    "\n",
    "for epoch in range(START_EPOCH, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ---------- TRAIN ----------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", leave=True)\n",
    "    for xb, yb in pbar:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')): # AMP dinon-aktifkan dulu\n",
    "        #     logits = model(xb)               # [B, C, H, W]\n",
    "        #     loss = criterion(logits, yb)\n",
    "\n",
    "        logits = model(xb)               # [B, C, H, W]\n",
    "        loss = criterion(logits, yb)\n",
    "        \n",
    "        # # scaler backward + step               # dikomen dulu diganti yang tanpa AMP \n",
    "        # if device.type == 'cuda':\n",
    "        #     scaler.scale(loss).backward()\n",
    "        #     scaler.step(optimizer)\n",
    "        #     scaler.update()\n",
    "        # else:\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "\n",
    "        # backward + grad clip + step (no AMP)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * xb.shape[0]\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        # pixel accuracy (batch)\n",
    "        valid = (yb >= 0)\n",
    "        running_correct += (preds[valid] == yb[valid]).sum().item()\n",
    "        running_total += valid.sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\"TrainLoss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_train_loss = running_loss / max(1, len(train_loader.dataset))\n",
    "    train_acc = running_correct / (running_total + 1e-9)\n",
    "\n",
    "    # ---------- VALIDATION ----------\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    val_running_total = 0\n",
    "    val_ious = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            \n",
    "            val_running_loss += loss.item() * xb.shape[0]\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            valid = (yb >= 0)\n",
    "            val_running_correct += (preds[valid] == yb[valid]).sum().item()\n",
    "            val_running_total += valid.sum().item()\n",
    "\n",
    "            val_ious.extend(iou_per_class(preds, yb, num_classes_actual))\n",
    "\n",
    "    avg_val_loss = val_running_loss / max(1, len(val_loader.dataset))\n",
    "    val_acc = val_running_correct / (val_running_total + 1e-9)\n",
    "    mean_iou = np.nanmean([v for v in val_ious if not np.isnan(v)])\n",
    "    \n",
    "    # Scheduler step (menggunakan val_acc sebagai metric)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # Logging & checkpoint\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_miou\"].append(mean_iou)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | TrainLoss={avg_train_loss:.4f} | ValLoss={avg_val_loss:.4f} | TrainAcc={train_acc:.4f} | ValAcc={val_acc:.4f} | mIoU={mean_iou:.4f} | Time={elapsed/3600:.2f} jam\")\n",
    "\n",
    "    # Checkpoint (tanpa simpan scaler state)\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # Save best model by val_acc\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(checkpoint, best_model_path)\n",
    "        print(\"[INFO] Model terbaik disimpan.\")\n",
    "\n",
    "# ---------- AFTER ALL EPOCHS: PLOT LOSS ----------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc3ea49-ca7b-4589-9a4f-fd7f6428345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
