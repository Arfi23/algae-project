{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba550fc2-7c28-4aaf-88dc-d674f3645334",
   "metadata": {},
   "source": [
    "### Versi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fc193c-5510-46f6-8950-cef44e0e096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device yang digunakan: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Cell 1. Import Library dan Setup Environment\n",
    "# ===========================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Gunakan GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device yang digunakan:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47021b9-620b-47b4-b014-7f8f942937b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Cell 2. Fungsi Bantuan Umum\n",
    "# ===========================================\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Menetapkan seed random agar hasil eksperimen bisa direplikasi\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "def visualize_tile(x_tile, y_true=None, y_pred=None, json_path=None,class_names=None, idx=0):\n",
    "    \"\"\"\n",
    "    Menampilkan citra tile beserta mask ground-truth dan prediksi\n",
    "    \"\"\"\n",
    "    if isinstance(x_tile, torch.Tensor):\n",
    "        x = x_tile.cpu().numpy()\n",
    "        x = np.transpose(x, (1,2,0))  # ubah dari [B,H,W] -> [H,W,B]\n",
    "    else:\n",
    "        x = x_tile\n",
    "\n",
    "    # menampilkan pseudo-RGB (karena data hyperspectral)\n",
    "    B = x.shape[2]\n",
    "    b1, b2, b3 = int(B*0.05), int(B*0.5), int(B*0.9)\n",
    "    rgb = x[..., [b1, b2, b3]]\n",
    "    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-9)\n",
    "\n",
    "    # Coba baca colormap dari file JSON\n",
    "    if json_path and os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            label_info = json.load(f)\n",
    "        custom_colors = [c[\"color\"][:7] for c in label_info]\n",
    "        cmap = ListedColormap(custom_colors)\n",
    "    else:\n",
    "        print(\"File json tidak terbaca, menggunakan cmap tab20\")\n",
    "        cmap = \"tab20\"  # fallback\n",
    "\n",
    "        \n",
    "\n",
    "    # Visualisasi\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.imshow(rgb_norm); plt.title(\"Citra (Pseudo-RGB)\")\n",
    "    if y_true is not None:\n",
    "        plt.subplot(1,3,2); plt.imshow(y_true, cmap=cmap); plt.title(\"Ground Truth\")\n",
    "    if y_pred is not None:\n",
    "        plt.subplot(1,3,3); plt.imshow(y_pred, cmap=cmap); plt.title(\"Prediksi\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad9e39d-a3a4-4af2-9cca-44e1836dac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Cell 3. Dataset Loader (SeaweedDataset) dan Label Mapping\n",
    "# =========================================================\n",
    "\n",
    "def load_label_mapping(json_path):\n",
    "    \"\"\"Membaca file label_classes.json untuk mapping id ke nama kelas\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    idx_to_name = {i: item[\"name\"] for i, item in enumerate(data)}\n",
    "    return idx_to_name\n",
    "\n",
    "def normalize_reflectance(cube):\n",
    "    \"\"\"Menormalkan nilai reflektansi ke rentang 0-1 per tile\"\"\"\n",
    "    cube = np.nan_to_num(cube).astype(np.float32)\n",
    "    min_val = np.nanmin(cube)\n",
    "    max_val = np.nanmax(cube)\n",
    "    if max_val > min_val:\n",
    "        cube = (cube - min_val) / (max_val - min_val)\n",
    "    return cube\n",
    "\n",
    "\n",
    "class SeaweedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset hemat memori berbasis file .npy hasil konversi.\n",
    "    Membaca tile langsung dari disk menggunakan mmap_mode=\"r\".\n",
    "    \"\"\"\n",
    "    def __init__(self, data_files, label_map, tile_size=128, normalize=True):\n",
    "        self.data_files = data_files\n",
    "        self.label_map = label_map\n",
    "        self.tile_size = tile_size\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Daftar pasangan (file_x, file_y)\n",
    "        self.pairs = []\n",
    "        for f in data_files:\n",
    "            if f.endswith(\"_x.npy\"):\n",
    "                fy = f.replace(\"_x.npy\", \"_y.npy\")\n",
    "                if os.path.exists(fy):\n",
    "                    self.pairs.append((f, fy))\n",
    "        \n",
    "        # Hanya menyimpan indeks tile berdasarkan ukuran file .npy\n",
    "        self.index = []  \n",
    "        for file_idx, (fx, fy) in enumerate(self.pairs):\n",
    "            x = np.load(fx, mmap_mode=\"r\")\n",
    "            H, W, _ = x.shape\n",
    "            for i in range(0, H - tile_size + 1, tile_size):\n",
    "                for j in range(0, W - tile_size + 1, tile_size):\n",
    "                    self.index.append((file_idx, i, j))\n",
    "            del x  # bebaskan referensi memori\n",
    "\n",
    "        print(f\"[INFO] Total tile terdaftar: {len(self.index)} dari {len(self.pairs)} file\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, i, j = self.index[idx]\n",
    "        fx, fy = self.pairs[file_idx]\n",
    "        \n",
    "        # Memuat tile menggunakan mmap\n",
    "        x = np.load(fx, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size, :]\n",
    "        y = np.load(fy, mmap_mode=\"r\")[i:i+self.tile_size, j:j+self.tile_size]\n",
    "\n",
    "        # Abaikan tile kosong (semua 0)\n",
    "        if not np.any(y > 0):\n",
    "            # Jika tile kosong, ambil tile lain secara acak agar batch tetap penuh\n",
    "            return self.__getitem__(np.random.randint(0, len(self.index)))\n",
    "\n",
    "        if self.normalize:\n",
    "            x = normalize_reflectance(x)\n",
    "\n",
    "        # Konversi ke tensor\n",
    "        x_tensor = torch.tensor(x.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "def detect_actual_classes(pairs):\n",
    "    \"\"\"Scan semua file y.npy untuk mendeteksi kelas yang benar-benar ada\"\"\"\n",
    "    found = set()\n",
    "    for _, fy in pairs:\n",
    "        y = np.load(fy, mmap_mode=\"r\")\n",
    "        found |= set(np.unique(y))\n",
    "    found = sorted(list(found))\n",
    "    print(f\"[INFO] Kelas AKTUAL yang ditemukan di dataset: {found}\")\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ce7f40-73e1-4678-9b6b-61fe3ed57384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total kelas di JSON: 41\n",
      "Total pasangan file X-Y ditemukan: 18 (expected: 18)\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_003_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_003_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_004_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_004_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_008_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_008_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_009_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_009_processed_y.npy\n",
      "Contoh: massimal_smola_maholmen_202306211129-2_hsi_013_processed_x.npy <-> massimal_smola_maholmen_202306211129-2_hsi_013_processed_y.npy\n",
      "\n",
      "=== FINAL SPLIT PER FILE ===\n",
      "Train : 11\n",
      "Val   : 5\n",
      "Test  : 2\n",
      "[INFO] Kelas AKTUAL yang ditemukan di dataset: [np.int32(0), np.int32(8), np.int32(12), np.int32(13), np.int32(14), np.int32(18), np.int32(38)]\n",
      "\n",
      "num_classes SEMENTARA = 7 (nanti kita pakai ini untuk model)\n",
      "[INFO] Total tile terdaftar: 1099 dari 11 file\n",
      "[INFO] Total tile terdaftar: 518 dari 5 file\n",
      "[INFO] Total tile terdaftar: 161 dari 2 file\n",
      "\n",
      "Total TILE train: 1099, val: 518, test: 161\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Cell 4. Load Dataset dan FILE-LEVEL Splitting\n",
    "# ==============================================\n",
    "\n",
    "data_dir = \"../data/npy_converted\"\n",
    "label_json_path = \"../data/annotation/segmentation_masks/label_classes.json\"\n",
    "\n",
    "label_map = load_label_mapping(label_json_path)\n",
    "print(f\"Jumlah total kelas di JSON: {len(label_map)}\")\n",
    "\n",
    "# Ambil semua file _x.npy\n",
    "all_x_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\"_x.npy\")])\n",
    "pairs = [(fx, fx.replace(\"_x.npy\", \"_y.npy\")) for fx in all_x_files if os.path.exists(fx.replace(\"_x.npy\", \"_y.npy\"))]\n",
    "\n",
    "print(f\"Total pasangan file X-Y ditemukan: {len(pairs)} (expected: 18)\")\n",
    "for p in pairs[:5]:\n",
    "    print(\"Contoh:\", os.path.basename(p[0]), \"<->\", os.path.basename(p[1]))\n",
    "\n",
    "# Split deterministik berbasis urutan nama (11 train, 5 val, 3 test)\n",
    "train_pairs = pairs[:11]\n",
    "val_pairs   = pairs[11:16]\n",
    "test_pairs  = pairs[16:]\n",
    "\n",
    "print(\"\\n=== FINAL SPLIT PER FILE ===\")\n",
    "print(f\"Train : {len(train_pairs)}\")\n",
    "print(f\"Val   : {len(val_pairs)}\")\n",
    "print(f\"Test  : {len(test_pairs)}\")\n",
    "\n",
    "# DETEKSI kelas aktual yang benar-benar muncul (bukan ambil dari JSON)\n",
    "actual_classes = detect_actual_classes(train_pairs + val_pairs + test_pairs)\n",
    "num_classes_actual = len(actual_classes)\n",
    "print(f\"\\nnum_classes SEMENTARA = {num_classes_actual} (nanti kita pakai ini untuk model)\")\n",
    "\n",
    "# Buat dataset per split\n",
    "train_dataset = SeaweedDataset([p[0] for p in train_pairs], label_map, tile_size=128)\n",
    "val_dataset   = SeaweedDataset([p[0] for p in val_pairs], label_map, tile_size=128)\n",
    "test_dataset  = SeaweedDataset([p[0] for p in test_pairs], label_map, tile_size=128, normalize=False)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"\\nTotal TILE train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c445cb-15d0-4575-8dbc-d5928ae6384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 900, 300)\n",
      "(2000, 900, 300)\n",
      "(2000, 900, 300)\n"
     ]
    }
   ],
   "source": [
    "# === CELL DEBUG (BUKAN CELL URUTAN ===\n",
    "\n",
    "print(np.load(train_pairs[0][0], mmap_mode=\"r\").shape)\n",
    "print(np.load(train_pairs[1][0], mmap_mode=\"r\").shape)\n",
    "print(np.load(train_pairs[-1][0], mmap_mode=\"r\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ce270-784b-48f3-b800-dadaf7207919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Cell 5. Model Fully Convolutional HybridSN (3D+2D CNN)\n",
    "# ======================================================\n",
    "\n",
    "class FCHybridSN(nn.Module):\n",
    "    def __init__(self, in_bands=300, num_classes=7): # nilai num_classes dijadikan default 7\n",
    "        super().__init__()\n",
    "        self.conv3d_1 = nn.Conv3d(1, 16, (7,3,3), padding=(0,1,1))\n",
    "        self.bn3d_1 = nn.BatchNorm3d(16)\n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, (5,3,3), padding=(0,1,1))\n",
    "        self.bn3d_2 = nn.BatchNorm3d(32)\n",
    "        self.conv3d_3 = nn.Conv3d(32, 64, (3,3,3), padding=(0,1,1))\n",
    "        self.bn3d_3 = nn.BatchNorm3d(64)\n",
    "\n",
    "        self._out_spec = in_bands - 12\n",
    "        mid_ch = 256\n",
    "        self.conv2d_1 = nn.Conv2d(64 * max(1, self._out_spec), mid_ch, 3, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2d_2 = nn.Conv2d(mid_ch, 128, 3, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(128)\n",
    "        self.conv2d_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.bn2d_3 = nn.BatchNorm2d(64)\n",
    "        self.classifier = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, Bands, H, W = x.shape\n",
    "        x3 = x.unsqueeze(1)\n",
    "        x3 = F.relu(self.bn3d_1(self.conv3d_1(x3)))\n",
    "        x3 = F.relu(self.bn3d_2(self.conv3d_2(x3)))\n",
    "        x3 = F.relu(self.bn3d_3(self.conv3d_3(x3)))\n",
    "        B, C3, out_spec, H, W = x3.shape\n",
    "        x2 = x3.view(B, C3 * out_spec, H, W)\n",
    "        x2 = F.relu(self.bn2d_1(self.conv2d_1(x2)))\n",
    "        x2 = F.relu(self.bn2d_2(self.conv2d_2(x2)))\n",
    "        x2 = F.relu(self.bn2d_3(self.conv2d_3(x2)))\n",
    "        return self.classifier(x2)\n",
    "\n",
    "# Ambil jumlah band langsung dari data TRAIN pertama\n",
    "sample_x = np.load(train_pairs[0][0], mmap_mode=\"r\")\n",
    "in_bands_actual = sample_x.shape[2]  # ambil jumlah band asli dari npy\n",
    "\n",
    "print(f\"Band input aktual terdeteksi: {in_bands_actual}\")\n",
    "\n",
    "# GUNAKAN jumlah kelas AKTUAL (bukan 41 dari JSON!)\n",
    "model = FCHybridSN(in_bands=in_bands_actual, num_classes=num_classes_actual).to(device)\n",
    "\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
